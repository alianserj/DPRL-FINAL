{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b1a211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "APPROACH 2 — FULL END-TO-END IMPLEMENTATION (DEBUG/PRINT HEAVY)\n",
    "\n",
    "What this script does (in order):\n",
    "\n",
    "1) Build a gridworld with two corridors separated by a wall (two “gaps”).\n",
    "2) Place two sensors: one near each gap. The *hidden sensor mode* m ∈ {0,1}\n",
    "   chooses which sensor is the active hotspot (high detection probability region).\n",
    "3) Robot receives noisy alarm measurements o_t ∈ {0,1}. Robot keeps a belief b_t(m)\n",
    "   and updates it with Bayes' rule.\n",
    "\n",
    "4) Define a small finite set of robot policies Π_R and sensor policies Π_S.\n",
    "   Each policy is treated as a pure action in an empirical normal-form game.\n",
    "\n",
    "5) Estimate empirical payoff matrices U_R, U_S by Monte Carlo rollouts.\n",
    "\n",
    "6) Compute Larson Nash Bargaining Solution (NBS) over a joint distribution x over\n",
    "   joint policy pairs (π_R, π_S) using projected gradient ascent on:\n",
    "       g(x) = log(u_R^T x - d_R) + log(u_S^T x - d_S)\n",
    "\n",
    "7) Extract marginals σ_R, σ_S from x, compute best responses:\n",
    "   - Robot BR: risk-weighted A* path under expected risk map induced by σ_S.\n",
    "   - Sensor BR: brute-force best fixed mode against σ_R (Monte Carlo).\n",
    "\n",
    "8) Iterate steps 5-7 (outer loop), printing each step.\n",
    "\n",
    "9) Final stage: “MultiNash-PF-like” multimodal trajectory demo:\n",
    "   - Take top-k joint pairs under x\n",
    "   - Convert discrete paths to continuous trajectories\n",
    "   - Sample noisy “particles” around each ref trajectory\n",
    "   - Refine with a cheap local smoother (stand-in for IPOPT refinement)\n",
    "   - Cluster refined trajectories by discrete Fréchet distance\n",
    "   - Print discovered “modes” (clusters)\n",
    "\n",
    "Dependencies:\n",
    "  - Python 3.9+\n",
    "  - numpy\n",
    "\n",
    "Run:\n",
    "  python approach2_nbs_stealth.py\n",
    "\n",
    "Suggested first run:\n",
    "  python approach2_nbs_stealth.py --outer-iters 2 --rollouts-payoff 10 --rollouts-br 15\n",
    "\n",
    "If you want VERY verbose per-step rollouts:\n",
    "  python approach2_nbs_stealth.py --debug-one-rollout-per-pair\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Any\n",
    "import argparse\n",
    "import heapq\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Small helper: consistent printing\n",
    "# =============================================================================\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    \"\"\"Print and flush immediately (useful for long runs).\"\"\"\n",
    "    print(msg, flush=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Grid World + Sensor Model\n",
    "# =============================================================================\n",
    "\n",
    "Action = Tuple[int, int]  # (dx, dy)\n",
    "\n",
    "MOVES: Dict[str, Action] = {\n",
    "    \"UP\": (0, -1),\n",
    "    \"DOWN\": (0, 1),\n",
    "    \"LEFT\": (-1, 0),\n",
    "    \"RIGHT\": (1, 0),\n",
    "    \"STAY\": (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GridConfig:\n",
    "    width: int\n",
    "    height: int\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    obstacles: frozenset  # set of blocked cells (x,y)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SensorConfig:\n",
    "    sensors: Tuple[Tuple[int, int], ...]  # sensor locations; mode=m chooses sensors[m] hotspot\n",
    "    radius: int                            # Manhattan radius of hotspot region\n",
    "    base_p: float                          # baseline detection probability\n",
    "    hotspot_p: float                       # detection probability inside hotspot\n",
    "\n",
    "\n",
    "class GridWorldStealthEnv:\n",
    "    \"\"\"\n",
    "    A very small \"stealth\" environment.\n",
    "\n",
    "    Hidden \"mode\" is not inside env state (we treat it as a parameter chosen by sensor policy),\n",
    "    but the robot maintains belief over modes using observation model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid: GridConfig, sensor_cfg: SensorConfig, fp: float = 0.05, fn: float = 0.10):\n",
    "        self.grid = grid\n",
    "        self.sensor_cfg = sensor_cfg\n",
    "\n",
    "        # Observation noise:\n",
    "        #   fp = false positive rate for alarm channel\n",
    "        #   fn = false negative rate\n",
    "        self.fp = float(fp)\n",
    "        self.fn = float(fn)\n",
    "        if not (0.0 <= self.fp <= 1.0 and 0.0 <= self.fn <= 1.0):\n",
    "            raise ValueError(\"fp and fn must be in [0,1].\")\n",
    "\n",
    "        self._rng = random.Random()\n",
    "        self.reset(sensor_mode=0)\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        self._rng.seed(int(seed))\n",
    "\n",
    "    def reset(self, sensor_mode: int = 0) -> Dict[str, Any]:\n",
    "        self.t = 0\n",
    "        self.pos = self.grid.start\n",
    "        self.sensor_mode = int(sensor_mode)\n",
    "        self.detected = False\n",
    "        self.total_true_risk = 0.0\n",
    "        return {\"pos\": self.pos, \"t\": self.t}\n",
    "\n",
    "    def in_bounds(self, p: Tuple[int, int]) -> bool:\n",
    "        x, y = p\n",
    "        return 0 <= x < self.grid.width and 0 <= y < self.grid.height\n",
    "\n",
    "    def is_free(self, p: Tuple[int, int]) -> bool:\n",
    "        return self.in_bounds(p) and (p not in self.grid.obstacles)\n",
    "\n",
    "    def true_detection_prob(self, p: Tuple[int, int], mode: int) -> float:\n",
    "        \"\"\"\n",
    "        True detection probability at position p when active mode=mode.\n",
    "        Mode selects which sensor center is \"hot\".\n",
    "        \"\"\"\n",
    "        base = self.sensor_cfg.base_p\n",
    "        hot = self.sensor_cfg.hotspot_p\n",
    "        sx, sy = self.sensor_cfg.sensors[mode]\n",
    "        x, y = p\n",
    "        d = abs(x - sx) + abs(y - sy)  # Manhattan distance\n",
    "        return hot if d <= self.sensor_cfg.radius else base\n",
    "\n",
    "    def observation_prob(self, alarm: int, p_true: float) -> float:\n",
    "        \"\"\"\n",
    "        We emit a noisy \"alarm\" o∈{0,1}. Probability alarm=1:\n",
    "          p_alarm = p_true*(1-fn) + (1-p_true)*fp\n",
    "        \"\"\"\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        if alarm == 1:\n",
    "            return p_alarm\n",
    "        if alarm == 0:\n",
    "            return 1.0 - p_alarm\n",
    "        raise ValueError(\"alarm must be 0 or 1\")\n",
    "\n",
    "    def step(self, a: Action) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute robot action, update robot position, sample detection, and output alarm observation.\n",
    "\n",
    "        Returns dict with:\n",
    "          pos, t, alarm, p_true, detected, done\n",
    "        \"\"\"\n",
    "        if self.detected:\n",
    "            # absorbing terminal\n",
    "            return {\n",
    "                \"pos\": self.pos,\n",
    "                \"t\": self.t,\n",
    "                \"alarm\": 1,\n",
    "                \"p_true\": 1.0,\n",
    "                \"detected\": True,\n",
    "                \"done\": True,\n",
    "            }\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        nx = self.pos[0] + int(a[0])\n",
    "        ny = self.pos[1] + int(a[1])\n",
    "        np_ = (nx, ny)\n",
    "\n",
    "        # If invalid move or obstacle -> stay in place\n",
    "        if self.is_free(np_):\n",
    "            self.pos = np_\n",
    "\n",
    "        # true risk at new position\n",
    "        p_true = self.true_detection_prob(self.pos, self.sensor_mode)\n",
    "        self.total_true_risk += p_true\n",
    "\n",
    "        # sample true detection event\n",
    "        if self._rng.random() < p_true:\n",
    "            self.detected = True\n",
    "\n",
    "        # sample noisy alarm\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        alarm = 1 if (self._rng.random() < p_alarm) else 0\n",
    "\n",
    "        done = self.detected or (self.pos == self.grid.goal) or (self.t >= 200)\n",
    "        return {\n",
    "            \"pos\": self.pos,\n",
    "            \"t\": self.t,\n",
    "            \"alarm\": alarm,\n",
    "            \"p_true\": p_true,\n",
    "            \"detected\": self.detected,\n",
    "            \"done\": done,\n",
    "        }\n",
    "\n",
    "\n",
    "def build_two_corridor_grid(width: int = 15, height: int = 9) -> GridConfig:\n",
    "    \"\"\"\n",
    "    Build a vertical wall splitting the grid, with two gaps (two corridors).\n",
    "    This creates multimodality: robot can go through upper gap or lower gap.\n",
    "    \"\"\"\n",
    "    obstacles = set()\n",
    "    wall_x = width // 2\n",
    "    gap_ys = {2, 6}  # two corridor openings\n",
    "\n",
    "    for y in range(height):\n",
    "        if y not in gap_ys:\n",
    "            obstacles.add((wall_x, y))\n",
    "\n",
    "    start = (1, height - 2)\n",
    "    goal = (width - 2, 1)\n",
    "\n",
    "    if start in obstacles or goal in obstacles:\n",
    "        raise RuntimeError(\"Start/goal ended up blocked; change map parameters.\")\n",
    "\n",
    "    return GridConfig(width=width, height=height, start=start, goal=goal, obstacles=frozenset(obstacles))\n",
    "\n",
    "\n",
    "def print_grid_ascii(grid: GridConfig, sensor_cfg: SensorConfig) -> None:\n",
    "    \"\"\"Print an ASCII map for debugging.\"\"\"\n",
    "    W, H = grid.width, grid.height\n",
    "    obs = set(grid.obstacles)\n",
    "    sens = set(sensor_cfg.sensors)\n",
    "\n",
    "    for y in range(H):\n",
    "        row = []\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p == grid.start:\n",
    "                row.append(\"R\")\n",
    "            elif p == grid.goal:\n",
    "                row.append(\"G\")\n",
    "            elif p in sens:\n",
    "                row.append(\"S\")\n",
    "            elif p in obs:\n",
    "                row.append(\"#\")\n",
    "            else:\n",
    "                row.append(\".\")\n",
    "        log(\"\".join(row))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Robot belief over modes: exact Bayes filter for discrete mode set\n",
    "# =============================================================================\n",
    "\n",
    "class ModeBelief:\n",
    "    \"\"\"\n",
    "    Belief b(m) over discrete hidden sensor mode m ∈ {0,...,M-1}.\n",
    "    We do exact Bayes update:\n",
    "        b_{t+1}(m) ∝ P(o_{t+1} | pos_{t+1}, m) * b_t(m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, M: int, init: Optional[np.ndarray] = None):\n",
    "        self.M = int(M)\n",
    "        if self.M <= 0:\n",
    "            raise ValueError(\"M must be positive.\")\n",
    "\n",
    "        if init is None:\n",
    "            self.b = np.full(self.M, 1.0 / self.M)\n",
    "        else:\n",
    "            init = np.asarray(init, dtype=float).reshape(-1)\n",
    "            if init.shape != (self.M,):\n",
    "                raise ValueError(\"init belief has wrong shape.\")\n",
    "            if np.any(init < 0):\n",
    "                raise ValueError(\"init belief must be nonnegative.\")\n",
    "            s = float(init.sum())\n",
    "            self.b = init / s if s > 0 else np.full(self.M, 1.0 / self.M)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        env: GridWorldStealthEnv,\n",
    "        alarm: int,\n",
    "        pos: Tuple[int, int],\n",
    "        kappa: float = 1e-12,\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        b'(m) ∝ P(alarm | pos, m) * b(m)\n",
    "        \"\"\"\n",
    "        likelihood = np.zeros(self.M, dtype=float)\n",
    "        for m in range(self.M):\n",
    "            p_true = env.true_detection_prob(pos, m)\n",
    "            likelihood[m] = env.observation_prob(alarm, p_true)\n",
    "\n",
    "        prior = self.b\n",
    "        post_unnorm = prior * likelihood\n",
    "        Z = float(post_unnorm.sum())\n",
    "\n",
    "        if (not np.isfinite(Z)) or Z < kappa:\n",
    "            # If something goes numerically wrong, keep prior.\n",
    "            if verbose:\n",
    "                log(f\"[BeliefUpdate] WARNING: normalization Z={Z}. Keeping prior.\")\n",
    "            post = prior\n",
    "        else:\n",
    "            post = post_unnorm / Z\n",
    "\n",
    "        if verbose:\n",
    "            log(\n",
    "                f\"[BeliefUpdate] pos={pos} alarm={alarm} \"\n",
    "                f\"prior={prior.round(3)} like={likelihood.round(3)} post={post.round(3)}\"\n",
    "            )\n",
    "\n",
    "        self.b = post\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# A* pathfinding (robot BR oracle uses this)\n",
    "# =============================================================================\n",
    "\n",
    "def astar_path(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    verbose: bool = False,\n",
    "    max_expansions: int = 250_000,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Classic A* on 4-neighbor grid with custom step cost.\n",
    "    Returns a list of cells [start, ..., goal].\n",
    "    \"\"\"\n",
    "\n",
    "    def h(p: Tuple[int, int]) -> float:\n",
    "        # Manhattan heuristic\n",
    "        return abs(p[0] - goal[0]) + abs(p[1] - goal[1])\n",
    "\n",
    "    def neighbors(p: Tuple[int, int]):\n",
    "        x, y = p\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            np_ = (x + dx, y + dy)\n",
    "            if 0 <= np_[0] < grid.width and 0 <= np_[1] < grid.height and np_ not in grid.obstacles:\n",
    "                yield np_\n",
    "\n",
    "    open_heap: List[Tuple[float, float, Tuple[int, int]]] = []\n",
    "    heapq.heappush(open_heap, (h(start), 0.0, start))\n",
    "\n",
    "    came_from: Dict[Tuple[int, int], Optional[Tuple[int, int]]] = {start: None}\n",
    "    gscore: Dict[Tuple[int, int], float] = {start: 0.0}\n",
    "\n",
    "    expansions = 0\n",
    "\n",
    "    while open_heap:\n",
    "        f, g, cur = heapq.heappop(open_heap)\n",
    "        expansions += 1\n",
    "\n",
    "        if verbose and expansions % 5000 == 0:\n",
    "            log(f\"[A*] expansions={expansions} open={len(open_heap)} cur={cur} g={g:.2f} f={f:.2f}\")\n",
    "\n",
    "        if cur == goal:\n",
    "            # reconstruct\n",
    "            path: List[Tuple[int, int]] = []\n",
    "            while cur is not None:\n",
    "                path.append(cur)\n",
    "                cur = came_from[cur]\n",
    "            path.reverse()\n",
    "            if verbose:\n",
    "                log(f\"[A*] SUCCESS path_len={len(path)} expansions={expansions}\")\n",
    "            return path\n",
    "\n",
    "        if expansions > max_expansions:\n",
    "            raise RuntimeError(f\"A* exceeded max_expansions={max_expansions}; maybe unreachable?\")\n",
    "\n",
    "        for nb in neighbors(cur):\n",
    "            tentative = gscore[cur] + float(step_cost(cur, nb))\n",
    "            if nb not in gscore or tentative < gscore[nb] - 1e-12:\n",
    "                gscore[nb] = tentative\n",
    "                came_from[nb] = cur\n",
    "                heapq.heappush(open_heap, (tentative + h(nb), tentative, nb))\n",
    "\n",
    "    raise RuntimeError(\"A* failed: goal unreachable.\")\n",
    "\n",
    "\n",
    "def astar_via_waypoint(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    waypoint: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    verbose: bool = False,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"Force a path to go through waypoint by chaining A*.\"\"\"\n",
    "    p1 = astar_path(grid, start, waypoint, step_cost, verbose=verbose)\n",
    "    p2 = astar_path(grid, waypoint, goal, step_cost, verbose=verbose)\n",
    "    return p1[:-1] + p2  # avoid duplicating waypoint cell\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policies (Robot + Sensor)\n",
    "# =============================================================================\n",
    "\n",
    "class RobotPolicy:\n",
    "    \"\"\"Interface for robot decision-making.\"\"\"\n",
    "    name: str = \"RobotPolicy\"\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(\n",
    "        self,\n",
    "        env: GridWorldStealthEnv,\n",
    "        belief: ModeBelief,\n",
    "        last_obs: Optional[int],\n",
    "        verbose: bool = False,\n",
    "    ) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedPathPolicy(RobotPolicy):\n",
    "    \"\"\"Robot follows a fixed precomputed path (interpretable macro strategy).\"\"\"\n",
    "\n",
    "    def __init__(self, path: List[Tuple[int, int]], name: str):\n",
    "        if len(path) < 2:\n",
    "            raise ValueError(\"path must include start and goal.\")\n",
    "        self.path = list(path)\n",
    "        self.name = str(name)\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        # Attempt to align with path index; otherwise start at 0.\n",
    "        try:\n",
    "            self._idx = self.path.index(start_pos)\n",
    "        except ValueError:\n",
    "            self._idx = 0\n",
    "\n",
    "    def act(\n",
    "        self,\n",
    "        env: GridWorldStealthEnv,\n",
    "        belief: ModeBelief,\n",
    "        last_obs: Optional[int],\n",
    "        verbose: bool = False,\n",
    "    ) -> Action:\n",
    "        cur = env.pos\n",
    "        if self._idx >= len(self.path) - 1:\n",
    "            return MOVES[\"STAY\"]\n",
    "\n",
    "        # If we somehow deviated, try to resync (robustness).\n",
    "        if cur != self.path[self._idx]:\n",
    "            try:\n",
    "                j = self.path.index(cur, self._idx)\n",
    "                self._idx = j\n",
    "            except ValueError:\n",
    "                if verbose:\n",
    "                    log(f\"[{self.name}] WARNING: off-path at {cur}; STAY.\")\n",
    "                return MOVES[\"STAY\"]\n",
    "\n",
    "        nxt = self.path[self._idx + 1]\n",
    "        dx = int(np.clip(nxt[0] - cur[0], -1, 1))\n",
    "        dy = int(np.clip(nxt[1] - cur[1], -1, 1))\n",
    "        self._idx += 1\n",
    "\n",
    "        if verbose:\n",
    "            log(f\"[{self.name}] cur={cur} -> nxt={nxt} act={(dx, dy)} idx={self._idx} belief={belief.b.round(3)}\")\n",
    "\n",
    "        return (dx, dy)\n",
    "\n",
    "\n",
    "class RandomPolicy(RobotPolicy):\n",
    "    \"\"\"Baseline robot policy: random valid move.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"R_Random\"):\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(\n",
    "        self,\n",
    "        env: GridWorldStealthEnv,\n",
    "        belief: ModeBelief,\n",
    "        last_obs: Optional[int],\n",
    "        verbose: bool = False,\n",
    "    ) -> Action:\n",
    "        cands: List[Action] = []\n",
    "        for a in [(1, 0), (-1, 0), (0, 1), (0, -1), (0, 0)]:\n",
    "            np_ = (env.pos[0] + a[0], env.pos[1] + a[1])\n",
    "            if env.is_free(np_):\n",
    "                cands.append(a)\n",
    "\n",
    "        a = random.choice(cands) if cands else (0, 0)\n",
    "        if verbose:\n",
    "            log(f\"[{self.name}] cur={env.pos} act={a} belief={belief.b.round(3)}\")\n",
    "        return a\n",
    "\n",
    "\n",
    "class SensorPolicy:\n",
    "    \"\"\"Interface for sensor controller.\"\"\"\n",
    "    name: str = \"SensorPolicy\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedModeSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Sensor policy that always picks the same mode (hotspot at fixed sensor).\"\"\"\n",
    "\n",
    "    def __init__(self, mode: int, name: Optional[str] = None):\n",
    "        self.mode = int(mode)\n",
    "        self.name = name or f\"S_Mode{mode}\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return self.mode\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rollout simulator (empirical payoff estimation)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStats:\n",
    "    steps: int\n",
    "    reached_goal: bool\n",
    "    detected: bool\n",
    "    total_true_risk: float\n",
    "    robot_utility: float\n",
    "    sensor_utility: float\n",
    "\n",
    "\n",
    "def rollout_episode(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_policy: RobotPolicy,\n",
    "    sensor_policy: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    max_steps: int = 200,\n",
    "    lambda_risk: float = 1.0,\n",
    "    det_penalty: float = 50.0,\n",
    "    sensor_energy_per_step: float = 0.2,\n",
    "    seed: Optional[int] = None,\n",
    "    verbose: bool = False,\n",
    ") -> EpisodeStats:\n",
    "    \"\"\"\n",
    "    Run one episode and compute utilities.\n",
    "\n",
    "    Robot cost:\n",
    "      cost_R = steps + lambda_risk * sum_t p_true(t) + det_penalty * 1{detected}\n",
    "      utility_R = -cost_R\n",
    "\n",
    "    Sensor utility:\n",
    "      utility_S = det_penalty * 1{detected} + lambda_risk * sum_t p_true(t) - sensor_energy_per_step * steps\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "\n",
    "    sensor_policy.reset()\n",
    "    env.reset(sensor_mode=sensor_policy.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)  # uniform initial belief\n",
    "    robot_policy.reset(env.pos)\n",
    "\n",
    "    total_risk = 0.0\n",
    "    steps = 0\n",
    "    last_alarm = None\n",
    "\n",
    "    if verbose:\n",
    "        log(\"=\" * 80)\n",
    "        log(f\"[Rollout] START robot={robot_policy.name} sensor={sensor_policy.name} start={env.pos} goal={env.grid.goal}\")\n",
    "        log(f\"[Rollout] belief0={belief.b.round(3)} over modes 0..{M_modes-1}\")\n",
    "        log(\"=\" * 80)\n",
    "\n",
    "    while steps < max_steps:\n",
    "        # Sensor chooses mode (can depend on time)\n",
    "        env.sensor_mode = sensor_policy.select_mode(env.t)\n",
    "\n",
    "        # Robot picks action based on belief\n",
    "        a = robot_policy.act(env, belief, last_alarm, verbose=verbose)\n",
    "\n",
    "        out = env.step(a)\n",
    "        steps += 1\n",
    "        total_risk += out[\"p_true\"]\n",
    "\n",
    "        # Belief update using observation model\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"], verbose=verbose)\n",
    "        last_alarm = out[\"alarm\"]\n",
    "\n",
    "        if verbose:\n",
    "            log(f\"[Rollout] t={out['t']:3d} pos={out['pos']} a={a} p_true={out['p_true']:.3f} \"\n",
    "                f\"alarm={out['alarm']} detected={out['detected']} done={out['done']}\")\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = env.detected\n",
    "\n",
    "    robot_cost = steps + lambda_risk * total_risk + (det_penalty if detected else 0.0)\n",
    "    robot_utility = -robot_cost\n",
    "\n",
    "    sensor_utility = (det_penalty if detected else 0.0) + lambda_risk * total_risk - sensor_energy_per_step * steps\n",
    "\n",
    "    if verbose:\n",
    "        log(\"-\" * 80)\n",
    "        log(f\"[Rollout] END steps={steps} reached_goal={reached_goal} detected={detected} total_risk={total_risk:.3f}\")\n",
    "        log(f\"[Rollout] U_R={robot_utility:.3f} U_S={sensor_utility:.3f}\")\n",
    "        log(\"=\" * 80)\n",
    "\n",
    "    return EpisodeStats(\n",
    "        steps=steps,\n",
    "        reached_goal=reached_goal,\n",
    "        detected=detected,\n",
    "        total_true_risk=total_risk,\n",
    "        robot_utility=robot_utility,\n",
    "        sensor_utility=sensor_utility,\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Empirical payoff estimation: U_R, U_S matrices\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_payoff_matrices(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_policies: List[RobotPolicy],\n",
    "    sensor_policies: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    N_rollouts: int = 30,\n",
    "    base_seed: int = 123,\n",
    "    debug_one_rollout_per_pair: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[int, int], Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Estimate empirical payoff matrices by Monte Carlo rollouts.\n",
    "\n",
    "    Returns:\n",
    "      U_R: (m,n)\n",
    "      U_S: (m,n)\n",
    "      diagnostics[(i,j)] = extra stats (det_rate, goal_rate, etc.)\n",
    "    \"\"\"\n",
    "    m = len(robot_policies)\n",
    "    n = len(sensor_policies)\n",
    "\n",
    "    U_R = np.zeros((m, n), dtype=float)\n",
    "    U_S = np.zeros((m, n), dtype=float)\n",
    "    diagnostics: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    if verbose:\n",
    "        log(\"\\n\" + \"#\" * 80)\n",
    "        log(f\"[EvalPayoffs] m={m} robot policies, n={n} sensor policies, N_rollouts={N_rollouts}, base_seed={base_seed}\")\n",
    "        log(\"#\" * 80)\n",
    "\n",
    "    for i, rp in enumerate(robot_policies):\n",
    "        for j, sp in enumerate(sensor_policies):\n",
    "            if debug_one_rollout_per_pair and verbose:\n",
    "                log(\"\\n\" + \"-\" * 80)\n",
    "                log(f\"[EvalPayoffs][DEBUG] One fully-verbose rollout for (R{i}:{rp.name}, S{j}:{sp.name})\")\n",
    "                rollout_episode(\n",
    "                    env,\n",
    "                    rp,\n",
    "                    sp,\n",
    "                    M_modes=M_modes,\n",
    "                    seed=base_seed + 100000 * i + 1000 * j,\n",
    "                    verbose=True,\n",
    "                )\n",
    "                log(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "            r_utils: List[float] = []\n",
    "            s_utils: List[float] = []\n",
    "            detected_count = 0\n",
    "            goal_count = 0\n",
    "            steps_list: List[int] = []\n",
    "            risk_list: List[float] = []\n",
    "\n",
    "            for k in range(N_rollouts):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                stats = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed, verbose=False)\n",
    "                r_utils.append(stats.robot_utility)\n",
    "                s_utils.append(stats.sensor_utility)\n",
    "                detected_count += int(stats.detected)\n",
    "                goal_count += int(stats.reached_goal)\n",
    "                steps_list.append(stats.steps)\n",
    "                risk_list.append(stats.total_true_risk)\n",
    "\n",
    "            U_R[i, j] = float(np.mean(r_utils))\n",
    "            U_S[i, j] = float(np.mean(s_utils))\n",
    "\n",
    "            diagnostics[(i, j)] = {\n",
    "                \"det_rate\": detected_count / N_rollouts,\n",
    "                \"goal_rate\": goal_count / N_rollouts,\n",
    "                \"mean_steps\": float(np.mean(steps_list)),\n",
    "                \"mean_risk\": float(np.mean(risk_list)),\n",
    "                \"std_UR\": float(np.std(r_utils)),\n",
    "                \"std_US\": float(np.std(s_utils)),\n",
    "            }\n",
    "\n",
    "            if verbose:\n",
    "                d = diagnostics[(i, j)]\n",
    "                log(\n",
    "                    f\"[EvalPayoffs] (R{i}:{rp.name}, S{j}:{sp.name}) -> \"\n",
    "                    f\"U_R={U_R[i,j]:8.3f}±{d['std_UR']:.2f} | \"\n",
    "                    f\"U_S={U_S[i,j]:8.3f}±{d['std_US']:.2f} | \"\n",
    "                    f\"det%={d['det_rate']*100:5.1f} goal%={d['goal_rate']*100:5.1f} \"\n",
    "                    f\"steps={d['mean_steps']:.1f} risk={d['mean_risk']:.2f}\"\n",
    "                )\n",
    "\n",
    "    if verbose:\n",
    "        log(\"#\" * 80 + \"\\n\")\n",
    "\n",
    "    return U_R, U_S, diagnostics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Larson NBS meta-solver: projected gradient ascent on log Nash product\n",
    "# =============================================================================\n",
    "\n",
    "def project_to_simplex(v: np.ndarray, z: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Euclidean projection onto simplex:\n",
    "      { x >= 0, sum(x) = z }\n",
    "    Robust implementation with sorting / threshold.\n",
    "    \"\"\"\n",
    "    if z <= 0:\n",
    "        raise ValueError(\"z must be > 0\")\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    n = v.size\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - z))[0]\n",
    "    if rho.size == 0:\n",
    "        return np.full(n, z / n)\n",
    "    rho = int(rho[-1])\n",
    "    theta = (cssv[rho] - z) / (rho + 1.0)\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    s = float(w.sum())\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full(n, z / n)\n",
    "    return w * (z / s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NBSResult:\n",
    "    x: np.ndarray\n",
    "    history: List[Dict[str, float]]\n",
    "\n",
    "\n",
    "def solve_nbs_projected_gradient(\n",
    "    uR: np.ndarray,\n",
    "    uS: np.ndarray,\n",
    "    max_iters: int = 400,\n",
    "    alpha: float = 0.5,\n",
    "    alpha_min: float = 1e-6,\n",
    "    tol: float = 1e-6,\n",
    "    kappa: float = 1e-6,\n",
    "    verbose: bool = True,\n",
    ") -> NBSResult:\n",
    "    \"\"\"\n",
    "    Solve NBS on joint distribution x over joint actions:\n",
    "\n",
    "      maximize g(x) = log(uR^T x - dR) + log(uS^T x - dS)\n",
    "      s.t. x in simplex\n",
    "\n",
    "    Disagreement:\n",
    "      dR = min(uR) - 1\n",
    "      dS = min(uS) - 1\n",
    "\n",
    "    Gradient:\n",
    "      ∇g(x) = uR/(uR^T x - dR) + uS/(uS^T x - dS)\n",
    "\n",
    "    Update:\n",
    "      y = x + alpha * ∇g(x)\n",
    "      x = Proj_simplex(y)\n",
    "\n",
    "    Includes backtracking line search for robustness.\n",
    "    \"\"\"\n",
    "    uR = np.asarray(uR, dtype=float).reshape(-1)\n",
    "    uS = np.asarray(uS, dtype=float).reshape(-1)\n",
    "    if uR.shape != uS.shape:\n",
    "        raise ValueError(\"uR and uS must have same shape.\")\n",
    "    d = uR.size\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Need at least 2 joint actions to compute NBS.\")\n",
    "\n",
    "    dR = float(np.min(uR) - 1.0)\n",
    "    dS = float(np.min(uS) - 1.0)\n",
    "\n",
    "    x = np.full(d, 1.0 / d, dtype=float)  # start uniform\n",
    "\n",
    "    def gains(xv: np.ndarray) -> Tuple[float, float]:\n",
    "        gR = float(uR @ xv - dR)\n",
    "        gS = float(uS @ xv - dS)\n",
    "        return gR, gS\n",
    "\n",
    "    def objective(xv: np.ndarray) -> float:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return float(np.log(gR) + np.log(gS))\n",
    "\n",
    "    def grad(xv: np.ndarray) -> np.ndarray:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return (uR / gR) + (uS / gS)\n",
    "\n",
    "    hist: List[Dict[str, float]] = []\n",
    "    last_obj = objective(x)\n",
    "\n",
    "    if verbose:\n",
    "        gR, gS = gains(x)\n",
    "        log(\"\\n\" + \"#\" * 80)\n",
    "        log(\"[NBS] Starting projected gradient ascent\")\n",
    "        log(f\"[NBS] d={d} joint actions | dR={dR:.3f}, dS={dS:.3f}\")\n",
    "        log(f\"[NBS] init gains=(R:{gR:.3f}, S:{gS:.3f}) obj={last_obj:.6f}\")\n",
    "        log(\"#\" * 80)\n",
    "\n",
    "    for t in range(1, max_iters + 1):\n",
    "        gvec = grad(x)\n",
    "        gnorm = float(np.linalg.norm(gvec))\n",
    "        if not np.isfinite(gnorm) or gnorm == 0.0:\n",
    "            if verbose:\n",
    "                log(f\"[NBS] WARNING: bad gradient norm at iter={t}: {gnorm}\")\n",
    "            break\n",
    "\n",
    "        # Backtracking line search\n",
    "        a = alpha\n",
    "        improved = False\n",
    "        for _ in range(30):\n",
    "            y = x + a * gvec\n",
    "            x_new = project_to_simplex(y)\n",
    "            obj_new = objective(x_new)\n",
    "            if obj_new >= last_obj - 1e-12:\n",
    "                improved = True\n",
    "                break\n",
    "            a *= 0.5\n",
    "            if a < alpha_min:\n",
    "                break\n",
    "\n",
    "        if not improved:\n",
    "            if verbose:\n",
    "                log(f\"[NBS] Line search failed at iter={t}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        delta_l1 = float(np.linalg.norm(x_new - x, ord=1))\n",
    "        x = x_new\n",
    "        last_obj = obj_new\n",
    "        gR, gS = gains(x)\n",
    "\n",
    "        hist.append(\n",
    "            {\n",
    "                \"iter\": float(t),\n",
    "                \"obj\": float(last_obj),\n",
    "                \"gain_R\": float(gR),\n",
    "                \"gain_S\": float(gS),\n",
    "                \"alpha\": float(a),\n",
    "                \"delta_L1\": float(delta_l1),\n",
    "                \"grad_norm\": float(gnorm),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if verbose and (t <= 10 or t % 25 == 0):\n",
    "            topk = np.argsort(-x)[:5]\n",
    "            top_str = \", \".join([f\"{idx}:{x[idx]:.3f}\" for idx in topk])\n",
    "            log(\n",
    "                f\"[NBS] iter={t:4d} obj={last_obj:.6f} gains(R={gR:.3f},S={gS:.3f}) \"\n",
    "                f\"alpha={a:.3g} L1delta={delta_l1:.3g} top={top_str}\"\n",
    "            )\n",
    "\n",
    "        if delta_l1 < tol:\n",
    "            if verbose:\n",
    "                log(f\"[NBS] Converged at iter={t} (L1delta={delta_l1:.2e} < tol={tol})\")\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        log(\"-\" * 80)\n",
    "        log(\"[NBS] Finished. Top joint actions (index:prob):\")\n",
    "        for idx in np.argsort(-x)[:10]:\n",
    "            log(f\"  {idx:3d}: {x[idx]:.6f}\")\n",
    "        log(f\"[NBS] sum(x)={x.sum():.6f} (should be 1.0)\")\n",
    "        log(\"#\" * 80 + \"\\n\")\n",
    "\n",
    "    return NBSResult(x=x, history=hist)\n",
    "\n",
    "\n",
    "def joint_to_matrix(x: np.ndarray, m: int, n: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size != m * n:\n",
    "        raise ValueError(\"x size mismatch.\")\n",
    "    return x.reshape((m, n))\n",
    "\n",
    "\n",
    "def marginals_from_joint(x: np.ndarray, m: int, n: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X = joint_to_matrix(x, m, n)\n",
    "    sigma_R = X.sum(axis=1)  # sum over sensors\n",
    "    sigma_S = X.sum(axis=0)  # sum over robots\n",
    "    if sigma_R.sum() > 0:\n",
    "        sigma_R = sigma_R / sigma_R.sum()\n",
    "    if sigma_S.sum() > 0:\n",
    "        sigma_S = sigma_S / sigma_S.sum()\n",
    "    return sigma_R, sigma_S\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Best Response Oracles (tractable, no deep nets)\n",
    "# =============================================================================\n",
    "\n",
    "def build_initial_robot_policies(env: GridWorldStealthEnv) -> List[RobotPolicy]:\n",
    "    \"\"\"\n",
    "    Create interpretable initial robot policies:\n",
    "      - shortest path\n",
    "      - forced upper corridor\n",
    "      - forced lower corridor\n",
    "      - random baseline\n",
    "    \"\"\"\n",
    "    grid = env.grid\n",
    "    step_cost = lambda a, b: 1.0\n",
    "\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    return [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_initial_sensor_policies(M_modes: int) -> List[SensorPolicy]:\n",
    "    \"\"\"Start with each fixed mode as a pure sensor policy.\"\"\"\n",
    "    return [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "\n",
    "\n",
    "def find_sensor_policy_by_mode(sensor_policies: List[SensorPolicy], mode: int) -> Optional[FixedModeSensorPolicy]:\n",
    "    for sp in sensor_policies:\n",
    "        if isinstance(sp, FixedModeSensorPolicy) and sp.mode == mode:\n",
    "            return sp\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_expected_risk_map(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensor_policies: List[SensorPolicy],\n",
    "    sigma_S: np.ndarray,\n",
    "    M_modes: int,\n",
    "    verbose: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute expected true detection probability p_true(cell) under sensor mixture sigma_S.\n",
    "\n",
    "    We approximate each sensor policy by its mode at t=0 (fixed-mode policies in this script).\n",
    "    \"\"\"\n",
    "    sigma_S = np.asarray(sigma_S, dtype=float).reshape(-1)\n",
    "    if sigma_S.size != len(sensor_policies):\n",
    "        raise ValueError(\"sigma_S length mismatch.\")\n",
    "\n",
    "    W, H = env.grid.width, env.grid.height\n",
    "    risk = np.zeros((H, W), dtype=float)\n",
    "\n",
    "    # implied mode distribution from the mixture over sensor policies\n",
    "    mode_probs = np.zeros(M_modes, dtype=float)\n",
    "    for j, sp in enumerate(sensor_policies):\n",
    "        m = sp.select_mode(0)\n",
    "        if not (0 <= m < M_modes):\n",
    "            raise ValueError(f\"sensor policy returned invalid mode {m}\")\n",
    "        mode_probs[m] += sigma_S[j]\n",
    "    if mode_probs.sum() > 0:\n",
    "        mode_probs = mode_probs / mode_probs.sum()\n",
    "\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p in env.grid.obstacles:\n",
    "                risk[y, x] = np.nan\n",
    "                continue\n",
    "            val = 0.0\n",
    "            for m in range(M_modes):\n",
    "                val += mode_probs[m] * env.true_detection_prob(p, m)\n",
    "            risk[y, x] = val\n",
    "\n",
    "    if verbose:\n",
    "        finite = risk[np.isfinite(risk)]\n",
    "        log(\"\\n\" + \"#\" * 80)\n",
    "        log(\"[RiskMap] Expected risk map computed from sensor mixture\")\n",
    "        log(f\"[RiskMap] sigma_S={sigma_S.round(3)} -> mode_probs={mode_probs.round(3)}\")\n",
    "        log(f\"[RiskMap] risk stats: min={finite.min():.3f} mean={finite.mean():.3f} max={finite.max():.3f}\")\n",
    "        log(\"#\" * 80 + \"\\n\")\n",
    "\n",
    "    return risk\n",
    "\n",
    "\n",
    "def robot_best_response_from_sigmaS(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensor_policies: List[SensorPolicy],\n",
    "    sigma_S: np.ndarray,\n",
    "    existing_robot_policies: List[RobotPolicy],\n",
    "    M_modes: int,\n",
    "    risk_weight: float = 12.0,\n",
    "    verbose: bool = True,\n",
    ") -> RobotPolicy:\n",
    "    \"\"\"\n",
    "    Robot BR to sigma_S:\n",
    "      - compute expected risk map under sigma_S\n",
    "      - solve risk-weighted shortest path with A*\n",
    "      - return a FixedPathPolicy following that path\n",
    "    \"\"\"\n",
    "    risk = compute_expected_risk_map(env, sensor_policies, sigma_S, M_modes=M_modes, verbose=verbose)\n",
    "\n",
    "    def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "        x, y = to\n",
    "        r = risk[y, x]\n",
    "        if not np.isfinite(r):\n",
    "            return 1e9\n",
    "        return 1.0 + risk_weight * float(r)\n",
    "\n",
    "    if verbose:\n",
    "        log(\"[RobotBR] Running risk-weighted A* ...\")\n",
    "        log(f\"[RobotBR] risk_weight={risk_weight}\")\n",
    "\n",
    "    path = astar_path(env.grid, env.grid.start, env.grid.goal, step_cost, verbose=verbose)\n",
    "\n",
    "    # Avoid duplicates if same path already exists\n",
    "    path_tuple = tuple(path)\n",
    "    for pol in existing_robot_policies:\n",
    "        if isinstance(pol, FixedPathPolicy) and tuple(pol.path) == path_tuple:\n",
    "            if verbose:\n",
    "                log(f\"[RobotBR] BR path already exists as '{pol.name}'. Returning existing.\")\n",
    "            return pol\n",
    "\n",
    "    name = f\"R_BR_RiskAStar_w{risk_weight:.1f}_len{len(path)}\"\n",
    "    if verbose:\n",
    "        log(f\"[RobotBR] Created NEW robot policy: {name}\")\n",
    "\n",
    "    return FixedPathPolicy(path, name=name)\n",
    "\n",
    "\n",
    "def sensor_best_response_from_sigmaR(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_policies: List[RobotPolicy],\n",
    "    sigma_R: np.ndarray,\n",
    "    candidate_modes: List[int],\n",
    "    M_modes: int,\n",
    "    N_rollouts: int = 30,\n",
    "    base_seed: int = 999,\n",
    "    verbose: bool = True,\n",
    ") -> FixedModeSensorPolicy:\n",
    "    \"\"\"\n",
    "    Sensor BR to sigma_R:\n",
    "      brute-force evaluate each fixed mode by Monte Carlo:\n",
    "        pick robot policy index i ~ sigma_R\n",
    "        rollout (π_R^i, sensor_mode)\n",
    "      choose mode with highest E[U_S]\n",
    "    \"\"\"\n",
    "    sigma_R = np.asarray(sigma_R, dtype=float).reshape(-1)\n",
    "    if sigma_R.size != len(robot_policies):\n",
    "        raise ValueError(\"sigma_R length mismatch.\")\n",
    "\n",
    "    rng = np.random.default_rng(base_seed)\n",
    "\n",
    "    if verbose:\n",
    "        log(\"\\n\" + \"#\" * 80)\n",
    "        log(\"[SensorBR] Searching best-response fixed sensor mode against robot mixture\")\n",
    "        log(f\"[SensorBR] sigma_R={sigma_R.round(3)}\")\n",
    "        log(f\"[SensorBR] candidate_modes={candidate_modes}\")\n",
    "        log(\"#\" * 80)\n",
    "\n",
    "    best_mode: Optional[int] = None\n",
    "    best_val = -1e18\n",
    "\n",
    "    for mode in candidate_modes:\n",
    "        if not (0 <= mode < M_modes):\n",
    "            continue\n",
    "\n",
    "        sp = FixedModeSensorPolicy(mode, name=f\"S_BR_Mode{mode}\")\n",
    "        utils: List[float] = []\n",
    "\n",
    "        for k in range(N_rollouts):\n",
    "            i = int(rng.choice(len(robot_policies), p=sigma_R))\n",
    "            rp = robot_policies[i]\n",
    "            seed = base_seed + 10000 * mode + k\n",
    "            stats = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed, verbose=False)\n",
    "            utils.append(stats.sensor_utility)\n",
    "\n",
    "        mean_u = float(np.mean(utils))\n",
    "        std_u = float(np.std(utils))\n",
    "\n",
    "        if verbose:\n",
    "            log(f\"[SensorBR] mode={mode} -> E[U_S]={mean_u:8.3f} (std={std_u:6.2f})\")\n",
    "\n",
    "        if mean_u > best_val:\n",
    "            best_val = mean_u\n",
    "            best_mode = mode\n",
    "\n",
    "    if best_mode is None:\n",
    "        raise RuntimeError(\"No valid sensor mode found.\")\n",
    "\n",
    "    if verbose:\n",
    "        log(\"-\" * 80)\n",
    "        log(f\"[SensorBR] Best response mode={best_mode} with estimated E[U_S]={best_val:.3f}\")\n",
    "        log(\"#\" * 80 + \"\\n\")\n",
    "\n",
    "    return FixedModeSensorPolicy(best_mode, name=f\"S_BR_Mode{best_mode}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Outer loop: Empirical game + NBS + BR expansion\n",
    "# =============================================================================\n",
    "\n",
    "def run_approach2_pipeline(\n",
    "    verbose: bool = True,\n",
    "    outer_iters: int = 3,\n",
    "    rollouts_payoff: int = 20,\n",
    "    rollouts_br: int = 30,\n",
    "    risk_weight_br: float = 12.0,\n",
    "    M_modes: int = 2,\n",
    "    seed: int = 0,\n",
    "    debug_one_rollout_per_pair: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      - build env\n",
    "      - init Π_R, Π_S\n",
    "      - repeat:\n",
    "          evaluate payoffs\n",
    "          solve NBS for x*\n",
    "          compute marginals σ_R, σ_S\n",
    "          compute BR policies and expand sets\n",
    "    \"\"\"\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),  # sensor centers near the two gaps\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10)\n",
    "    env.seed(seed)\n",
    "\n",
    "    if verbose:\n",
    "        log(\"\\n\" + \"=\" * 100)\n",
    "        log(\"[PIPELINE] Approach 2: Empirical POMDP Policy Game + Larson NBS + BR Expansion\")\n",
    "        log(\"=\" * 100)\n",
    "        log(f\"[PIPELINE] Grid: {grid.width}x{grid.height} start={grid.start} goal={grid.goal} obstacles={len(grid.obstacles)}\")\n",
    "        log(f\"[PIPELINE] Sensors: {sensor_cfg.sensors} radius={sensor_cfg.radius} base_p={sensor_cfg.base_p} hotspot_p={sensor_cfg.hotspot_p}\")\n",
    "        log(\"[PIPELINE] ASCII map (R=start, G=goal, S=sensors, #=wall):\")\n",
    "        print_grid_ascii(grid, sensor_cfg)\n",
    "        log(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    robot_policies = build_initial_robot_policies(env)\n",
    "    sensor_policies = build_initial_sensor_policies(M_modes)\n",
    "\n",
    "    if verbose:\n",
    "        log(\"[PIPELINE] Initial Robot Policies:\")\n",
    "        for i, p in enumerate(robot_policies):\n",
    "            log(f\"  R{i}: {p.name}\")\n",
    "        log(\"[PIPELINE] Initial Sensor Policies:\")\n",
    "        for j, p in enumerate(sensor_policies):\n",
    "            log(f\"  S{j}: {p.name}\")\n",
    "        log(\"\")\n",
    "\n",
    "    x_star: Optional[np.ndarray] = None\n",
    "    sigma_R: Optional[np.ndarray] = None\n",
    "    sigma_S: Optional[np.ndarray] = None\n",
    "    outer_history: List[Dict[str, float]] = []\n",
    "\n",
    "    for k in range(1, outer_iters + 1):\n",
    "        if verbose:\n",
    "            log(\"\\n\" + \"=\" * 100)\n",
    "            log(f\"[PIPELINE] OUTER ITERATION {k}/{outer_iters}\")\n",
    "            log(\"=\" * 100)\n",
    "\n",
    "        # 1) empirical payoffs\n",
    "        U_R, U_S, diag = evaluate_payoff_matrices(\n",
    "            env,\n",
    "            robot_policies,\n",
    "            sensor_policies,\n",
    "            M_modes=M_modes,\n",
    "            N_rollouts=rollouts_payoff,\n",
    "            base_seed=1000 + 100 * k,\n",
    "            debug_one_rollout_per_pair=debug_one_rollout_per_pair,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # 2) flatten payoffs into vectors over joint actions\n",
    "        m = len(robot_policies)\n",
    "        n = len(sensor_policies)\n",
    "        uR_vec = U_R.reshape(-1)  # length d=m*n\n",
    "        uS_vec = U_S.reshape(-1)\n",
    "\n",
    "        if verbose:\n",
    "            log(\"[PIPELINE] Flattened payoff vectors for NBS:\")\n",
    "            log(f\"[PIPELINE] m={m}, n={n}, d=m*n={m*n}\")\n",
    "            log(f\"[PIPELINE] uR range=[{uR_vec.min():.3f}, {uR_vec.max():.3f}]\")\n",
    "            log(f\"[PIPELINE] uS range=[{uS_vec.min():.3f}, {uS_vec.max():.3f}]\")\n",
    "            log(\"\")\n",
    "\n",
    "        # 3) solve NBS\n",
    "        nbs_res = solve_nbs_projected_gradient(uR_vec, uS_vec, max_iters=300, alpha=0.5, tol=1e-6, verbose=verbose)\n",
    "        x_star = nbs_res.x\n",
    "\n",
    "        # 4) compute marginals\n",
    "        X = joint_to_matrix(x_star, m, n)\n",
    "        sigma_R, sigma_S = marginals_from_joint(x_star, m, n)\n",
    "\n",
    "        if verbose:\n",
    "            log(\"[PIPELINE] NBS joint distribution x* reshaped as matrix X (rows=robot, cols=sensor):\")\n",
    "            log(np.array_str(X, precision=3, suppress_small=True))\n",
    "            log(f\"[PIPELINE] Robot marginal sigma_R: {sigma_R.round(3)}\")\n",
    "            log(f\"[PIPELINE] Sensor marginal sigma_S: {sigma_S.round(3)}\")\n",
    "            log(\"\")\n",
    "            log(\"[PIPELINE] Top-5 joint policy pairs (i,j) by x* probability:\")\n",
    "            top_pairs = np.argsort(-x_star)[:5]\n",
    "            for rank, flat_idx in enumerate(top_pairs, start=1):\n",
    "                i, j = np.unravel_index(int(flat_idx), (m, n))\n",
    "                log(f\"  #{rank}: (R{i}:{robot_policies[i].name}, S{j}:{sensor_policies[j].name}) prob={X[i,j]:.4f}\")\n",
    "            log(\"\")\n",
    "\n",
    "        # 5) robot BR to sigma_S\n",
    "        br_robot = robot_best_response_from_sigmaS(\n",
    "            env,\n",
    "            sensor_policies,\n",
    "            sigma_S,\n",
    "            existing_robot_policies=robot_policies,\n",
    "            M_modes=M_modes,\n",
    "            risk_weight=risk_weight_br,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        if br_robot not in robot_policies:\n",
    "            robot_policies.append(br_robot)\n",
    "            if verbose:\n",
    "                log(f\"[PIPELINE] Added NEW robot policy: {br_robot.name}\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                log(f\"[PIPELINE] Robot BR already in set: {br_robot.name}\")\n",
    "\n",
    "        # 6) sensor BR to sigma_R\n",
    "        br_sensor = sensor_best_response_from_sigmaR(\n",
    "            env,\n",
    "            robot_policies,\n",
    "            sigma_R,\n",
    "            candidate_modes=list(range(M_modes)),\n",
    "            M_modes=M_modes,\n",
    "            N_rollouts=rollouts_br,\n",
    "            base_seed=2000 + 100 * k,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        existing = find_sensor_policy_by_mode(sensor_policies, br_sensor.mode)\n",
    "        if existing is None:\n",
    "            sensor_policies.append(br_sensor)\n",
    "            if verbose:\n",
    "                log(f\"[PIPELINE] Added NEW sensor policy: {br_sensor.name}\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                log(f\"[PIPELINE] Sensor BR mode already present as '{existing.name}'. Not adding duplicate.\")\n",
    "\n",
    "        outer_history.append(\n",
    "            {\n",
    "                \"outer_iter\": float(k),\n",
    "                \"num_robot\": float(len(robot_policies)),\n",
    "                \"num_sensor\": float(len(sensor_policies)),\n",
    "                \"max_joint_prob\": float(np.max(x_star)),\n",
    "                \"nbs_obj_last\": float(nbs_res.history[-1][\"obj\"]) if nbs_res.history else float(\"nan\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        log(\"\\n\" + \"=\" * 100)\n",
    "        log(\"[PIPELINE] FINISHED OUTER LOOP\")\n",
    "        log(\"=\" * 100)\n",
    "        log(f\"[PIPELINE] Final |Pi_R|={len(robot_policies)}, |Pi_S|={len(sensor_policies)}\")\n",
    "        log(\"[PIPELINE] Final Robot Policies:\")\n",
    "        for i, p in enumerate(robot_policies):\n",
    "            log(f\"  R{i}: {p.name}\")\n",
    "        log(\"[PIPELINE] Final Sensor Policies:\")\n",
    "        for j, p in enumerate(sensor_policies):\n",
    "            log(f\"  S{j}: {p.name}\")\n",
    "        log(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"env\": env,\n",
    "        \"robot_policies\": robot_policies,\n",
    "        \"sensor_policies\": sensor_policies,\n",
    "        \"x_star\": x_star,\n",
    "        \"sigma_R\": sigma_R,\n",
    "        \"sigma_S\": sigma_S,\n",
    "        \"outer_history\": outer_history,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Final stage: MultiNash-PF-like multimodal trajectory demo (runnable stand-in)\n",
    "# =============================================================================\n",
    "\n",
    "def grid_path_to_continuous(path: List[Tuple[int, int]]) -> np.ndarray:\n",
    "    \"\"\"Convert grid cells to continuous 2D points.\"\"\"\n",
    "    return np.array([[float(x), float(y)] for (x, y) in path], dtype=float)\n",
    "\n",
    "\n",
    "def discrete_frechet(P: np.ndarray, Q: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Discrete Fréchet distance between two point sequences P and Q.\n",
    "    Used for clustering trajectories into modes.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    m, n = P.shape[0], Q.shape[0]\n",
    "    ca = np.full((m, n), -1.0, dtype=float)\n",
    "\n",
    "    def dist(i: int, j: int) -> float:\n",
    "        return float(np.linalg.norm(P[i] - Q[j]))\n",
    "\n",
    "    def rec(i: int, j: int) -> float:\n",
    "        if ca[i, j] > -0.5:\n",
    "            return float(ca[i, j])\n",
    "        if i == 0 and j == 0:\n",
    "            ca[i, j] = dist(0, 0)\n",
    "        elif i > 0 and j == 0:\n",
    "            ca[i, j] = max(rec(i - 1, 0), dist(i, 0))\n",
    "        elif i == 0 and j > 0:\n",
    "            ca[i, j] = max(rec(0, j - 1), dist(0, j))\n",
    "        else:\n",
    "            ca[i, j] = max(min(rec(i - 1, j), rec(i - 1, j - 1), rec(i, j - 1)), dist(i, j))\n",
    "        return float(ca[i, j])\n",
    "\n",
    "    return rec(m - 1, n - 1)\n",
    "\n",
    "\n",
    "def smooth_trajectory(\n",
    "    traj: np.ndarray,\n",
    "    ref: np.ndarray,\n",
    "    risk_map: np.ndarray,\n",
    "    w_track: float = 1.0,\n",
    "    w_smooth: float = 0.3,\n",
    "    w_risk: float = 5.0,\n",
    "    iters: int = 60,\n",
    "    step: float = 0.25,\n",
    "    verbose: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cheap local refinement (stand-in for IPOPT):\n",
    "      - endpoint-fixed gradient-like update combining:\n",
    "          tracking term\n",
    "          smoothness term\n",
    "          risk term\n",
    "    \"\"\"\n",
    "    traj = traj.copy()\n",
    "    L = traj.shape[0]\n",
    "    H, W = risk_map.shape\n",
    "\n",
    "    def risk_at(p: np.ndarray) -> float:\n",
    "        x = int(round(float(p[0])))\n",
    "        y = int(round(float(p[1])))\n",
    "        x = max(0, min(W - 1, x))\n",
    "        y = max(0, min(H - 1, y))\n",
    "        r = risk_map[y, x]\n",
    "        return 1.0 if (not np.isfinite(r)) else float(r)\n",
    "\n",
    "    for it in range(iters):\n",
    "        for t in range(1, L - 1):\n",
    "            # smoothness: pull toward midpoint of neighbors\n",
    "            mid = 0.5 * (traj[t - 1] + traj[t + 1])\n",
    "            grad_smooth = traj[t] - mid\n",
    "\n",
    "            # tracking: pull toward reference\n",
    "            grad_track = traj[t] - ref[t]\n",
    "\n",
    "            # risk: approximate gradient by finite differences\n",
    "            eps = 1.0\n",
    "            r0 = risk_at(traj[t])\n",
    "            rx = (risk_at(traj[t] + np.array([eps, 0.0])) - r0) / eps\n",
    "            ry = (risk_at(traj[t] + np.array([0.0, eps])) - r0) / eps\n",
    "            grad_risk = np.array([rx, ry])\n",
    "\n",
    "            grad = w_smooth * grad_smooth + w_track * grad_track + w_risk * grad_risk\n",
    "            traj[t] = traj[t] - step * grad\n",
    "\n",
    "        if verbose and (it % 20 == 0 or it == iters - 1):\n",
    "            # approximate objective value for debugging\n",
    "            obj = 0.0\n",
    "            for k in range(L):\n",
    "                obj += w_track * float(np.sum((traj[k] - ref[k]) ** 2))\n",
    "                if k > 0:\n",
    "                    obj += w_smooth * float(np.sum((traj[k] - traj[k - 1]) ** 2))\n",
    "                obj += w_risk * risk_at(traj[k])\n",
    "            log(f\"[Smooth] iter={it} approx_obj={obj:.3f}\")\n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "def multimodal_trajectory_modes_demo(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_policies: List[RobotPolicy],\n",
    "    sensor_policies: List[SensorPolicy],\n",
    "    x_star: np.ndarray,\n",
    "    top_k_pairs: int = 4,\n",
    "    particles_per_pair: int = 20,\n",
    "    noise_std: float = 0.4,\n",
    "    cluster_threshold: float = 1.0,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runnable “MultiNash-PF-like” structure:\n",
    "      1) pick top-k joint pairs from x*\n",
    "      2) convert robot path to continuous ref trajectory\n",
    "      3) sample noisy particles around each ref\n",
    "      4) refine each particle (cheap smoothing)\n",
    "      5) cluster refined trajectories by Fréchet distance\n",
    "      6) report clusters as “modes”\n",
    "    \"\"\"\n",
    "    m = len(robot_policies)\n",
    "    n = len(sensor_policies)\n",
    "    X = joint_to_matrix(x_star, m, n)\n",
    "    sigma_R, sigma_S = marginals_from_joint(x_star, m, n)\n",
    "\n",
    "    # risk map under sigma_S (used in refinement objective)\n",
    "    risk_map = compute_expected_risk_map(env, sensor_policies, sigma_S, M_modes=len(env.sensor_cfg.sensors), verbose=verbose)\n",
    "\n",
    "    # top-k joint pairs\n",
    "    flat_idx = np.argsort(-x_star)[:top_k_pairs]\n",
    "    pairs = [np.unravel_index(int(idx), (m, n)) for idx in flat_idx]\n",
    "\n",
    "    if verbose:\n",
    "        log(\"\\n\" + \"=\" * 100)\n",
    "        log(\"[TrajectoryModes] MultiNash-PF-like demo: sample -> refine -> cluster\")\n",
    "        log(\"=\" * 100)\n",
    "        for rank, (i, j) in enumerate(pairs, start=1):\n",
    "            log(f\"[TrajectoryModes] Top#{rank}: (R{i}:{robot_policies[i].name}, S{j}:{sensor_policies[j].name}) prob={X[i,j]:.4f}\")\n",
    "        log(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    # build reference trajectories from path-based robot policies\n",
    "    refs: List[np.ndarray] = []\n",
    "    ref_meta: List[Tuple[int, int, str, str, float]] = []\n",
    "    for (i, j) in pairs:\n",
    "        rp = robot_policies[i]\n",
    "        if isinstance(rp, FixedPathPolicy):\n",
    "            ref = grid_path_to_continuous(rp.path)\n",
    "            refs.append(ref)\n",
    "            ref_meta.append((i, j, rp.name, sensor_policies[j].name, float(X[i, j])))\n",
    "\n",
    "    if not refs:\n",
    "        raise RuntimeError(\"No FixedPathPolicy among top pairs; cannot build trajectories.\")\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    all_trajs: List[np.ndarray] = []\n",
    "    all_scores: List[float] = []\n",
    "    all_origin: List[int] = []\n",
    "\n",
    "    H, W = risk_map.shape\n",
    "\n",
    "    def risk_at(p: np.ndarray) -> float:\n",
    "        x = int(round(float(p[0])))\n",
    "        y = int(round(float(p[1])))\n",
    "        x = max(0, min(W - 1, x))\n",
    "        y = max(0, min(H - 1, y))\n",
    "        r = risk_map[y, x]\n",
    "        return 1.0 if (not np.isfinite(r)) else float(r)\n",
    "\n",
    "    def traj_objective(traj: np.ndarray, ref: np.ndarray) -> float:\n",
    "        w_track, w_smooth, w_risk = 1.0, 0.3, 5.0\n",
    "        obj = 0.0\n",
    "        for t in range(traj.shape[0]):\n",
    "            obj += w_track * float(np.sum((traj[t] - ref[t]) ** 2))\n",
    "            if t > 0:\n",
    "                obj += w_smooth * float(np.sum((traj[t] - traj[t - 1]) ** 2))\n",
    "            obj += w_risk * risk_at(traj[t])\n",
    "        return float(obj)\n",
    "\n",
    "    if verbose:\n",
    "        log(\"[TrajectoryModes] Sampling and refining particles...\")\n",
    "        log(f\"[TrajectoryModes] particles_per_pair={particles_per_pair}, noise_std={noise_std}, cluster_threshold={cluster_threshold}\")\n",
    "        log(\"\")\n",
    "\n",
    "    for idx_ref, ref in enumerate(refs):\n",
    "        i, j, rn, sn, prob = ref_meta[idx_ref]\n",
    "        if verbose:\n",
    "            log(f\"[TrajectoryModes] Ref#{idx_ref}: len={len(ref)} from (R{i}:{rn}, S{j}:{sn}) prob={prob:.4f}\")\n",
    "\n",
    "        for _ in range(particles_per_pair):\n",
    "            noise = rng.normal(0.0, noise_std, size=ref.shape)\n",
    "            traj0 = ref + noise\n",
    "            traj0[0] = ref[0]\n",
    "            traj0[-1] = ref[-1]  # endpoints fixed\n",
    "\n",
    "            traj1 = smooth_trajectory(traj0, ref, risk_map, iters=60, verbose=False)\n",
    "            score = traj_objective(traj1, ref)\n",
    "\n",
    "            all_trajs.append(traj1)\n",
    "            all_scores.append(score)\n",
    "            all_origin.append(idx_ref)\n",
    "\n",
    "    if verbose:\n",
    "        log(\"\\n[TrajectoryModes] Refinement complete.\")\n",
    "        log(f\"[TrajectoryModes] Total refined trajectories = {len(all_trajs)}\")\n",
    "        log(f\"[TrajectoryModes] Score stats: min={min(all_scores):.3f}, mean={statistics.mean(all_scores):.3f}, max={max(all_scores):.3f}\")\n",
    "        log(\"\")\n",
    "\n",
    "    # Cluster by Fréchet distance (greedy, best-first)\n",
    "    order = list(np.argsort(all_scores))\n",
    "    clusters: List[List[int]] = []\n",
    "    reps: List[int] = []\n",
    "\n",
    "    def add_to_clusters(idx: int) -> None:\n",
    "        for c_idx, rep_idx in enumerate(reps):\n",
    "            d = discrete_frechet(all_trajs[idx], all_trajs[rep_idx])\n",
    "            if d <= cluster_threshold:\n",
    "                clusters[c_idx].append(idx)\n",
    "                return\n",
    "        clusters.append([idx])\n",
    "        reps.append(idx)\n",
    "\n",
    "    for idx in order:\n",
    "        add_to_clusters(int(idx))\n",
    "\n",
    "    if verbose:\n",
    "        log(\"#\" * 80)\n",
    "        log(f\"[TrajectoryModes] Clustering complete: found {len(clusters)} modes (clusters)\")\n",
    "        for c_idx, members in enumerate(clusters):\n",
    "            best = min(members, key=lambda ii: all_scores[ii])\n",
    "            log(f\"  Mode{c_idx}: size={len(members)} best_score={all_scores[best]:.3f} origin_ref={all_origin[best]}\")\n",
    "        log(\"#\" * 80 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"risk_map\": risk_map,\n",
    "        \"ref_meta\": ref_meta,\n",
    "        \"refs\": refs,\n",
    "        \"all_trajs\": all_trajs,\n",
    "        \"all_scores\": all_scores,\n",
    "        \"clusters\": clusters,\n",
    "        \"reps\": reps,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main\n",
    "# =============================================================================\n",
    "\n",
    "def main(argv=None) -> None:  # <-- accept argv for notebooks\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--outer-iters\", type=int, default=3)\n",
    "    parser.add_argument(\"--rollouts-payoff\", type=int, default=20)\n",
    "    parser.add_argument(\"--rollouts-br\", type=int, default=30)\n",
    "    parser.add_argument(\"--risk-weight-br\", type=float, default=12.0)\n",
    "    parser.add_argument(\"--m-modes\", type=int, default=2)\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\"--debug-one-rollout-per-pair\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no-trajectory-demo\", action=\"store_true\")\n",
    "    parser.add_argument(\"--traj-top-k\", type=int, default=4)\n",
    "    parser.add_argument(\"--traj-particles-per-pair\", type=int, default=20)\n",
    "    parser.add_argument(\"--traj-noise-std\", type=float, default=0.4)\n",
    "    parser.add_argument(\"--traj-cluster-threshold\", type=float, default=1.0)\n",
    "\n",
    "    # <-- ignore ipykernel-injected args like --f=...json\n",
    "    args, _unknown = parser.parse_known_args(args=argv)\n",
    "\n",
    "    res = run_approach2_pipeline(\n",
    "        verbose=True,\n",
    "        outer_iters=args.outer_iters,\n",
    "        rollouts_payoff=args.rollouts_payoff,\n",
    "        rollouts_br=args.rollouts_br,\n",
    "        risk_weight_br=args.risk_weight_br,\n",
    "        M_modes=args.m_modes,\n",
    "        seed=args.seed,\n",
    "        debug_one_rollout_per_pair=args.debug_one_rollout_per_pair,\n",
    "    )\n",
    "\n",
    "    if not args.no_trajectory_demo:\n",
    "        multimodal_trajectory_modes_demo(\n",
    "            res[\"env\"],\n",
    "            res[\"robot_policies\"],\n",
    "            res[\"sensor_policies\"],\n",
    "            res[\"x_star\"],\n",
    "            top_k_pairs=args.traj_top_k,\n",
    "            particles_per_pair=args.traj_particles_per_pair,\n",
    "            noise_std=args.traj_noise_std,\n",
    "            cluster_threshold=args.traj_cluster_threshold,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "# In notebooks, avoid auto-running on cell execution\n",
    "if __name__ == \"__main__\" and (\"ipykernel\" not in sys.modules):\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871d5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"approach2_robust.py\n",
    "\n",
    "Robust, low-noise logging version of Approach 2.\n",
    "\n",
    "Key upgrades vs the earlier script:\n",
    "  1) Reproducible randomness everywhere (env + random policy) using numpy Generator.\n",
    "  2) Logging is stage-based with levels (INFO/DEBUG) to avoid massive logs.\n",
    "  3) More checks (shapes, simplex, NaNs, unreachable paths, invalid modes).\n",
    "  4) NBS supports optional entropy regularization (keeps x* mixed if desired).\n",
    "  5) TrajectoryModes demo ignores zero-prob pairs by default and can weight sampling by x*.\n",
    "\n",
    "Run:\n",
    "  python approach2_robust.py --outer-iters 3 --rollouts-payoff 20 --rollouts-br 30 --log-level INFO\n",
    "\n",
    "If you want mixed x* (useful for “mode recovery”):\n",
    "  python approach2_robust.py --entropy-tau 0.02\n",
    "\n",
    "If you want to print one detailed rollout occasionally:\n",
    "  python approach2_robust.py --debug-rollout-pair 2,1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import heapq\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Minimal stage-based logger with levels.\"\"\"\n",
    "\n",
    "    LEVELS = {\"QUIET\": 0, \"INFO\": 1, \"DEBUG\": 2}\n",
    "\n",
    "    def __init__(self, level: str = \"INFO\"):\n",
    "        level = level.upper()\n",
    "        if level not in self.LEVELS:\n",
    "            raise ValueError(f\"Unknown log level: {level}. Use QUIET/INFO/DEBUG\")\n",
    "        self.level = level\n",
    "        self.k = self.LEVELS[level]\n",
    "\n",
    "    def banner(self, title: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(title)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "    def info(self, msg: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(msg)\n",
    "\n",
    "    def debug(self, msg: str) -> None:\n",
    "        if self.k >= 2:\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Types\n",
    "# =============================================================================\n",
    "\n",
    "Action = Tuple[int, int]  # (dx, dy)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Environment (grid + hidden sensor mode + noisy alarm observation)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GridConfig:\n",
    "    width: int\n",
    "    height: int\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    obstacles: frozenset  # set[(x,y)]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SensorConfig:\n",
    "    sensors: Tuple[Tuple[int, int], ...]  # centers\n",
    "    radius: int\n",
    "    base_p: float\n",
    "    hotspot_p: float\n",
    "\n",
    "\n",
    "class GridWorldStealthEnv:\n",
    "    \"\"\"Grid world where sensor chooses a mode selecting an active hotspot sensor.\"\"\"\n",
    "\n",
    "    def __init__(self, grid: GridConfig, sensor_cfg: SensorConfig, fp: float = 0.05, fn: float = 0.10, seed: int = 0):\n",
    "        self.grid = grid\n",
    "        self.sensor_cfg = sensor_cfg\n",
    "        self.fp = float(fp)\n",
    "        self.fn = float(fn)\n",
    "        if not (0.0 <= self.fp <= 1.0 and 0.0 <= self.fn <= 1.0):\n",
    "            raise ValueError(\"fp and fn must be in [0,1].\")\n",
    "        if not (0.0 <= sensor_cfg.base_p <= 1.0 and 0.0 <= sensor_cfg.hotspot_p <= 1.0):\n",
    "            raise ValueError(\"base_p and hotspot_p must be in [0,1].\")\n",
    "        if sensor_cfg.radius < 0:\n",
    "            raise ValueError(\"radius must be >= 0\")\n",
    "        if len(sensor_cfg.sensors) == 0:\n",
    "            raise ValueError(\"Need at least one sensor center.\")\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.reset(sensor_mode=0)\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    def reset(self, sensor_mode: int = 0) -> Dict[str, Any]:\n",
    "        self.t = 0\n",
    "        self.pos = self.grid.start\n",
    "        self.sensor_mode = int(sensor_mode)\n",
    "        self.detected = False\n",
    "        self.total_true_risk = 0.0\n",
    "        return {\"pos\": self.pos, \"t\": self.t}\n",
    "\n",
    "    def in_bounds(self, p: Tuple[int, int]) -> bool:\n",
    "        x, y = p\n",
    "        return 0 <= x < self.grid.width and 0 <= y < self.grid.height\n",
    "\n",
    "    def is_free(self, p: Tuple[int, int]) -> bool:\n",
    "        return self.in_bounds(p) and (p not in self.grid.obstacles)\n",
    "\n",
    "    def true_detection_prob(self, p: Tuple[int, int], mode: int) -> float:\n",
    "        \"\"\"p_true(p | mode).\"\"\"\n",
    "        if not (0 <= mode < len(self.sensor_cfg.sensors)):\n",
    "            raise ValueError(f\"mode {mode} out of range\")\n",
    "        base = self.sensor_cfg.base_p\n",
    "        hot = self.sensor_cfg.hotspot_p\n",
    "        sx, sy = self.sensor_cfg.sensors[mode]\n",
    "        x, y = p\n",
    "        d = abs(x - sx) + abs(y - sy)\n",
    "        return hot if d <= self.sensor_cfg.radius else base\n",
    "\n",
    "    def observation_prob(self, alarm: int, p_true: float) -> float:\n",
    "        \"\"\"P(o=alarm | p_true) with FP/FN noise.\"\"\"\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        if alarm == 1:\n",
    "            return p_alarm\n",
    "        if alarm == 0:\n",
    "            return 1.0 - p_alarm\n",
    "        raise ValueError(\"alarm must be 0 or 1\")\n",
    "\n",
    "    def step(self, a: Action) -> Dict[str, Any]:\n",
    "        if self.detected:\n",
    "            return {\"pos\": self.pos, \"t\": self.t, \"alarm\": 1, \"p_true\": 1.0, \"detected\": True, \"done\": True}\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        nx = self.pos[0] + int(a[0])\n",
    "        ny = self.pos[1] + int(a[1])\n",
    "        np_ = (nx, ny)\n",
    "        if self.is_free(np_):\n",
    "            self.pos = np_\n",
    "\n",
    "        p_true = self.true_detection_prob(self.pos, self.sensor_mode)\n",
    "        self.total_true_risk += p_true\n",
    "\n",
    "        # detection event\n",
    "        if self.rng.random() < p_true:\n",
    "            self.detected = True\n",
    "\n",
    "        # noisy alarm\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        alarm = 1 if (self.rng.random() < p_alarm) else 0\n",
    "\n",
    "        done = self.detected or (self.pos == self.grid.goal) or (self.t >= 200)\n",
    "        return {\"pos\": self.pos, \"t\": self.t, \"alarm\": alarm, \"p_true\": p_true, \"detected\": self.detected, \"done\": done}\n",
    "\n",
    "\n",
    "def build_two_corridor_grid(width: int = 15, height: int = 9) -> GridConfig:\n",
    "    obstacles = set()\n",
    "    wall_x = width // 2\n",
    "    gap_ys = {2, 6}\n",
    "    for y in range(height):\n",
    "        if y not in gap_ys:\n",
    "            obstacles.add((wall_x, y))\n",
    "\n",
    "    start = (1, height - 2)\n",
    "    goal = (width - 2, 1)\n",
    "    if start in obstacles or goal in obstacles:\n",
    "        raise RuntimeError(\"Start/goal blocked unexpectedly.\")\n",
    "\n",
    "    return GridConfig(width=width, height=height, start=start, goal=goal, obstacles=frozenset(obstacles))\n",
    "\n",
    "\n",
    "def print_grid_ascii(grid: GridConfig, sensor_cfg: SensorConfig) -> None:\n",
    "    W, H = grid.width, grid.height\n",
    "    obs = set(grid.obstacles)\n",
    "    sens = set(sensor_cfg.sensors)\n",
    "    for y in range(H):\n",
    "        row = []\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p == grid.start:\n",
    "                row.append(\"R\")\n",
    "            elif p == grid.goal:\n",
    "                row.append(\"G\")\n",
    "            elif p in sens:\n",
    "                row.append(\"S\")\n",
    "            elif p in obs:\n",
    "                row.append(\"#\")\n",
    "            else:\n",
    "                row.append(\".\")\n",
    "        print(\"\".join(row))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Belief over sensor modes (exact Bayes filter)\n",
    "# =============================================================================\n",
    "\n",
    "class ModeBelief:\n",
    "    def __init__(self, M: int, init: Optional[np.ndarray] = None):\n",
    "        self.M = int(M)\n",
    "        if self.M <= 0:\n",
    "            raise ValueError(\"M must be >= 1\")\n",
    "        if init is None:\n",
    "            self.b = np.full(self.M, 1.0 / self.M)\n",
    "        else:\n",
    "            init = np.asarray(init, dtype=float).reshape(-1)\n",
    "            if init.shape != (self.M,):\n",
    "                raise ValueError(\"init shape mismatch\")\n",
    "            if np.any(init < 0):\n",
    "                raise ValueError(\"init must be nonnegative\")\n",
    "            s = float(init.sum())\n",
    "            self.b = init / s if s > 0 else np.full(self.M, 1.0 / self.M)\n",
    "\n",
    "    def update(self, env: GridWorldStealthEnv, alarm: int, pos: Tuple[int, int], eps: float = 1e-12) -> None:\n",
    "        like = np.zeros(self.M, dtype=float)\n",
    "        for m in range(self.M):\n",
    "            p_true = env.true_detection_prob(pos, m)\n",
    "            like[m] = env.observation_prob(alarm, p_true)\n",
    "        post = self.b * like\n",
    "        Z = float(post.sum())\n",
    "        if (not np.isfinite(Z)) or Z < eps:\n",
    "            # robust fallback\n",
    "            return\n",
    "        self.b = post / Z\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policies\n",
    "# =============================================================================\n",
    "\n",
    "class RobotPolicy:\n",
    "    name: str = \"RobotPolicy\"\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedPathPolicy(RobotPolicy):\n",
    "    def __init__(self, path: List[Tuple[int, int]], name: str):\n",
    "        if len(path) < 2:\n",
    "            raise ValueError(\"Path must have >=2 states\")\n",
    "        self.path = list(path)\n",
    "        self.name = str(name)\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        try:\n",
    "            self._idx = self.path.index(start_pos)\n",
    "        except ValueError:\n",
    "            self._idx = 0\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        cur = env.pos\n",
    "        if self._idx >= len(self.path) - 1:\n",
    "            return (0, 0)\n",
    "        if cur != self.path[self._idx]:\n",
    "            # resync if possible\n",
    "            try:\n",
    "                self._idx = self.path.index(cur, self._idx)\n",
    "            except ValueError:\n",
    "                return (0, 0)\n",
    "        nxt = self.path[self._idx + 1]\n",
    "        dx = int(np.clip(nxt[0] - cur[0], -1, 1))\n",
    "        dy = int(np.clip(nxt[1] - cur[1], -1, 1))\n",
    "        self._idx += 1\n",
    "        return (dx, dy)\n",
    "\n",
    "\n",
    "class RandomPolicy(RobotPolicy):\n",
    "    \"\"\"Reproducible random policy: uses env.rng (NOT global random).\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"R_Random\"):\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        candidates: List[Action] = []\n",
    "        x, y = env.pos\n",
    "        for a in [(1, 0), (-1, 0), (0, 1), (0, -1), (0, 0)]:\n",
    "            np_ = (x + a[0], y + a[1])\n",
    "            if env.is_free(np_):\n",
    "                candidates.append(a)\n",
    "        if not candidates:\n",
    "            return (0, 0)\n",
    "        idx = int(env.rng.integers(0, len(candidates)))\n",
    "        return candidates[idx]\n",
    "\n",
    "\n",
    "class SensorPolicy:\n",
    "    name: str = \"SensorPolicy\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedModeSensorPolicy(SensorPolicy):\n",
    "    def __init__(self, mode: int, name: Optional[str] = None):\n",
    "        self.mode = int(mode)\n",
    "        self.name = name or f\"S_Mode{mode}\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return self.mode\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# A* (used by robot BR)\n",
    "# =============================================================================\n",
    "\n",
    "def astar_path(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    max_expansions: int = 250_000,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    if start == goal:\n",
    "        return [start]\n",
    "\n",
    "    def h(p: Tuple[int, int]) -> float:\n",
    "        return abs(p[0] - goal[0]) + abs(p[1] - goal[1])\n",
    "\n",
    "    def neighbors(p: Tuple[int, int]):\n",
    "        x, y = p\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            np_ = (x + dx, y + dy)\n",
    "            if 0 <= np_[0] < grid.width and 0 <= np_[1] < grid.height and np_ not in grid.obstacles:\n",
    "                yield np_\n",
    "\n",
    "    open_heap: List[Tuple[float, float, Tuple[int, int]]] = []\n",
    "    heapq.heappush(open_heap, (h(start), 0.0, start))\n",
    "\n",
    "    came: Dict[Tuple[int, int], Optional[Tuple[int, int]]] = {start: None}\n",
    "    gscore: Dict[Tuple[int, int], float] = {start: 0.0}\n",
    "\n",
    "    expansions = 0\n",
    "    while open_heap:\n",
    "        f, g, cur = heapq.heappop(open_heap)\n",
    "        expansions += 1\n",
    "        if cur == goal:\n",
    "            path: List[Tuple[int, int]] = []\n",
    "            while cur is not None:\n",
    "                path.append(cur)\n",
    "                cur = came[cur]\n",
    "            path.reverse()\n",
    "            return path\n",
    "        if expansions > max_expansions:\n",
    "            raise RuntimeError(\"A* exceeded max expansions\")\n",
    "\n",
    "        for nb in neighbors(cur):\n",
    "            tentative = gscore[cur] + float(step_cost(cur, nb))\n",
    "            if (nb not in gscore) or (tentative < gscore[nb] - 1e-12):\n",
    "                gscore[nb] = tentative\n",
    "                came[nb] = cur\n",
    "                heapq.heappush(open_heap, (tentative + h(nb), tentative, nb))\n",
    "\n",
    "    raise RuntimeError(\"A* failed: unreachable goal\")\n",
    "\n",
    "\n",
    "def astar_via_waypoint(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    waypoint: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    ") -> List[Tuple[int, int]]:\n",
    "    p1 = astar_path(grid, start, waypoint, step_cost)\n",
    "    p2 = astar_path(grid, waypoint, goal, step_cost)\n",
    "    return p1[:-1] + p2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rollouts + payoffs\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStats:\n",
    "    steps: int\n",
    "    reached_goal: bool\n",
    "    detected: bool\n",
    "    total_true_risk: float\n",
    "    U_R: float\n",
    "    U_S: float\n",
    "\n",
    "\n",
    "def rollout_episode(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    seed: int,\n",
    "    max_steps: int = 200,\n",
    "    lambda_risk: float = 1.0,\n",
    "    det_penalty: float = 50.0,\n",
    "    sensor_energy_per_step: float = 0.2,\n",
    "    step_debug: bool = False,\n",
    ") -> EpisodeStats:\n",
    "    \"\"\"One episode with reproducible seeding; optional step_debug prints only when enabled.\"\"\"\n",
    "\n",
    "    env.seed(seed)\n",
    "    sensor.reset()\n",
    "    env.reset(sensor_mode=sensor.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)\n",
    "    robot.reset(env.pos)\n",
    "\n",
    "    total_risk = 0.0\n",
    "    last_alarm: Optional[int] = None\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.sensor_mode = sensor.select_mode(env.t)\n",
    "        a = robot.act(env, belief, last_alarm)\n",
    "        out = env.step(a)\n",
    "\n",
    "        total_risk += out[\"p_true\"]\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "        last_alarm = out[\"alarm\"]\n",
    "\n",
    "        if step_debug:\n",
    "            print(f\"[Step] t={out['t']:3d} pos={out['pos']} a={a} p_true={out['p_true']:.3f} alarm={out['alarm']} det={out['detected']} done={out['done']} b={belief.b.round(3)}\")\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = bool(env.detected)\n",
    "    steps = int(env.t)\n",
    "\n",
    "    cost_R = steps + lambda_risk * total_risk + (det_penalty if detected else 0.0)\n",
    "    U_R = -float(cost_R)\n",
    "\n",
    "    U_S = float((det_penalty if detected else 0.0) + lambda_risk * total_risk - sensor_energy_per_step * steps)\n",
    "\n",
    "    return EpisodeStats(steps=steps, reached_goal=reached_goal, detected=detected, total_true_risk=float(total_risk), U_R=U_R, U_S=U_S)\n",
    "\n",
    "\n",
    "def evaluate_payoffs(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    debug_rollout_pair: Optional[Tuple[int, int]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[int, int], Dict[str, float]]]:\n",
    "    m, n = len(robots), len(sensors)\n",
    "    U_R = np.zeros((m, n), dtype=float)\n",
    "    U_S = np.zeros((m, n), dtype=float)\n",
    "    diag: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    log.info(f\"[Eval] Estimating payoffs: m={m}, n={n}, rollouts={rollouts}, base_seed={base_seed}\")\n",
    "\n",
    "    for i, rpol in enumerate(robots):\n",
    "        for j, spol in enumerate(sensors):\n",
    "            step_debug = (debug_rollout_pair == (i, j))\n",
    "            r_list: List[float] = []\n",
    "            s_list: List[float] = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            steps_list: List[int] = []\n",
    "            risk_list: List[float] = []\n",
    "\n",
    "            for k in range(rollouts):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rpol, spol, M_modes=M_modes, seed=seed, step_debug=step_debug)\n",
    "                r_list.append(st.U_R)\n",
    "                s_list.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "                steps_list.append(st.steps)\n",
    "                risk_list.append(st.total_true_risk)\n",
    "\n",
    "                # If we printed step-debug once, don't do it for all rollouts\n",
    "                if step_debug:\n",
    "                    step_debug = False\n",
    "\n",
    "            U_R[i, j] = float(np.mean(r_list))\n",
    "            U_S[i, j] = float(np.mean(s_list))\n",
    "\n",
    "            diag[(i, j)] = {\n",
    "                \"det_rate\": det / rollouts,\n",
    "                \"goal_rate\": goal / rollouts,\n",
    "                \"mean_steps\": float(np.mean(steps_list)),\n",
    "                \"mean_risk\": float(np.mean(risk_list)),\n",
    "                \"std_UR\": float(np.std(r_list)),\n",
    "                \"std_US\": float(np.std(s_list)),\n",
    "            }\n",
    "\n",
    "    # Print compact summary table (INFO) — only key pairs\n",
    "    if log.k >= 1:\n",
    "        log.info(\"[Eval] Compact payoff summary (showing all pairs but one-line each):\")\n",
    "        for i, rpol in enumerate(robots):\n",
    "            for j, spol in enumerate(sensors):\n",
    "                d = diag[(i, j)]\n",
    "                log.info(\n",
    "                    f\"  (R{i}:{rpol.name}, S{j}:{spol.name}) \"\n",
    "                    f\"UR={U_R[i,j]:8.3f}±{d['std_UR']:.2f} | \"\n",
    "                    f\"US={U_S[i,j]:8.3f}±{d['std_US']:.2f} | \"\n",
    "                    f\"det%={100*d['det_rate']:5.1f} goal%={100*d['goal_rate']:5.1f} \"\n",
    "                    f\"steps={d['mean_steps']:.1f} risk={d['mean_risk']:.2f}\"\n",
    "                )\n",
    "\n",
    "    return U_R, U_S, diag\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NBS solver (with optional entropy regularization)\n",
    "# =============================================================================\n",
    "\n",
    "def project_simplex(v: np.ndarray, z: float = 1.0) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    if v.size == 0:\n",
    "        raise ValueError(\"Empty vector\")\n",
    "    if z <= 0:\n",
    "        raise ValueError(\"z must be > 0\")\n",
    "\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, v.size + 1) > (cssv - z))[0]\n",
    "    if rho.size == 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    rho = int(rho[-1])\n",
    "    theta = (cssv[rho] - z) / (rho + 1.0)\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    s = float(w.sum())\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    return w * (z / s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NBSResult:\n",
    "    x: np.ndarray\n",
    "    obj: float\n",
    "    gains: Tuple[float, float]\n",
    "    support: int\n",
    "\n",
    "\n",
    "def solve_nbs(\n",
    "    uR: np.ndarray,\n",
    "    uS: np.ndarray,\n",
    "    log: Logger,\n",
    "    max_iters: int = 400,\n",
    "    alpha: float = 0.5,\n",
    "    tol_l1: float = 1e-6,\n",
    "    kappa: float = 1e-6,\n",
    "    disagreement: str = \"minminus\",  # or \"uniform\"\n",
    "    entropy_tau: float = 0.0,\n",
    ") -> NBSResult:\n",
    "    \"\"\"Projected gradient ascent on:\n",
    "\n",
    "        f(x) = log(uR^T x - dR) + log(uS^T x - dS) + tau * H(x)\n",
    "\n",
    "    where H(x) = -sum x_i log x_i.\n",
    "\n",
    "    disagreement:\n",
    "      - minminus: dR=min(uR)-1, dS=min(uS)-1\n",
    "      - uniform : dR=uR^T unif, dS=uS^T unif  (much less degenerate)\n",
    "    \"\"\"\n",
    "\n",
    "    uR = np.asarray(uR, dtype=float).reshape(-1)\n",
    "    uS = np.asarray(uS, dtype=float).reshape(-1)\n",
    "    if uR.shape != uS.shape:\n",
    "        raise ValueError(\"uR and uS must have same shape\")\n",
    "    d = uR.size\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Need >=2 joint actions\")\n",
    "\n",
    "    unif = np.full(d, 1.0 / d)\n",
    "\n",
    "    disagreement = disagreement.lower().strip()\n",
    "    if disagreement == \"minminus\":\n",
    "        dR = float(np.min(uR) - 1.0)\n",
    "        dS = float(np.min(uS) - 1.0)\n",
    "    elif disagreement == \"uniform\":\n",
    "        # IMPORTANT: this is an *outside option* baseline, not a security level.\n",
    "        dR = float(uR @ unif)\n",
    "        dS = float(uS @ unif)\n",
    "    else:\n",
    "        raise ValueError(\"disagreement must be 'minminus' or 'uniform'\")\n",
    "\n",
    "    x = unif.copy()\n",
    "\n",
    "    def gains(xv: np.ndarray) -> Tuple[float, float]:\n",
    "        return float(uR @ xv - dR), float(uS @ xv - dS)\n",
    "\n",
    "    def entropy(xv: np.ndarray) -> float:\n",
    "        # stable entropy for simplex vector\n",
    "        xx = np.clip(xv, 1e-12, 1.0)\n",
    "        return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "    def obj(xv: np.ndarray) -> float:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return float(np.log(gR) + np.log(gS) + entropy_tau * entropy(xv))\n",
    "\n",
    "    def grad(xv: np.ndarray) -> np.ndarray:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        g = (uR / gR) + (uS / gS)\n",
    "        if entropy_tau > 0:\n",
    "            # grad of tau * (-sum x log x) is tau * (-(log x + 1))\n",
    "            xx = np.clip(xv, 1e-12, 1.0)\n",
    "            g += entropy_tau * (-(np.log(xx) + 1.0))\n",
    "        return g\n",
    "\n",
    "    last = obj(x)\n",
    "\n",
    "    # Keep logs short: only print start + end + occasional progress if DEBUG.\n",
    "    log.info(f\"[NBS] d={d} disagreement=({dR:.3f},{dS:.3f}) entropy_tau={entropy_tau:.3g}\")\n",
    "\n",
    "    for t in range(1, max_iters + 1):\n",
    "        g = grad(x)\n",
    "        a = alpha\n",
    "        improved = False\n",
    "        for _ in range(30):\n",
    "            x_new = project_simplex(x + a * g)\n",
    "            new_obj = obj(x_new)\n",
    "            if new_obj >= last - 1e-12:\n",
    "                improved = True\n",
    "                break\n",
    "            a *= 0.5\n",
    "            if a < 1e-6:\n",
    "                break\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "        delta = float(np.linalg.norm(x_new - x, ord=1))\n",
    "        x = x_new\n",
    "        last = new_obj\n",
    "\n",
    "        if log.k >= 2 and (t <= 5 or t % 25 == 0):\n",
    "            top = np.argsort(-x)[:5]\n",
    "            top_str = \", \".join([f\"{i}:{x[i]:.3f}\" for i in top])\n",
    "            gR, gS = gains(x)\n",
    "            log.debug(f\"[NBS][it={t:3d}] obj={last:.6f} gains=({gR:.3f},{gS:.3f}) L1={delta:.2e} top={top_str}\")\n",
    "\n",
    "        if delta < tol_l1:\n",
    "            break\n",
    "\n",
    "    gR, gS = gains(x)\n",
    "    support = int(np.sum(x > 1e-6))\n",
    "    log.info(f\"[NBS] done: obj={last:.6f} gains=({gR:.3f},{gS:.3f}) support={support}/{d}\")\n",
    "\n",
    "    return NBSResult(x=x, obj=float(last), gains=(float(gR), float(gS)), support=support)\n",
    "\n",
    "\n",
    "def joint_to_matrix(x: np.ndarray, m: int, n: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size != m * n:\n",
    "        raise ValueError(\"x size mismatch\")\n",
    "    return x.reshape((m, n))\n",
    "\n",
    "\n",
    "def marginals_from_joint(x: np.ndarray, m: int, n: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X = joint_to_matrix(x, m, n)\n",
    "    sigma_R = X.sum(axis=1)\n",
    "    sigma_S = X.sum(axis=0)\n",
    "    if sigma_R.sum() > 0:\n",
    "        sigma_R = sigma_R / sigma_R.sum()\n",
    "    if sigma_S.sum() > 0:\n",
    "        sigma_S = sigma_S / sigma_S.sum()\n",
    "    return sigma_R, sigma_S\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Best responses\n",
    "# =============================================================================\n",
    "\n",
    "def compute_expected_risk_map(env: GridWorldStealthEnv, sensors: List[SensorPolicy], sigma_S: np.ndarray, M_modes: int) -> np.ndarray:\n",
    "    sigma_S = np.asarray(sigma_S, dtype=float).reshape(-1)\n",
    "    if sigma_S.size != len(sensors):\n",
    "        raise ValueError(\"sigma_S length mismatch\")\n",
    "\n",
    "    mode_probs = np.zeros(M_modes, dtype=float)\n",
    "    for j, sp in enumerate(sensors):\n",
    "        m = sp.select_mode(0)\n",
    "        if not (0 <= m < M_modes):\n",
    "            raise ValueError(f\"Sensor policy returned invalid mode {m}\")\n",
    "        mode_probs[m] += sigma_S[j]\n",
    "\n",
    "    if mode_probs.sum() > 0:\n",
    "        mode_probs = mode_probs / mode_probs.sum()\n",
    "\n",
    "    H, W = env.grid.height, env.grid.width\n",
    "    risk = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in env.grid.obstacles:\n",
    "                risk[y, x] = np.nan\n",
    "                continue\n",
    "            val = 0.0\n",
    "            for m in range(M_modes):\n",
    "                val += mode_probs[m] * env.true_detection_prob((x, y), m)\n",
    "            risk[y, x] = val\n",
    "\n",
    "    return risk\n",
    "\n",
    "\n",
    "def robot_best_response(env: GridWorldStealthEnv, sensors: List[SensorPolicy], sigma_S: np.ndarray, robots: List[RobotPolicy], M_modes: int, risk_weight: float, log: Logger) -> RobotPolicy:\n",
    "    risk = compute_expected_risk_map(env, sensors, sigma_S, M_modes=M_modes)\n",
    "\n",
    "    def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "        x, y = to\n",
    "        r = risk[y, x]\n",
    "        if not np.isfinite(r):\n",
    "            return 1e9\n",
    "        return 1.0 + risk_weight * float(r)\n",
    "\n",
    "    try:\n",
    "        path = astar_path(env.grid, env.grid.start, env.grid.goal, step_cost)\n",
    "    except Exception as e:\n",
    "        log.info(f\"[RobotBR] WARNING: A* failed: {e}. Returning existing shortest if present.\")\n",
    "        for p in robots:\n",
    "            if isinstance(p, FixedPathPolicy) and \"Shortest\" in p.name:\n",
    "                return p\n",
    "        return robots[0]\n",
    "\n",
    "    path_tuple = tuple(path)\n",
    "    for p in robots:\n",
    "        if isinstance(p, FixedPathPolicy) and tuple(p.path) == path_tuple:\n",
    "            log.info(f\"[RobotBR] BR path already exists: {p.name}\")\n",
    "            return p\n",
    "\n",
    "    newp = FixedPathPolicy(path, name=f\"R_BR_RiskAStar_w{risk_weight:.1f}_len{len(path)}\")\n",
    "    log.info(f\"[RobotBR] Added new robot policy: {newp.name}\")\n",
    "    return newp\n",
    "\n",
    "\n",
    "def sensor_best_response(env: GridWorldStealthEnv, robots: List[RobotPolicy], sigma_R: np.ndarray, candidate_modes: List[int], M_modes: int, rollouts: int, base_seed: int, log: Logger) -> FixedModeSensorPolicy:\n",
    "    sigma_R = np.asarray(sigma_R, dtype=float).reshape(-1)\n",
    "    if sigma_R.size != len(robots):\n",
    "        raise ValueError(\"sigma_R length mismatch\")\n",
    "\n",
    "    rng = np.random.default_rng(base_seed)\n",
    "\n",
    "    best_mode: Optional[int] = None\n",
    "    best_val = -1e18\n",
    "\n",
    "    for mode in candidate_modes:\n",
    "        if not (0 <= mode < M_modes):\n",
    "            continue\n",
    "        sp = FixedModeSensorPolicy(mode, name=f\"S_BR_Mode{mode}\")\n",
    "        vals: List[float] = []\n",
    "        for k in range(rollouts):\n",
    "            i = int(rng.choice(len(robots), p=sigma_R))\n",
    "            rp = robots[i]\n",
    "            seed = base_seed + 10000 * mode + k\n",
    "            st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed)\n",
    "            vals.append(st.U_S)\n",
    "        mean_u = float(np.mean(vals))\n",
    "        if log.k >= 2:\n",
    "            log.debug(f\"[SensorBR] mode={mode} E[US]={mean_u:.3f} std={float(np.std(vals)):.2f}\")\n",
    "        if mean_u > best_val:\n",
    "            best_val = mean_u\n",
    "            best_mode = mode\n",
    "\n",
    "    if best_mode is None:\n",
    "        raise RuntimeError(\"No valid sensor BR mode found\")\n",
    "\n",
    "    log.info(f\"[SensorBR] Best mode={best_mode} E[US]={best_val:.3f}\")\n",
    "    return FixedModeSensorPolicy(best_mode, name=f\"S_BR_Mode{best_mode}\")\n",
    "\n",
    "\n",
    "def find_sensor_by_mode(pols: List[SensorPolicy], mode: int) -> Optional[FixedModeSensorPolicy]:\n",
    "    for p in pols:\n",
    "        if isinstance(p, FixedModeSensorPolicy) and p.mode == mode:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_initial_policies(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "    ]\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Trajectory modes demo (reduced confusion)\n",
    "# =============================================================================\n",
    "\n",
    "def grid_path_to_continuous(path: List[Tuple[int, int]]) -> np.ndarray:\n",
    "    return np.array([[float(x), float(y)] for (x, y) in path], dtype=float)\n",
    "\n",
    "\n",
    "def discrete_frechet(P: np.ndarray, Q: np.ndarray) -> float:\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    m, n = P.shape[0], Q.shape[0]\n",
    "    ca = np.full((m, n), -1.0, dtype=float)\n",
    "\n",
    "    def dist(i: int, j: int) -> float:\n",
    "        return float(np.linalg.norm(P[i] - Q[j]))\n",
    "\n",
    "    def rec(i: int, j: int) -> float:\n",
    "        if ca[i, j] > -0.5:\n",
    "            return float(ca[i, j])\n",
    "        if i == 0 and j == 0:\n",
    "            ca[i, j] = dist(0, 0)\n",
    "        elif i > 0 and j == 0:\n",
    "            ca[i, j] = max(rec(i - 1, 0), dist(i, 0))\n",
    "        elif i == 0 and j > 0:\n",
    "            ca[i, j] = max(rec(0, j - 1), dist(0, j))\n",
    "        else:\n",
    "            ca[i, j] = max(min(rec(i - 1, j), rec(i - 1, j - 1), rec(i, j - 1)), dist(i, j))\n",
    "        return float(ca[i, j])\n",
    "\n",
    "    return rec(m - 1, n - 1)\n",
    "\n",
    "\n",
    "def smooth_trajectory(traj: np.ndarray, ref: np.ndarray, risk_map: np.ndarray, iters: int = 60, step: float = 0.25) -> np.ndarray:\n",
    "    traj = traj.copy()\n",
    "    L = traj.shape[0]\n",
    "    H, W = risk_map.shape\n",
    "\n",
    "    def risk_at(p: np.ndarray) -> float:\n",
    "        x = int(round(float(p[0])))\n",
    "        y = int(round(float(p[1])))\n",
    "        x = max(0, min(W - 1, x))\n",
    "        y = max(0, min(H - 1, y))\n",
    "        r = risk_map[y, x]\n",
    "        return 1.0 if (not np.isfinite(r)) else float(r)\n",
    "\n",
    "    w_track, w_smooth, w_risk = 1.0, 0.3, 5.0\n",
    "\n",
    "    for _ in range(iters):\n",
    "        for t in range(1, L - 1):\n",
    "            mid = 0.5 * (traj[t - 1] + traj[t + 1])\n",
    "            grad_smooth = traj[t] - mid\n",
    "            grad_track = traj[t] - ref[t]\n",
    "\n",
    "            eps = 1.0\n",
    "            r0 = risk_at(traj[t])\n",
    "            rx = (risk_at(traj[t] + np.array([eps, 0.0])) - r0) / eps\n",
    "            ry = (risk_at(traj[t] + np.array([0.0, eps])) - r0) / eps\n",
    "            grad_risk = np.array([rx, ry])\n",
    "\n",
    "            grad = w_smooth * grad_smooth + w_track * grad_track + w_risk * grad_risk\n",
    "            traj[t] = traj[t] - step * grad\n",
    "\n",
    "    # restore endpoints\n",
    "    traj[0] = ref[0]\n",
    "    traj[-1] = ref[-1]\n",
    "    return traj\n",
    "\n",
    "\n",
    "def trajectory_modes_demo(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    x_star: np.ndarray,\n",
    "    log: Logger,\n",
    "    top_k_pairs: int = 4,\n",
    "    particles_per_pair: int = 20,\n",
    "    noise_std: float = 0.4,\n",
    "    cluster_threshold: float = 1.0,\n",
    "    min_prob: float = 1e-6,\n",
    ") -> None:\n",
    "    m, n = len(robots), len(sensors)\n",
    "    X = joint_to_matrix(x_star, m, n)\n",
    "    sigma_R, sigma_S = marginals_from_joint(x_star, m, n)\n",
    "\n",
    "    risk_map = compute_expected_risk_map(env, sensors, sigma_S, M_modes=len(env.sensor_cfg.sensors))\n",
    "\n",
    "    # ONLY take positive-prob pairs (avoid the “prob=0 refs” confusion)\n",
    "    flat = np.argsort(-x_star)\n",
    "    pairs: List[Tuple[int, int]] = []\n",
    "    for idx in flat:\n",
    "        if x_star[idx] < min_prob:\n",
    "            break\n",
    "        pairs.append(tuple(np.unravel_index(int(idx), (m, n))))\n",
    "        if len(pairs) >= top_k_pairs:\n",
    "            break\n",
    "\n",
    "    if not pairs:\n",
    "        log.info(\"[TrajModes] No pairs above min_prob; skipping trajectory demo.\")\n",
    "        return\n",
    "\n",
    "    log.banner(\"[TrajModes] Trajectory modes demo (positive-prob pairs only)\")\n",
    "    for rank, (i, j) in enumerate(pairs, start=1):\n",
    "        log.info(f\"  Top#{rank}: (R{i}:{robots[i].name}, S{j}:{sensors[j].name}) prob={X[i,j]:.4f}\")\n",
    "\n",
    "    # Build ref trajectories\n",
    "    refs: List[np.ndarray] = []\n",
    "    ref_w: List[float] = []\n",
    "    for (i, j) in pairs:\n",
    "        rp = robots[i]\n",
    "        if not isinstance(rp, FixedPathPolicy):\n",
    "            continue\n",
    "        refs.append(grid_path_to_continuous(rp.path))\n",
    "        ref_w.append(float(X[i, j]))\n",
    "\n",
    "    if not refs:\n",
    "        log.info(\"[TrajModes] No FixedPathPolicy among selected pairs; skipping.\")\n",
    "        return\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    all_trajs: List[np.ndarray] = []\n",
    "    all_scores: List[float] = []\n",
    "\n",
    "    H, W = risk_map.shape\n",
    "\n",
    "    def risk_at(p: np.ndarray) -> float:\n",
    "        x = int(round(float(p[0])))\n",
    "        y = int(round(float(p[1])))\n",
    "        x = max(0, min(W - 1, x))\n",
    "        y = max(0, min(H - 1, y))\n",
    "        r = risk_map[y, x]\n",
    "        return 1.0 if (not np.isfinite(r)) else float(r)\n",
    "\n",
    "    def objective(traj: np.ndarray, ref: np.ndarray) -> float:\n",
    "        w_track, w_smooth, w_risk = 1.0, 0.3, 5.0\n",
    "        obj = 0.0\n",
    "        for t in range(traj.shape[0]):\n",
    "            obj += w_track * float(np.sum((traj[t] - ref[t]) ** 2))\n",
    "            if t > 0:\n",
    "                obj += w_smooth * float(np.sum((traj[t] - traj[t - 1]) ** 2))\n",
    "            obj += w_risk * risk_at(traj[t])\n",
    "        return float(obj)\n",
    "\n",
    "    log.info(f\"[TrajModes] Sampling {particles_per_pair} particles per ref, noise_std={noise_std}\")\n",
    "\n",
    "    for ref in refs:\n",
    "        for _ in range(particles_per_pair):\n",
    "            traj0 = ref + rng.normal(0.0, noise_std, size=ref.shape)\n",
    "            traj0[0] = ref[0]\n",
    "            traj0[-1] = ref[-1]\n",
    "            traj1 = smooth_trajectory(traj0, ref, risk_map)\n",
    "            all_trajs.append(traj1)\n",
    "            all_scores.append(objective(traj1, ref))\n",
    "\n",
    "    # Greedy clustering by Fréchet (best-first)\n",
    "    order = list(np.argsort(all_scores))\n",
    "    clusters: List[List[int]] = []\n",
    "    reps: List[int] = []\n",
    "\n",
    "    def assign(idx: int) -> None:\n",
    "        for c_idx, rep in enumerate(reps):\n",
    "            if discrete_frechet(all_trajs[idx], all_trajs[rep]) <= cluster_threshold:\n",
    "                clusters[c_idx].append(idx)\n",
    "                return\n",
    "        clusters.append([idx])\n",
    "        reps.append(idx)\n",
    "\n",
    "    for idx in order:\n",
    "        assign(int(idx))\n",
    "\n",
    "    log.info(f\"[TrajModes] Done. clusters={len(clusters)} (threshold={cluster_threshold})\")\n",
    "    for c_idx, members in enumerate(clusters[:10]):\n",
    "        best = min(members, key=lambda ii: all_scores[ii])\n",
    "        log.info(f\"  Mode{c_idx}: size={len(members)} best_score={all_scores[best]:.3f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(args: argparse.Namespace) -> None:\n",
    "    log = Logger(args.log_level)\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "\n",
    "    log.banner(\"[PIPELINE] Approach 2 (Robust): Empirical POMDP Policy Game + NBS + BR\")\n",
    "    log.info(f\"Grid: {grid.width}x{grid.height} start={grid.start} goal={grid.goal} obstacles={len(grid.obstacles)}\")\n",
    "    log.info(f\"Sensors: {sensor_cfg.sensors} radius={sensor_cfg.radius} base_p={sensor_cfg.base_p} hotspot_p={sensor_cfg.hotspot_p}\")\n",
    "    if log.k >= 2:\n",
    "        log.debug(\"ASCII map:\")\n",
    "        print_grid_ascii(grid, sensor_cfg)\n",
    "\n",
    "    M_modes = args.m_modes\n",
    "    if M_modes != len(sensor_cfg.sensors):\n",
    "        log.info(f\"[WARN] m_modes={M_modes} but sensors listed={len(sensor_cfg.sensors)}. Using sensors count.\")\n",
    "        M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "    robots, sensors = build_initial_policies(env, M_modes=M_modes)\n",
    "\n",
    "    log.info(\"Initial robots: \" + \", \".join([p.name for p in robots]))\n",
    "    log.info(\"Initial sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "\n",
    "    debug_pair = None\n",
    "    if args.debug_rollout_pair:\n",
    "        parts = args.debug_rollout_pair.split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            debug_pair = (int(parts[0]), int(parts[1]))\n",
    "            log.info(f\"[DEBUG] Will print one step-by-step rollout for pair {debug_pair} (only once).\")\n",
    "\n",
    "    x_star = None\n",
    "\n",
    "    for it in range(1, args.outer_iters + 1):\n",
    "        log.banner(f\"[PIPELINE] Outer iter {it}/{args.outer_iters}\")\n",
    "\n",
    "        U_R, U_S, diag = evaluate_payoffs(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            M_modes=M_modes,\n",
    "            rollouts=args.rollouts_payoff,\n",
    "            base_seed=1000 + 100 * it,\n",
    "            log=log,\n",
    "            debug_rollout_pair=debug_pair,\n",
    "        )\n",
    "        debug_pair = None  # only once\n",
    "\n",
    "        # Solve NBS over joint actions\n",
    "        uR = U_R.reshape(-1)\n",
    "        uS = U_S.reshape(-1)\n",
    "\n",
    "        nbs = solve_nbs(\n",
    "            uR,\n",
    "            uS,\n",
    "            log=log,\n",
    "            max_iters=400,\n",
    "            alpha=0.5,\n",
    "            disagreement=args.disagreement,\n",
    "            entropy_tau=args.entropy_tau,\n",
    "        )\n",
    "        x_star = nbs.x\n",
    "\n",
    "        m, n = len(robots), len(sensors)\n",
    "        X = joint_to_matrix(x_star, m, n)\n",
    "        sigma_R, sigma_S = marginals_from_joint(x_star, m, n)\n",
    "\n",
    "        # Compact report\n",
    "        top = np.argsort(-x_star)[:min(5, x_star.size)]\n",
    "        log.info(\"[NBS] Top joint actions:\")\n",
    "        for k, idx in enumerate(top, start=1):\n",
    "            i, j = np.unravel_index(int(idx), (m, n))\n",
    "            log.info(f\"  #{k}: (R{i}:{robots[i].name}, S{j}:{sensors[j].name}) prob={X[i,j]:.4f}\")\n",
    "        log.info(f\"[NBS] sigma_R={sigma_R.round(3)}\")\n",
    "        log.info(f\"[NBS] sigma_S={sigma_S.round(3)}\")\n",
    "\n",
    "        # Best responses\n",
    "        br_r = robot_best_response(env, sensors, sigma_S, robots, M_modes=M_modes, risk_weight=args.risk_weight_br, log=log)\n",
    "        if br_r not in robots:\n",
    "            robots.append(br_r)\n",
    "\n",
    "        br_s = sensor_best_response(\n",
    "            env,\n",
    "            robots,\n",
    "            sigma_R,\n",
    "            candidate_modes=list(range(M_modes)),\n",
    "            M_modes=M_modes,\n",
    "            rollouts=args.rollouts_br,\n",
    "            base_seed=2000 + 100 * it,\n",
    "            log=log,\n",
    "        )\n",
    "        if find_sensor_by_mode(sensors, br_s.mode) is None:\n",
    "            sensors.append(br_s)\n",
    "        else:\n",
    "            log.info(f\"[SensorBR] Mode {br_s.mode} already present; not adding duplicate.\")\n",
    "\n",
    "        log.info(f\"[Sets] |Pi_R|={len(robots)} |Pi_S|={len(sensors)}\")\n",
    "\n",
    "    log.banner(\"[PIPELINE] Finished\")\n",
    "    log.info(\"Final robots: \" + \", \".join([p.name for p in robots]))\n",
    "    log.info(\"Final sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "\n",
    "    if args.run_traj_demo and x_star is not None:\n",
    "        trajectory_modes_demo(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            x_star,\n",
    "            log=log,\n",
    "            top_k_pairs=args.traj_top_k,\n",
    "            particles_per_pair=args.traj_particles,\n",
    "            noise_std=args.traj_noise_std,\n",
    "            cluster_threshold=args.traj_cluster_threshold,\n",
    "            min_prob=args.traj_min_prob,\n",
    "        )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLI\n",
    "# =============================================================================\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> Tuple[argparse.Namespace, List[str]]:\n",
    "    \"\"\"Parse CLI args.\n",
    "\n",
    "    Uses parse_known_args so Jupyter/ipykernel-injected args like --f=... don't crash.\n",
    "    \"\"\"\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "    p.add_argument(\"--log-level\", type=str, default=\"INFO\", choices=[\"QUIET\", \"INFO\", \"DEBUG\"])\n",
    "\n",
    "    p.add_argument(\"--outer-iters\", type=int, default=3)\n",
    "    p.add_argument(\"--m-modes\", type=int, default=2)\n",
    "\n",
    "    p.add_argument(\"--rollouts-payoff\", type=int, default=20)\n",
    "    p.add_argument(\"--rollouts-br\", type=int, default=30)\n",
    "    p.add_argument(\"--risk-weight-br\", type=float, default=12.0)\n",
    "\n",
    "    # NBS knobs\n",
    "    p.add_argument(\"--disagreement\", type=str, default=\"minminus\", choices=[\"minminus\", \"uniform\"])\n",
    "    p.add_argument(\"--entropy-tau\", type=float, default=0.0)\n",
    "\n",
    "    # Debug: print ONE step-by-step rollout for (Ri,Sj)\n",
    "    p.add_argument(\"--debug-rollout-pair\", type=str, default=\"\")\n",
    "\n",
    "    # Trajectory modes demo\n",
    "    p.add_argument(\"--run-traj-demo\", action=\"store_true\")\n",
    "    p.add_argument(\"--traj-top-k\", type=int, default=4)\n",
    "    p.add_argument(\"--traj-particles\", type=int, default=20)\n",
    "    p.add_argument(\"--traj-noise-std\", type=float, default=0.4)\n",
    "    p.add_argument(\"--traj-cluster-threshold\", type=float, default=1.0)\n",
    "    p.add_argument(\"--traj-min-prob\", type=float, default=1e-6)\n",
    "\n",
    "    args, unknown = p.parse_known_args(args=argv)\n",
    "    return args, unknown\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"Entry point.\n",
    "\n",
    "    In notebooks, call:\n",
    "        main(argv=[])\n",
    "    to avoid picking up ipykernel flags.\n",
    "    \"\"\"\n",
    "    args, unknown = parse_args(argv=argv)\n",
    "\n",
    "    # If running as a normal script and there are unknown args, warn once.\n",
    "    # In notebooks, ipykernel injects args like \"--f=...json\" — we intentionally ignore them.\n",
    "    if unknown and (\"ipykernel\" not in sys.modules):\n",
    "        print(f\"[WARN] Ignoring unknown CLI args: {unknown}\")\n",
    "    # normalize empty debug string\n",
    "    if args.debug_rollout_pair.strip() == \"\":\n",
    "        args.debug_rollout_pair = \"\"\n",
    "    run_pipeline(args)\n",
    "\n",
    "\n",
    "# In notebooks, avoid auto-running on cell execution.\n",
    "if __name__ == \"__main__\" and (\"ipykernel\" not in sys.modules):\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96fa458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"approach2_robust_correlated.py\n",
    "\n",
    "Robust, reduced-log implementation of \"Approach 2\" on a stealth gridworld POMDP,\n",
    "with a FIX for the conceptual mismatch:\n",
    "\n",
    "  Old mismatch: solve a joint distribution x*(i,j) over (robot policy i, sensor policy j)\n",
    "  but then compute best responses against the MARGINALS sigma_R, sigma_S.\n",
    "\n",
    "  Fix here: treat x* as a CORRELATION DEVICE (mediator) and compute BRs against\n",
    "  CONDITIONAL distributions:\n",
    "\n",
    "      q_S(.|i) = X[i,:] / sigma_R[i]   (sensor conditional given robot recommendation i)\n",
    "      q_R(.|j) = X[:,j] / sigma_S[j]   (robot conditional given sensor recommendation j)\n",
    "\n",
    "  Then add the most profitable *deviation* policy (oracle) based on the most violated\n",
    "  conditional recommendation.\n",
    "\n",
    "This makes the PSRO-style expansion step consistent with a correlated-strategy viewpoint.\n",
    "\n",
    "Also included:\n",
    "  - Benchmark harness vs simple heuristics on the same game.\n",
    "  - Saved plots (training curves + bar charts) for presentations.\n",
    "\n",
    "Run (script):\n",
    "  python approach2_robust_correlated.py --solver correlated --outer-iters 3 --save-plots\n",
    "\n",
    "Run (notebook):\n",
    "  main(argv=[\"--solver\",\"both\",\"--outer-iters\",\"3\",\"--save-plots\"])  # ignore ipykernel args\n",
    "\n",
    "Tips to encourage MULTIMODAL x* (useful for your \"mode recovery\"):\n",
    "  --disagreement uniform --entropy-tau 0.02\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import heapq\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Stage-based logger with QUIET/INFO/DEBUG.\"\"\"\n",
    "\n",
    "    LEVELS = {\"QUIET\": 0, \"INFO\": 1, \"DEBUG\": 2}\n",
    "\n",
    "    def __init__(self, level: str = \"INFO\"):\n",
    "        level = level.upper()\n",
    "        if level not in self.LEVELS:\n",
    "            raise ValueError(f\"Unknown log level: {level}. Use QUIET/INFO/DEBUG\")\n",
    "        self.level = level\n",
    "        self.k = self.LEVELS[level]\n",
    "\n",
    "    def banner(self, title: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(\"\" + \"=\" * 100)\n",
    "            print(title)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "    def info(self, msg: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(msg)\n",
    "\n",
    "    def debug(self, msg: str) -> None:\n",
    "        if self.k >= 2:\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Types\n",
    "# =============================================================================\n",
    "\n",
    "Action = Tuple[int, int]  # (dx, dy)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Environment (grid + hidden sensor mode + noisy alarm observation)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GridConfig:\n",
    "    width: int\n",
    "    height: int\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    obstacles: frozenset\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SensorConfig:\n",
    "    sensors: Tuple[Tuple[int, int], ...]\n",
    "    radius: int\n",
    "    base_p: float\n",
    "    hotspot_p: float\n",
    "\n",
    "\n",
    "class GridWorldStealthEnv:\n",
    "    \"\"\"Grid world with detection risk controlled by a hidden/selected sensor 'mode'.\"\"\"\n",
    "\n",
    "    def __init__(self, grid: GridConfig, sensor_cfg: SensorConfig, fp: float = 0.05, fn: float = 0.10, seed: int = 0):\n",
    "        self.grid = grid\n",
    "        self.sensor_cfg = sensor_cfg\n",
    "        self.fp = float(fp)\n",
    "        self.fn = float(fn)\n",
    "\n",
    "        if not (0.0 <= self.fp <= 1.0 and 0.0 <= self.fn <= 1.0):\n",
    "            raise ValueError(\"fp and fn must be in [0,1].\")\n",
    "        if not (0.0 <= sensor_cfg.base_p <= 1.0 and 0.0 <= sensor_cfg.hotspot_p <= 1.0):\n",
    "            raise ValueError(\"base_p and hotspot_p must be in [0,1].\")\n",
    "        if sensor_cfg.radius < 0:\n",
    "            raise ValueError(\"radius must be >= 0\")\n",
    "        if len(sensor_cfg.sensors) == 0:\n",
    "            raise ValueError(\"Need at least one sensor center.\")\n",
    "\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.reset(sensor_mode=0)\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    def reset(self, sensor_mode: int = 0) -> Dict[str, Any]:\n",
    "        self.t = 0\n",
    "        self.pos = self.grid.start\n",
    "        self.sensor_mode = int(sensor_mode)\n",
    "        self.detected = False\n",
    "        self.total_true_risk = 0.0\n",
    "        return {\"pos\": self.pos, \"t\": self.t}\n",
    "\n",
    "    def in_bounds(self, p: Tuple[int, int]) -> bool:\n",
    "        x, y = p\n",
    "        return 0 <= x < self.grid.width and 0 <= y < self.grid.height\n",
    "\n",
    "    def is_free(self, p: Tuple[int, int]) -> bool:\n",
    "        return self.in_bounds(p) and (p not in self.grid.obstacles)\n",
    "\n",
    "    def true_detection_prob(self, p: Tuple[int, int], mode: int) -> float:\n",
    "        if not (0 <= mode < len(self.sensor_cfg.sensors)):\n",
    "            raise ValueError(f\"mode {mode} out of range\")\n",
    "        sx, sy = self.sensor_cfg.sensors[mode]\n",
    "        x, y = p\n",
    "        d = abs(x - sx) + abs(y - sy)\n",
    "        return self.sensor_cfg.hotspot_p if d <= self.sensor_cfg.radius else self.sensor_cfg.base_p\n",
    "\n",
    "    def observation_prob(self, alarm: int, p_true: float) -> float:\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        if alarm == 1:\n",
    "            return p_alarm\n",
    "        if alarm == 0:\n",
    "            return 1.0 - p_alarm\n",
    "        raise ValueError(\"alarm must be 0 or 1\")\n",
    "\n",
    "    def step(self, a: Action) -> Dict[str, Any]:\n",
    "        if self.detected:\n",
    "            return {\"pos\": self.pos, \"t\": self.t, \"alarm\": 1, \"p_true\": 1.0, \"detected\": True, \"done\": True}\n",
    "\n",
    "        self.t += 1\n",
    "        nx = self.pos[0] + int(a[0])\n",
    "        ny = self.pos[1] + int(a[1])\n",
    "        np_ = (nx, ny)\n",
    "        if self.is_free(np_):\n",
    "            self.pos = np_\n",
    "\n",
    "        p_true = float(self.true_detection_prob(self.pos, self.sensor_mode))\n",
    "        self.total_true_risk += p_true\n",
    "\n",
    "        if self.rng.random() < p_true:\n",
    "            self.detected = True\n",
    "\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        alarm = 1 if (self.rng.random() < p_alarm) else 0\n",
    "\n",
    "        done = self.detected or (self.pos == self.grid.goal) or (self.t >= 200)\n",
    "        return {\"pos\": self.pos, \"t\": self.t, \"alarm\": alarm, \"p_true\": p_true, \"detected\": self.detected, \"done\": done}\n",
    "\n",
    "\n",
    "def build_two_corridor_grid(width: int = 15, height: int = 9) -> GridConfig:\n",
    "    obstacles = set()\n",
    "    wall_x = width // 2\n",
    "    gap_ys = {2, 6}\n",
    "    for y in range(height):\n",
    "        if y not in gap_ys:\n",
    "            obstacles.add((wall_x, y))\n",
    "\n",
    "    start = (1, height - 2)\n",
    "    goal = (width - 2, 1)\n",
    "    if start in obstacles or goal in obstacles:\n",
    "        raise RuntimeError(\"Start/goal blocked unexpectedly\")\n",
    "\n",
    "    return GridConfig(width=width, height=height, start=start, goal=goal, obstacles=frozenset(obstacles))\n",
    "\n",
    "\n",
    "def print_grid_ascii(grid: GridConfig, sensor_cfg: SensorConfig) -> None:\n",
    "    W, H = grid.width, grid.height\n",
    "    obs = set(grid.obstacles)\n",
    "    sens = set(sensor_cfg.sensors)\n",
    "    for y in range(H):\n",
    "        row = []\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p == grid.start:\n",
    "                row.append(\"R\")\n",
    "            elif p == grid.goal:\n",
    "                row.append(\"G\")\n",
    "            elif p in sens:\n",
    "                row.append(\"S\")\n",
    "            elif p in obs:\n",
    "                row.append(\"#\")\n",
    "            else:\n",
    "                row.append(\".\")\n",
    "        print(\"\".join(row))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Belief over modes\n",
    "# =============================================================================\n",
    "\n",
    "class ModeBelief:\n",
    "    \"\"\"Exact belief over discrete modes m in {0..M-1}.\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, init: Optional[np.ndarray] = None):\n",
    "        self.M = int(M)\n",
    "        if self.M <= 0:\n",
    "            raise ValueError(\"M must be >= 1\")\n",
    "        if init is None:\n",
    "            self.b = np.full(self.M, 1.0 / self.M)\n",
    "        else:\n",
    "            init = np.asarray(init, dtype=float).reshape(-1)\n",
    "            if init.shape != (self.M,):\n",
    "                raise ValueError(\"init shape mismatch\")\n",
    "            if np.any(init < 0):\n",
    "                raise ValueError(\"init must be nonnegative\")\n",
    "            s = float(init.sum())\n",
    "            self.b = init / s if s > 0 else np.full(self.M, 1.0 / self.M)\n",
    "\n",
    "    def update(self, env: GridWorldStealthEnv, alarm: int, pos: Tuple[int, int], eps: float = 1e-12) -> None:\n",
    "        like = np.zeros(self.M, dtype=float)\n",
    "        for m in range(self.M):\n",
    "            p_true = env.true_detection_prob(pos, m)\n",
    "            like[m] = env.observation_prob(alarm, p_true)\n",
    "        post = self.b * like\n",
    "        Z = float(post.sum())\n",
    "        if (not np.isfinite(Z)) or Z < eps:\n",
    "            return\n",
    "        self.b = post / Z\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policies\n",
    "# =============================================================================\n",
    "\n",
    "class RobotPolicy:\n",
    "    name: str = \"RobotPolicy\"\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedPathPolicy(RobotPolicy):\n",
    "    def __init__(self, path: List[Tuple[int, int]], name: str):\n",
    "        if len(path) < 2:\n",
    "            raise ValueError(\"Path must have >=2 states\")\n",
    "        self.path = list(path)\n",
    "        self.name = str(name)\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        try:\n",
    "            self._idx = self.path.index(start_pos)\n",
    "        except ValueError:\n",
    "            self._idx = 0\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        cur = env.pos\n",
    "        if self._idx >= len(self.path) - 1:\n",
    "            return (0, 0)\n",
    "        if cur != self.path[self._idx]:\n",
    "            try:\n",
    "                self._idx = self.path.index(cur, self._idx)\n",
    "            except ValueError:\n",
    "                return (0, 0)\n",
    "        nxt = self.path[self._idx + 1]\n",
    "        dx = int(np.clip(nxt[0] - cur[0], -1, 1))\n",
    "        dy = int(np.clip(nxt[1] - cur[1], -1, 1))\n",
    "        self._idx += 1\n",
    "        return (dx, dy)\n",
    "\n",
    "\n",
    "class RandomPolicy(RobotPolicy):\n",
    "    \"\"\"Reproducible random policy using env.rng.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"R_Random\"):\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        candidates: List[Action] = []\n",
    "        x, y = env.pos\n",
    "        for a in [(1, 0), (-1, 0), (0, 1), (0, -1), (0, 0)]:\n",
    "            np_ = (x + a[0], y + a[1])\n",
    "            if env.is_free(np_):\n",
    "                candidates.append(a)\n",
    "        if not candidates:\n",
    "            return (0, 0)\n",
    "        return candidates[int(env.rng.integers(0, len(candidates)))]\n",
    "\n",
    "\n",
    "class OnlineBeliefReplanPolicy(RobotPolicy):\n",
    "    \"\"\"POMDP-ish heuristic: replan each step using risk map induced by current belief b_t.\"\"\"\n",
    "\n",
    "    def __init__(self, env: GridWorldStealthEnv, risk_weight: float = 12.0, name: str = \"R_OnlineBeliefReplan\"):\n",
    "        self.env = env\n",
    "        self.risk_weight = float(risk_weight)\n",
    "        self.name = name\n",
    "        self._cached_next: Optional[Tuple[int, int]] = None\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        self._cached_next = None\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        # Build a risk map from belief over modes.\n",
    "        mode_probs = belief.b\n",
    "\n",
    "        def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "            x, y = to\n",
    "            # expected risk at to under belief\n",
    "            r = 0.0\n",
    "            for m, pm in enumerate(mode_probs):\n",
    "                r += float(pm) * float(env.true_detection_prob(to, m))\n",
    "            return 1.0 + self.risk_weight * r\n",
    "\n",
    "        # Plan from current pos to goal (one-step receding horizon)\n",
    "        try:\n",
    "            path = astar_path(env.grid, env.pos, env.grid.goal, step_cost)\n",
    "            if len(path) < 2:\n",
    "                return (0, 0)\n",
    "            nxt = path[1]\n",
    "            dx = int(np.clip(nxt[0] - env.pos[0], -1, 1))\n",
    "            dy = int(np.clip(nxt[1] - env.pos[1], -1, 1))\n",
    "            return (dx, dy)\n",
    "        except Exception:\n",
    "            return (0, 0)\n",
    "\n",
    "\n",
    "class SensorPolicy:\n",
    "    name: str = \"SensorPolicy\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedModeSensorPolicy(SensorPolicy):\n",
    "    def __init__(self, mode: int, name: Optional[str] = None):\n",
    "        self.mode = int(mode)\n",
    "        self.name = name or f\"S_Mode{mode}\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return self.mode\n",
    "\n",
    "\n",
    "class AlternatingSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Simple sensor heuristic for benchmarks: alternate modes 0,1,0,1,...\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, name: str = \"S_Alternate\"):\n",
    "        self.M = int(M)\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(t % self.M)\n",
    "\n",
    "\n",
    "class RandomModeSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Benchmark sensor: random mode each step (uses numpy Generator for reproducibility).\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, seed: int = 0, name: str = \"S_RandomMode\"):\n",
    "        self.M = int(M)\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(self.rng.integers(0, self.M))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# A* (used for planning)\n",
    "# =============================================================================\n",
    "\n",
    "def astar_path(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    max_expansions: int = 250_000,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    if start == goal:\n",
    "        return [start]\n",
    "\n",
    "    def h(p: Tuple[int, int]) -> float:\n",
    "        return abs(p[0] - goal[0]) + abs(p[1] - goal[1])\n",
    "\n",
    "    def neighbors(p: Tuple[int, int]):\n",
    "        x, y = p\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            np_ = (x + dx, y + dy)\n",
    "            if 0 <= np_[0] < grid.width and 0 <= np_[1] < grid.height and np_ not in grid.obstacles:\n",
    "                yield np_\n",
    "\n",
    "    open_heap: List[Tuple[float, float, Tuple[int, int]]] = []\n",
    "    heapq.heappush(open_heap, (h(start), 0.0, start))\n",
    "\n",
    "    came: Dict[Tuple[int, int], Optional[Tuple[int, int]]] = {start: None}\n",
    "    gscore: Dict[Tuple[int, int], float] = {start: 0.0}\n",
    "\n",
    "    expansions = 0\n",
    "    while open_heap:\n",
    "        _, _, cur = heapq.heappop(open_heap)\n",
    "        expansions += 1\n",
    "        if cur == goal:\n",
    "            path: List[Tuple[int, int]] = []\n",
    "            while cur is not None:\n",
    "                path.append(cur)\n",
    "                cur = came[cur]\n",
    "            path.reverse()\n",
    "            return path\n",
    "        if expansions > max_expansions:\n",
    "            raise RuntimeError(\"A* exceeded max expansions\")\n",
    "\n",
    "        for nb in neighbors(cur):\n",
    "            tentative = gscore[cur] + float(step_cost(cur, nb))\n",
    "            if (nb not in gscore) or (tentative < gscore[nb] - 1e-12):\n",
    "                gscore[nb] = tentative\n",
    "                came[nb] = cur\n",
    "                heapq.heappush(open_heap, (tentative + h(nb), tentative, nb))\n",
    "\n",
    "    raise RuntimeError(\"A* failed: unreachable goal\")\n",
    "\n",
    "\n",
    "def astar_via_waypoint(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    waypoint: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    ") -> List[Tuple[int, int]]:\n",
    "    p1 = astar_path(grid, start, waypoint, step_cost)\n",
    "    p2 = astar_path(grid, waypoint, goal, step_cost)\n",
    "    return p1[:-1] + p2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rollouts + payoffs\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStats:\n",
    "    steps: int\n",
    "    reached_goal: bool\n",
    "    detected: bool\n",
    "    total_true_risk: float\n",
    "    U_R: float\n",
    "    U_S: float\n",
    "\n",
    "\n",
    "def rollout_episode(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    seed: int,\n",
    "    max_steps: int = 200,\n",
    "    lambda_risk: float = 1.0,\n",
    "    det_penalty: float = 50.0,\n",
    "    sensor_energy_per_step: float = 0.2,\n",
    "    step_debug: bool = False,\n",
    ") -> EpisodeStats:\n",
    "    env.seed(seed)\n",
    "    sensor.reset()\n",
    "    env.reset(sensor_mode=sensor.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)\n",
    "    robot.reset(env.pos)\n",
    "\n",
    "    total_risk = 0.0\n",
    "    last_alarm: Optional[int] = None\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.sensor_mode = sensor.select_mode(env.t)\n",
    "        a = robot.act(env, belief, last_alarm)\n",
    "        out = env.step(a)\n",
    "\n",
    "        total_risk += float(out[\"p_true\"])\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "        last_alarm = int(out[\"alarm\"])\n",
    "\n",
    "        if step_debug:\n",
    "            print(\n",
    "                f\"[Step] t={out['t']:3d} pos={out['pos']} a={a} p_true={out['p_true']:.3f} \"\n",
    "                f\"alarm={out['alarm']} det={out['detected']} done={out['done']} b={belief.b.round(3)}\"\n",
    "            )\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = bool(env.detected)\n",
    "    steps = int(env.t)\n",
    "\n",
    "    cost_R = steps + lambda_risk * total_risk + (det_penalty if detected else 0.0)\n",
    "    U_R = -float(cost_R)\n",
    "\n",
    "    U_S = float((det_penalty if detected else 0.0) + lambda_risk * total_risk - sensor_energy_per_step * steps)\n",
    "\n",
    "    return EpisodeStats(steps=steps, reached_goal=reached_goal, detected=detected, total_true_risk=float(total_risk), U_R=U_R, U_S=U_S)\n",
    "\n",
    "\n",
    "def evaluate_payoffs(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    debug_rollout_pair: Optional[Tuple[int, int]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[int, int], Dict[str, float]]]:\n",
    "    m, n = len(robots), len(sensors)\n",
    "    U_R = np.zeros((m, n), dtype=float)\n",
    "    U_S = np.zeros((m, n), dtype=float)\n",
    "    diag: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    log.info(f\"[Eval] Estimating payoffs: m={m}, n={n}, rollouts={rollouts}, base_seed={base_seed}\")\n",
    "\n",
    "    for i, rpol in enumerate(robots):\n",
    "        for j, spol in enumerate(sensors):\n",
    "            step_debug = (debug_rollout_pair == (i, j))\n",
    "\n",
    "            r_list: List[float] = []\n",
    "            s_list: List[float] = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            steps_list: List[int] = []\n",
    "            risk_list: List[float] = []\n",
    "\n",
    "            for k in range(rollouts):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rpol, spol, M_modes=M_modes, seed=seed, step_debug=step_debug)\n",
    "                r_list.append(st.U_R)\n",
    "                s_list.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "                steps_list.append(st.steps)\n",
    "                risk_list.append(st.total_true_risk)\n",
    "\n",
    "                if step_debug:\n",
    "                    step_debug = False  # only show one rollout\n",
    "\n",
    "            U_R[i, j] = float(np.mean(r_list))\n",
    "            U_S[i, j] = float(np.mean(s_list))\n",
    "\n",
    "            diag[(i, j)] = {\n",
    "                \"det_rate\": det / rollouts,\n",
    "                \"goal_rate\": goal / rollouts,\n",
    "                \"mean_steps\": float(np.mean(steps_list)),\n",
    "                \"mean_risk\": float(np.mean(risk_list)),\n",
    "                \"std_UR\": float(np.std(r_list)),\n",
    "                \"std_US\": float(np.std(s_list)),\n",
    "            }\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(\"[Eval] Compact payoff summary:\")\n",
    "        for i, rpol in enumerate(robots):\n",
    "            for j, spol in enumerate(sensors):\n",
    "                d = diag[(i, j)]\n",
    "                log.info(\n",
    "                    f\"  (R{i}:{rpol.name}, S{j}:{spol.name}) \"\n",
    "                    f\"UR={U_R[i,j]:8.3f}±{d['std_UR']:.2f} | \"\n",
    "                    f\"US={U_S[i,j]:8.3f}±{d['std_US']:.2f} | \"\n",
    "                    f\"det%={100*d['det_rate']:5.1f} goal%={100*d['goal_rate']:5.1f} \"\n",
    "                    f\"steps={d['mean_steps']:.1f} risk={d['mean_risk']:.2f}\"\n",
    "                )\n",
    "\n",
    "    return U_R, U_S, diag\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NBS solver (with optional entropy regularization)\n",
    "# =============================================================================\n",
    "\n",
    "def project_simplex(v: np.ndarray, z: float = 1.0) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    if v.size == 0:\n",
    "        raise ValueError(\"Empty vector\")\n",
    "    if z <= 0:\n",
    "        raise ValueError(\"z must be > 0\")\n",
    "\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, v.size + 1) > (cssv - z))[0]\n",
    "    if rho.size == 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    rho = int(rho[-1])\n",
    "    theta = (cssv[rho] - z) / (rho + 1.0)\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    s = float(w.sum())\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    return w * (z / s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NBSResult:\n",
    "    x: np.ndarray\n",
    "    obj: float\n",
    "    gains: Tuple[float, float]\n",
    "    support: int\n",
    "\n",
    "\n",
    "def solve_nbs(\n",
    "    uR: np.ndarray,\n",
    "    uS: np.ndarray,\n",
    "    log: Logger,\n",
    "    max_iters: int = 400,\n",
    "    alpha: float = 0.5,\n",
    "    tol_l1: float = 1e-6,\n",
    "    kappa: float = 1e-6,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    ") -> NBSResult:\n",
    "    uR = np.asarray(uR, dtype=float).reshape(-1)\n",
    "    uS = np.asarray(uS, dtype=float).reshape(-1)\n",
    "    if uR.shape != uS.shape:\n",
    "        raise ValueError(\"uR and uS must have same shape\")\n",
    "    d = uR.size\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Need >=2 joint actions\")\n",
    "\n",
    "    unif = np.full(d, 1.0 / d)\n",
    "\n",
    "    disagreement = disagreement.lower().strip()\n",
    "    if disagreement == \"minminus\":\n",
    "        dR = float(np.min(uR) - 1.0)\n",
    "        dS = float(np.min(uS) - 1.0)\n",
    "    elif disagreement == \"uniform\":\n",
    "        dR = float(uR @ unif)\n",
    "        dS = float(uS @ unif)\n",
    "    else:\n",
    "        raise ValueError(\"disagreement must be 'minminus' or 'uniform'\")\n",
    "\n",
    "    x = unif.copy()\n",
    "\n",
    "    def gains(xv: np.ndarray) -> Tuple[float, float]:\n",
    "        return float(uR @ xv - dR), float(uS @ xv - dS)\n",
    "\n",
    "    def entropy(xv: np.ndarray) -> float:\n",
    "        xx = np.clip(xv, 1e-12, 1.0)\n",
    "        return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "    def obj(xv: np.ndarray) -> float:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return float(np.log(gR) + np.log(gS) + entropy_tau * entropy(xv))\n",
    "\n",
    "    def grad(xv: np.ndarray) -> np.ndarray:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        g = (uR / gR) + (uS / gS)\n",
    "        if entropy_tau > 0:\n",
    "            xx = np.clip(xv, 1e-12, 1.0)\n",
    "            g += entropy_tau * (-(np.log(xx) + 1.0))\n",
    "        return g\n",
    "\n",
    "    last = obj(x)\n",
    "    log.info(f\"[NBS] d={d} disagreement=({dR:.3f},{dS:.3f}) entropy_tau={entropy_tau:.3g}\")\n",
    "\n",
    "    for t in range(1, max_iters + 1):\n",
    "        g = grad(x)\n",
    "        a = alpha\n",
    "        improved = False\n",
    "        for _ in range(30):\n",
    "            x_new = project_simplex(x + a * g)\n",
    "            new_obj = obj(x_new)\n",
    "            if new_obj >= last - 1e-12:\n",
    "                improved = True\n",
    "                break\n",
    "            a *= 0.5\n",
    "            if a < 1e-6:\n",
    "                break\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "        delta = float(np.linalg.norm(x_new - x, ord=1))\n",
    "        x = x_new\n",
    "        last = new_obj\n",
    "\n",
    "        if log.k >= 2 and (t <= 5 or t % 25 == 0):\n",
    "            gR, gS = gains(x)\n",
    "            top = np.argsort(-x)[:5]\n",
    "            top_str = \", \".join([f\"{i}:{x[i]:.3f}\" for i in top])\n",
    "            log.debug(f\"[NBS][it={t:3d}] obj={last:.6f} gains=({gR:.3f},{gS:.3f}) L1={delta:.2e} top={top_str}\")\n",
    "\n",
    "        if delta < tol_l1:\n",
    "            break\n",
    "\n",
    "    gR, gS = gains(x)\n",
    "    support = int(np.sum(x > 1e-6))\n",
    "    log.info(f\"[NBS] done: obj={last:.6f} gains=({gR:.3f},{gS:.3f}) support={support}/{d}\")\n",
    "\n",
    "    return NBSResult(x=x, obj=float(last), gains=(float(gR), float(gS)), support=support)\n",
    "\n",
    "\n",
    "def joint_to_matrix(x: np.ndarray, m: int, n: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size != m * n:\n",
    "        raise ValueError(\"x size mismatch\")\n",
    "    X = x.reshape((m, n))\n",
    "    s = float(X.sum())\n",
    "    if not np.isfinite(s) or abs(s - 1.0) > 1e-6:\n",
    "        # Renormalize defensively\n",
    "        X = X / max(s, 1e-12)\n",
    "    return X\n",
    "\n",
    "\n",
    "def marginals_from_joint(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    sigma_R = X.sum(axis=1)\n",
    "    sigma_S = X.sum(axis=0)\n",
    "    if sigma_R.sum() > 0:\n",
    "        sigma_R = sigma_R / sigma_R.sum()\n",
    "    if sigma_S.sum() > 0:\n",
    "        sigma_S = sigma_S / sigma_S.sum()\n",
    "    return sigma_R, sigma_S\n",
    "\n",
    "\n",
    "def entropy_of_joint(X: np.ndarray) -> float:\n",
    "    xx = np.clip(X.reshape(-1), 1e-12, 1.0)\n",
    "    return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Best responses (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_expected_risk_map_from_policy_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi: np.ndarray,\n",
    "    M_modes: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Expected p_true(cell) under mixture over sensor POLICIES (not modes).\n",
    "\n",
    "    For each sensor policy j, we use its mode at t=0 as its defining mode.\n",
    "    (This matches FixedModeSensorPolicy exactly.)\n",
    "    \"\"\"\n",
    "    pi = np.asarray(pi, dtype=float).reshape(-1)\n",
    "    if pi.size != len(sensors):\n",
    "        raise ValueError(\"mixture length mismatch\")\n",
    "\n",
    "    mode_probs = np.zeros(M_modes, dtype=float)\n",
    "    for j, sp in enumerate(sensors):\n",
    "        m = int(sp.select_mode(0))\n",
    "        if not (0 <= m < M_modes):\n",
    "            raise ValueError(\"invalid sensor mode\")\n",
    "        mode_probs[m] += float(pi[j])\n",
    "    if mode_probs.sum() > 0:\n",
    "        mode_probs = mode_probs / mode_probs.sum()\n",
    "\n",
    "    H, W = env.grid.height, env.grid.width\n",
    "    risk = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in env.grid.obstacles:\n",
    "                risk[y, x] = np.nan\n",
    "                continue\n",
    "            val = 0.0\n",
    "            for m in range(M_modes):\n",
    "                val += float(mode_probs[m]) * float(env.true_detection_prob((x, y), m))\n",
    "            risk[y, x] = float(val)\n",
    "\n",
    "    return risk\n",
    "\n",
    "\n",
    "def plan_risk_weighted_path(env: GridWorldStealthEnv, risk_map: np.ndarray, risk_weight: float) -> List[Tuple[int, int]]:\n",
    "    def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "        x, y = to\n",
    "        r = risk_map[y, x]\n",
    "        if not np.isfinite(r):\n",
    "            return 1e9\n",
    "        return 1.0 + float(risk_weight) * float(r)\n",
    "\n",
    "    return astar_path(env.grid, env.grid.start, env.grid.goal, step_cost)\n",
    "\n",
    "\n",
    "def robot_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi_S: np.ndarray,\n",
    "    robots: List[RobotPolicy],\n",
    "    M_modes: int,\n",
    "    risk_weight: float,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> RobotPolicy:\n",
    "    risk = compute_expected_risk_map_from_policy_mixture(env, sensors, pi_S, M_modes=M_modes)\n",
    "    try:\n",
    "        path = plan_risk_weighted_path(env, risk, risk_weight=risk_weight)\n",
    "    except Exception as e:\n",
    "        log.info(f\"[RobotBR] WARNING A* failed ({tag}): {e}\")\n",
    "        return robots[0]\n",
    "\n",
    "    path_tuple = tuple(path)\n",
    "    for p in robots:\n",
    "        if isinstance(p, FixedPathPolicy) and tuple(p.path) == path_tuple:\n",
    "            log.info(f\"[RobotBR] ({tag}) BR path already exists: {p.name}\")\n",
    "            return p\n",
    "\n",
    "    newp = FixedPathPolicy(path, name=f\"R_BR_{tag}_w{risk_weight:.1f}_len{len(path)}\")\n",
    "    log.info(f\"[RobotBR] ({tag}) Added new robot policy: {newp.name}\")\n",
    "    return newp\n",
    "\n",
    "\n",
    "def sensor_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    pi_R: np.ndarray,\n",
    "    candidate_modes: List[int],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> FixedModeSensorPolicy:\n",
    "    pi_R = np.asarray(pi_R, dtype=float).reshape(-1)\n",
    "    if pi_R.size != len(robots):\n",
    "        raise ValueError(\"pi_R length mismatch\")\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    best_mode: Optional[int] = None\n",
    "    best_val = -1e18\n",
    "\n",
    "    for mode in candidate_modes:\n",
    "        if not (0 <= mode < M_modes):\n",
    "            continue\n",
    "        sp = FixedModeSensorPolicy(mode, name=f\"S_BR_{tag}_Mode{mode}\")\n",
    "        vals: List[float] = []\n",
    "        for k in range(rollouts):\n",
    "            i = int(rng.choice(len(robots), p=pi_R))\n",
    "            rp = robots[i]\n",
    "            seed = base_seed + 10000 * mode + k\n",
    "            st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed)\n",
    "            vals.append(st.U_S)\n",
    "        mean_u = float(np.mean(vals))\n",
    "        if log.k >= 2:\n",
    "            log.debug(f\"[SensorBR] ({tag}) mode={mode} E[US]={mean_u:.3f} std={float(np.std(vals)):.2f}\")\n",
    "        if mean_u > best_val:\n",
    "            best_val = mean_u\n",
    "            best_mode = mode\n",
    "\n",
    "    if best_mode is None:\n",
    "        raise RuntimeError(\"No valid sensor BR mode found\")\n",
    "\n",
    "    log.info(f\"[SensorBR] ({tag}) Best mode={best_mode} E[US]={best_val:.3f}\")\n",
    "    return FixedModeSensorPolicy(best_mode, name=f\"S_BR_{tag}_Mode{best_mode}\")\n",
    "\n",
    "\n",
    "def conditional_sensor_given_robot(X: np.ndarray, i: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    row = np.asarray(X[i, :], dtype=float)\n",
    "    s = float(row.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(row, 1.0 / row.size)\n",
    "    return row / s\n",
    "\n",
    "\n",
    "def conditional_robot_given_sensor(X: np.ndarray, j: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    col = np.asarray(X[:, j], dtype=float)\n",
    "    s = float(col.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(col, 1.0 / col.size)\n",
    "    return col / s\n",
    "\n",
    "\n",
    "def compute_ce_regrets(U_R: np.ndarray, U_S: np.ndarray, X: np.ndarray, eps: float = 1e-12) -> Dict[str, float]:\n",
    "    \"\"\"Conditional recommendation regrets (CE-style) computed on current meta-game.\n",
    "\n",
    "    For robot (given recommendation i):\n",
    "        regret_R(i) = max_{i'} E_{j~q(.|i)}[U_R(i',j) - U_R(i,j)]\n",
    "\n",
    "    For sensor (given recommendation j):\n",
    "        regret_S(j) = max_{j'} E_{i~q(.|j)}[U_S(i,j') - U_S(i,j)]\n",
    "\n",
    "    Returns max and average regrets.\n",
    "    \"\"\"\n",
    "    m, n = U_R.shape\n",
    "    assert U_S.shape == (m, n)\n",
    "    assert X.shape == (m, n)\n",
    "\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "    reg_R = []\n",
    "    for i in range(m):\n",
    "        if sigma_R[i] <= eps:\n",
    "            continue\n",
    "        q = conditional_sensor_given_robot(X, i, eps=eps)\n",
    "        rec = float(np.dot(q, U_R[i, :]))\n",
    "        best = rec\n",
    "        for ip in range(m):\n",
    "            val = float(np.dot(q, U_R[ip, :]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_R.append(best - rec)\n",
    "\n",
    "    reg_S = []\n",
    "    for j in range(n):\n",
    "        if sigma_S[j] <= eps:\n",
    "            continue\n",
    "        q = conditional_robot_given_sensor(X, j, eps=eps)\n",
    "        rec = float(np.dot(q, U_S[:, j]))\n",
    "        best = rec\n",
    "        for jp in range(n):\n",
    "            val = float(np.dot(q, U_S[:, jp]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_S.append(best - rec)\n",
    "\n",
    "    return {\n",
    "        \"max_regret_R\": float(max(reg_R) if reg_R else 0.0),\n",
    "        \"max_regret_S\": float(max(reg_S) if reg_S else 0.0),\n",
    "        \"mean_regret_R\": float(np.mean(reg_R) if reg_R else 0.0),\n",
    "        \"mean_regret_S\": float(np.mean(reg_S) if reg_S else 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def find_sensor_by_mode(pols: List[SensorPolicy], mode: int) -> Optional[FixedModeSensorPolicy]:\n",
    "    for p in pols:\n",
    "        if isinstance(p, FixedModeSensorPolicy) and p.mode == mode:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policy initialization + evaluator for joint strategy\n",
    "# =============================================================================\n",
    "\n",
    "def build_initial_policies(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "    ]\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StrategyEval:\n",
    "    mean_U_R: float\n",
    "    mean_U_S: float\n",
    "    det_rate: float\n",
    "    goal_rate: float\n",
    "    mean_steps: float\n",
    "    mean_risk: float\n",
    "\n",
    "\n",
    "def evaluate_joint_strategy(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    X: np.ndarray,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    m, n = X.shape\n",
    "    probs = X.reshape(-1)\n",
    "    probs = probs / max(float(probs.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR = []\n",
    "    US = []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        idx = int(rng.choice(m * n, p=probs))\n",
    "        i, j = np.unravel_index(idx, (m, n))\n",
    "        seed = base_seed + k\n",
    "        st = rollout_episode(env, robots[i], sensors[j], M_modes=M_modes, seed=seed)\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training loop (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainHistoryRow:\n",
    "    outer_iter: int\n",
    "    m: int\n",
    "    n: int\n",
    "    nbs_obj: float\n",
    "    entropy_X: float\n",
    "    max_regret_R: float\n",
    "    max_regret_S: float\n",
    "    selfplay_UR: float\n",
    "    selfplay_US: float\n",
    "    selfplay_det: float\n",
    "    selfplay_goal: float\n",
    "    seconds: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    solver: str\n",
    "    env: GridWorldStealthEnv\n",
    "    robots: List[RobotPolicy]\n",
    "    sensors: List[SensorPolicy]\n",
    "    X: np.ndarray\n",
    "    history: List[TrainHistoryRow]\n",
    "\n",
    "\n",
    "def run_training(env: GridWorldStealthEnv, args: argparse.Namespace, solver: str, log: Logger) -> TrainResult:\n",
    "    t0_all = time.time()\n",
    "\n",
    "    grid = env.grid\n",
    "    sensor_cfg = env.sensor_cfg\n",
    "    M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "    robots, sensors = build_initial_policies(env, M_modes=M_modes)\n",
    "\n",
    "    # Optional: reduce initial set if you want smaller games.\n",
    "    # (We keep it as-is for benchmarks.)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(f\"[{solver}] Initial robots: \" + \", \".join([p.name for p in robots]))\n",
    "        log.info(f\"[{solver}] Initial sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "\n",
    "    debug_pair = None\n",
    "    if args.debug_rollout_pair:\n",
    "        parts = args.debug_rollout_pair.split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            debug_pair = (int(parts[0]), int(parts[1]))\n",
    "            log.info(f\"[{solver}] Will print one step-by-step rollout for pair {debug_pair} (only once).\")\n",
    "\n",
    "    history: List[TrainHistoryRow] = []\n",
    "    X = None\n",
    "\n",
    "    for it in range(1, args.outer_iters + 1):\n",
    "        t0 = time.time()\n",
    "        log.banner(f\"[{solver}] Outer iter {it}/{args.outer_iters}\")\n",
    "\n",
    "        U_R, U_S, _diag = evaluate_payoffs(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            M_modes=M_modes,\n",
    "            rollouts=args.rollouts_payoff,\n",
    "            base_seed=1000 + 100 * it,\n",
    "            log=log,\n",
    "            debug_rollout_pair=debug_pair,\n",
    "        )\n",
    "        debug_pair = None\n",
    "\n",
    "        # Solve NBS over joint actions\n",
    "        uR = U_R.reshape(-1)\n",
    "        uS = U_S.reshape(-1)\n",
    "\n",
    "        nbs = solve_nbs(\n",
    "            uR,\n",
    "            uS,\n",
    "            log=log,\n",
    "            disagreement=args.disagreement,\n",
    "            entropy_tau=args.entropy_tau,\n",
    "        )\n",
    "\n",
    "        m, n = U_R.shape\n",
    "        X = joint_to_matrix(nbs.x, m, n)\n",
    "        sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "        # Print top joint actions\n",
    "        top = np.argsort(-X.reshape(-1))[:min(5, X.size)]\n",
    "        log.info(f\"[{solver}] Top joint actions:\")\n",
    "        for k, idx in enumerate(top, start=1):\n",
    "            i, j = np.unravel_index(int(idx), (m, n))\n",
    "            log.info(f\"  #{k}: (R{i}:{robots[i].name}, S{j}:{sensors[j].name}) prob={X[i,j]:.4f}\")\n",
    "        log.info(f\"[{solver}] sigma_R={sigma_R.round(3)}\")\n",
    "        log.info(f\"[{solver}] sigma_S={sigma_S.round(3)}\")\n",
    "\n",
    "        # Stability diagnostics\n",
    "        regrets = compute_ce_regrets(U_R, U_S, X)\n",
    "        ent = entropy_of_joint(X)\n",
    "\n",
    "        # Self-play evaluation under joint X\n",
    "        sp = evaluate_joint_strategy(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            X,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.eval_episodes,\n",
    "            base_seed=9000 + 100 * it,\n",
    "        )\n",
    "\n",
    "        log.info(\n",
    "            f\"[{solver}] CE-regrets: maxR={regrets['max_regret_R']:.3f} maxS={regrets['max_regret_S']:.3f} | \"\n",
    "            f\"SelfPlay: UR={sp.mean_U_R:.2f} US={sp.mean_U_S:.2f} det%={100*sp.det_rate:.1f} goal%={100*sp.goal_rate:.1f} | \"\n",
    "            f\"H(X)={ent:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Best-response expansion\n",
    "        if solver == \"marginal\":\n",
    "            br_r = robot_best_response_to_mixture(\n",
    "                env,\n",
    "                sensors,\n",
    "                pi_S=sigma_S,\n",
    "                robots=robots,\n",
    "                M_modes=M_modes,\n",
    "                risk_weight=args.risk_weight_br,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "            br_s = sensor_best_response_to_mixture(\n",
    "                env,\n",
    "                robots,\n",
    "                pi_R=sigma_R,\n",
    "                candidate_modes=list(range(M_modes)),\n",
    "                M_modes=M_modes,\n",
    "                rollouts=args.rollouts_br,\n",
    "                base_seed=2000 + 100 * it,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "        elif solver == \"correlated\":\n",
    "            # FIX: choose conditional mixtures q(.|i) and q(.|j)\n",
    "            # We only check top-K recommendations to keep it tractable.\n",
    "            topK = max(1, int(args.cond_top_k))\n",
    "\n",
    "            # Robot: check the top-K robot recommendations by sigma_R\n",
    "            cand_i = list(np.argsort(-sigma_R)[:topK])\n",
    "            best_gain = 0.0\n",
    "            br_r = robots[0]\n",
    "\n",
    "            for i in cand_i:\n",
    "                qS = conditional_sensor_given_robot(X, int(i))\n",
    "                tag = f\"Cond_i{i}\"\n",
    "                pol = robot_best_response_to_mixture(\n",
    "                    env,\n",
    "                    sensors,\n",
    "                    pi_S=qS,\n",
    "                    robots=robots,\n",
    "                    M_modes=M_modes,\n",
    "                    risk_weight=args.risk_weight_br,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement: E_q[U_R(pol,j)] - E_q[U_R(i,j)]\n",
    "                # If pol is already in set, its payoff exists in U_R row of that policy.\n",
    "                # Otherwise we simulate pol against each sensor policy j.\n",
    "                if pol in robots:\n",
    "                    ip = robots.index(pol)\n",
    "                    dev = float(np.dot(qS, U_R[ip, :]))\n",
    "                else:\n",
    "                    # simulate quickly vs each sensor policy\n",
    "                    dev_vals = []\n",
    "                    for j in range(n):\n",
    "                        vals = []\n",
    "                        for kk in range(args.br_eval_rollouts):\n",
    "                            seed = 777000 + 1000 * it + 100 * i + 10 * j + kk\n",
    "                            st = rollout_episode(env, pol, sensors[j], M_modes=M_modes, seed=seed)\n",
    "                            vals.append(st.U_R)\n",
    "                        dev_vals.append(float(np.mean(vals)))\n",
    "                    dev = float(np.dot(qS, np.asarray(dev_vals)))\n",
    "\n",
    "                rec = float(np.dot(qS, U_R[i, :]))\n",
    "                gain = dev - rec\n",
    "                if gain > best_gain + 1e-9:\n",
    "                    best_gain = gain\n",
    "                    br_r = pol\n",
    "\n",
    "            if best_gain > args.add_threshold:\n",
    "                log.info(f\"[{solver}] Adding robot deviation with estimated conditional gain={best_gain:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No robot deviation above threshold (best_gain={best_gain:.3f}).\")\n",
    "\n",
    "            # Sensor: check top-K sensor recommendations by sigma_S\n",
    "            cand_j = list(np.argsort(-sigma_S)[:topK])\n",
    "            best_gain_s = 0.0\n",
    "            br_s = FixedModeSensorPolicy(0, name=\"S_dummy\")\n",
    "\n",
    "            for j in cand_j:\n",
    "                qR = conditional_robot_given_sensor(X, int(j))\n",
    "                tag = f\"Cond_j{j}\"\n",
    "                polS = sensor_best_response_to_mixture(\n",
    "                    env,\n",
    "                    robots,\n",
    "                    pi_R=qR,\n",
    "                    candidate_modes=list(range(M_modes)),\n",
    "                    M_modes=M_modes,\n",
    "                    rollouts=args.rollouts_br,\n",
    "                    base_seed=333000 + 1000 * it + 10 * j,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement for sensor\n",
    "                # rec under recommendation j is E_q[U_S(i,j)]\n",
    "                recS = float(np.dot(qR, U_S[:, j]))\n",
    "\n",
    "                # dev under mode polS.mode: if already present, use its column.\n",
    "                existing_col = None\n",
    "                for jj, spj in enumerate(sensors):\n",
    "                    if isinstance(spj, FixedModeSensorPolicy) and spj.mode == polS.mode:\n",
    "                        existing_col = jj\n",
    "                        break\n",
    "\n",
    "                if existing_col is not None:\n",
    "                    devS = float(np.dot(qR, U_S[:, existing_col]))\n",
    "                else:\n",
    "                    vals = []\n",
    "                    for kk in range(args.br_eval_rollouts):\n",
    "                        i_samp = int(np.random.default_rng(444 + kk).choice(len(robots), p=qR))\n",
    "                        seed = 888000 + 1000 * it + 10 * j + kk\n",
    "                        st = rollout_episode(env, robots[i_samp], polS, M_modes=M_modes, seed=seed)\n",
    "                        vals.append(st.U_S)\n",
    "                    devS = float(np.mean(vals))\n",
    "\n",
    "                gainS = devS - recS\n",
    "                if gainS > best_gain_s + 1e-9:\n",
    "                    best_gain_s = gainS\n",
    "                    br_s = polS\n",
    "\n",
    "            if best_gain_s > args.add_threshold:\n",
    "                log.info(f\"[{solver}] Adding sensor deviation with estimated conditional gain={best_gain_s:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No sensor deviation above threshold (best_gain={best_gain_s:.3f}).\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"solver must be marginal or correlated\")\n",
    "\n",
    "        # Add to sets (dedupe)\n",
    "        if br_r not in robots:\n",
    "            robots.append(br_r)\n",
    "\n",
    "        if isinstance(br_s, FixedModeSensorPolicy):\n",
    "            if find_sensor_by_mode(sensors, br_s.mode) is None:\n",
    "                sensors.append(br_s)\n",
    "            else:\n",
    "                log.info(f\"[{solver}] Sensor mode {br_s.mode} already present; not adding duplicate.\")\n",
    "\n",
    "        seconds = float(time.time() - t0)\n",
    "        history.append(\n",
    "            TrainHistoryRow(\n",
    "                outer_iter=it,\n",
    "                m=len(robots),\n",
    "                n=len(sensors),\n",
    "                nbs_obj=float(nbs.obj),\n",
    "                entropy_X=float(ent),\n",
    "                max_regret_R=float(regrets[\"max_regret_R\"]),\n",
    "                max_regret_S=float(regrets[\"max_regret_S\"]),\n",
    "                selfplay_UR=float(sp.mean_U_R),\n",
    "                selfplay_US=float(sp.mean_U_S),\n",
    "                selfplay_det=float(sp.det_rate),\n",
    "                selfplay_goal=float(sp.goal_rate),\n",
    "                seconds=seconds,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        log.info(f\"[{solver}] Sets: |Pi_R|={len(robots)} |Pi_S|={len(sensors)} | iter_seconds={seconds:.2f}\")\n",
    "\n",
    "    if X is None:\n",
    "        raise RuntimeError(\"Training produced no X\")\n",
    "\n",
    "    log.banner(f\"[{solver}] Finished\")\n",
    "    log.info(f\"[{solver}] Final robots: \" + \", \".join([p.name for p in robots]))\n",
    "    log.info(f\"[{solver}] Final sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "    log.info(f\"[{solver}] Total time: {time.time()-t0_all:.2f}s\")\n",
    "\n",
    "    return TrainResult(solver=solver, env=env, robots=robots, sensors=sensors, X=X, history=history)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Benchmarks + plotting\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BenchCell:\n",
    "    UR: float\n",
    "    US: float\n",
    "    det: float\n",
    "    goal: float\n",
    "\n",
    "\n",
    "def run_policy_matrix_benchmark(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_methods: List[RobotPolicy],\n",
    "    sensor_methods: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[Tuple[int, int], BenchCell]:\n",
    "    res: Dict[Tuple[int, int], BenchCell] = {}\n",
    "    for i, rp in enumerate(robot_methods):\n",
    "        for j, sp in enumerate(sensor_methods):\n",
    "            UR = []\n",
    "            US = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            for k in range(episodes):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed)\n",
    "                UR.append(st.U_R)\n",
    "                US.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "            res[(i, j)] = BenchCell(\n",
    "                UR=float(np.mean(UR)),\n",
    "                US=float(np.mean(US)),\n",
    "                det=float(det / episodes),\n",
    "                goal=float(goal / episodes),\n",
    "            )\n",
    "    return res\n",
    "\n",
    "\n",
    "def safe_makedirs(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_history_csv(hist: List[TrainHistoryRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    fields = list(TrainHistoryRow.__annotations__.keys())\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fields)\n",
    "        w.writeheader()\n",
    "        for row in hist:\n",
    "            w.writerow({k: getattr(row, k) for k in fields})\n",
    "\n",
    "\n",
    "def plot_training_curves(results: List[TrainResult], outdir: str) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # 1) NBS objective\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.nbs_obj for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"NBS objective\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_nbs_obj.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Max conditional regrets\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_R for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: maxRegret_R\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_S for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: maxRegret_S\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Max conditional regret\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_max_regrets.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 3) Entropy of X\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.entropy_X for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Entropy H(X)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_entropy_X.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 4) Self-play outcomes\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_UR for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: UR\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_US for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: US\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Expected utility under X\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_selfplay_utils.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_benchmark_bars(\n",
    "    robot_names: List[str],\n",
    "    sensor_names: List[str],\n",
    "    bench: Dict[Tuple[int, int], BenchCell],\n",
    "    outdir: str,\n",
    ") -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # For each sensor, bar chart of robot UR\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].UR for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Robot utility\")\n",
    "        plt.title(f\"Robot utility vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_UR_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # For each sensor, bar chart of robot goal rate\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].goal for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Goal rate\")\n",
    "        plt.title(f\"Robot goal rate vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_goal_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main pipeline wrapper\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(args: argparse.Namespace) -> None:\n",
    "    log = Logger(args.log_level)\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.banner(\"[PIPELINE] Stealth grid game\")\n",
    "        log.info(f\"Grid: {grid.width}x{grid.height} start={grid.start} goal={grid.goal} obstacles={len(grid.obstacles)}\")\n",
    "        log.info(f\"Sensors: {sensor_cfg.sensors} radius={sensor_cfg.radius} base_p={sensor_cfg.base_p} hotspot_p={sensor_cfg.hotspot_p}\")\n",
    "        if log.k >= 2:\n",
    "            log.debug(\"ASCII map:\")\n",
    "            print_grid_ascii(grid, sensor_cfg)\n",
    "\n",
    "    solvers: List[str]\n",
    "    if args.solver == \"both\":\n",
    "        solvers = [\"marginal\", \"correlated\"]\n",
    "    else:\n",
    "        solvers = [args.solver]\n",
    "\n",
    "    results: List[TrainResult] = []\n",
    "    for s in solvers:\n",
    "        # Use a fresh env copy per solver (to avoid RNG coupling)\n",
    "        env_s = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "        results.append(run_training(env_s, args, solver=s, log=log))\n",
    "\n",
    "    if args.results_dir:\n",
    "        safe_makedirs(args.results_dir)\n",
    "        for r in results:\n",
    "            save_history_csv(r.history, os.path.join(args.results_dir, f\"history_{r.solver}.csv\"))\n",
    "\n",
    "    if args.save_plots and args.results_dir:\n",
    "        plot_training_curves(results, outdir=args.results_dir)\n",
    "\n",
    "    # Benchmarks on the same game\n",
    "    if args.run_benchmarks:\n",
    "        log.banner(\"[BENCH] Heuristics on same game\")\n",
    "        M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "        # Robot heuristics\n",
    "        #  - fixed paths from initial set\n",
    "        robots_init, sensors_init = build_initial_policies_for_bench(env, M_modes)\n",
    "\n",
    "        # Sensor heuristics\n",
    "        sensor_methods: List[SensorPolicy] = [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=args.seed + 123, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "        bench = run_policy_matrix_benchmark(\n",
    "            env,\n",
    "            robot_methods=robots_init,\n",
    "            sensor_methods=sensor_methods,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.bench_episodes,\n",
    "            base_seed=555000,\n",
    "        )\n",
    "\n",
    "        # Print a compact table\n",
    "        robot_names = [p.name for p in robots_init]\n",
    "        sensor_names = [p.name for p in sensor_methods]\n",
    "        for j, sname in enumerate(sensor_names):\n",
    "            log.info(f\"[BENCH] Sensor={sname}\")\n",
    "            for i, rname in enumerate(robot_names):\n",
    "                cell = bench[(i, j)]\n",
    "                log.info(f\"  Robot={rname:22s} UR={cell.UR:8.2f} goal%={100*cell.goal:5.1f} det%={100*cell.det:5.1f}\")\n",
    "\n",
    "        if args.save_plots and args.results_dir:\n",
    "            plot_benchmark_bars(robot_names, sensor_names, bench, outdir=args.results_dir)\n",
    "\n",
    "\n",
    "# Helper: initial robot set for benchmark (without the solver-added BR policies)\n",
    "\n",
    "def build_initial_policies_for_bench(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    # Static risk-aware A* under UNIFORM mode belief\n",
    "    uniform_mode = np.full(M_modes, 1.0 / M_modes)\n",
    "\n",
    "    # Construct risk map directly under uniform belief\n",
    "    H, W = grid.height, grid.width\n",
    "    risk_uniform = np.zeros((H, W), dtype=float)\n",
    "    risk_worst = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in grid.obstacles:\n",
    "                risk_uniform[y, x] = np.nan\n",
    "                risk_worst[y, x] = np.nan\n",
    "                continue\n",
    "            vals = [env.true_detection_prob((x, y), m) for m in range(M_modes)]\n",
    "            risk_uniform[y, x] = float(np.dot(uniform_mode, vals))\n",
    "            risk_worst[y, x] = float(np.max(vals))\n",
    "\n",
    "    p_uniform = plan_risk_weighted_path(env, risk_uniform, risk_weight=12.0)\n",
    "    p_worst = plan_risk_weighted_path(env, risk_worst, risk_weight=12.0)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        FixedPathPolicy(p_uniform, \"R_RiskAStar_Uniform\"),\n",
    "        FixedPathPolicy(p_worst, \"R_RiskAStar_WorstCase\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "    ]\n",
    "\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLI\n",
    "# =============================================================================\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> Tuple[argparse.Namespace, List[str]]:\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "    p.add_argument(\"--log-level\", type=str, default=\"INFO\", choices=[\"QUIET\", \"INFO\", \"DEBUG\"])\n",
    "\n",
    "    p.add_argument(\"--solver\", type=str, default=\"correlated\", choices=[\"marginal\", \"correlated\", \"both\"])\n",
    "\n",
    "    p.add_argument(\"--outer-iters\", type=int, default=3)\n",
    "\n",
    "    p.add_argument(\"--rollouts-payoff\", type=int, default=20)\n",
    "    p.add_argument(\"--rollouts-br\", type=int, default=30)\n",
    "    p.add_argument(\"--risk-weight-br\", type=float, default=12.0)\n",
    "\n",
    "    # NBS knobs\n",
    "    p.add_argument(\"--disagreement\", type=str, default=\"minminus\", choices=[\"minminus\", \"uniform\"])\n",
    "    p.add_argument(\"--entropy-tau\", type=float, default=0.0)\n",
    "\n",
    "    # Fix mismatch knobs\n",
    "    p.add_argument(\"--cond-top-k\", type=int, default=2, help=\"How many top recommendations to check for conditional BRs\")\n",
    "    p.add_argument(\"--br-eval-rollouts\", type=int, default=8, help=\"Small evaluation rollouts for new deviations\")\n",
    "    p.add_argument(\"--add-threshold\", type=float, default=0.25, help=\"Minimum estimated conditional gain to add a deviation policy\")\n",
    "\n",
    "    # Eval episodes under joint X (self-play)\n",
    "    p.add_argument(\"--eval-episodes\", type=int, default=60)\n",
    "\n",
    "    # Debug\n",
    "    p.add_argument(\"--debug-rollout-pair\", type=str, default=\"\", help=\"Print one step-by-step rollout for i,j\")\n",
    "\n",
    "    # Outputs\n",
    "    p.add_argument(\"--results-dir\", type=str, default=\"results\", help=\"Directory for csv/plots\")\n",
    "    p.add_argument(\"--save-plots\", action=\"store_true\")\n",
    "\n",
    "    # Benchmarks\n",
    "    p.add_argument(\"--run-benchmarks\", action=\"store_true\")\n",
    "    p.add_argument(\"--bench-episodes\", type=int, default=80)\n",
    "\n",
    "    args, unknown = p.parse_known_args(args=argv)\n",
    "    return args, unknown\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    args, unknown = parse_args(argv=argv)\n",
    "    if unknown and (\"ipykernel\" not in sys.modules):\n",
    "        print(f\"[WARN] Ignoring unknown CLI args: {unknown}\")\n",
    "    if args.debug_rollout_pair.strip() == \"\":\n",
    "        args.debug_rollout_pair = \"\"\n",
    "    run_pipeline(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and (\"ipykernel\" not in sys.modules):\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d51c9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"approach2_robust_correlated.py\n",
    "\n",
    "Robust, reduced-log implementation of \"Approach 2\" on a stealth gridworld POMDP,\n",
    "with a FIX for the conceptual mismatch:\n",
    "\n",
    "  Old mismatch: solve a joint distribution x*(i,j) over (robot policy i, sensor policy j)\n",
    "  but then compute best responses against the MARGINALS sigma_R, sigma_S.\n",
    "\n",
    "  Fix here: treat x* as a CORRELATION DEVICE (mediator) and compute BRs against\n",
    "  CONDITIONAL distributions:\n",
    "\n",
    "      q_S(.|i) = X[i,:] / sigma_R[i]   (sensor conditional given robot recommendation i)\n",
    "      q_R(.|j) = X[:,j] / sigma_S[j]   (robot conditional given sensor recommendation j)\n",
    "\n",
    "  Then add the most profitable *deviation* policy (oracle) based on the most violated\n",
    "  conditional recommendation.\n",
    "\n",
    "This makes the PSRO-style expansion step consistent with a correlated-strategy viewpoint.\n",
    "\n",
    "Also included:\n",
    "  - Benchmark harness vs simple heuristics on the same game.\n",
    "  - Saved plots (training curves + bar charts) for presentations.\n",
    "\n",
    "Run (script):\n",
    "  python approach2_robust_correlated.py --solver correlated --outer-iters 3 --save-plots\n",
    "\n",
    "Run (notebook):\n",
    "  main(argv=[\"--solver\",\"both\",\"--outer-iters\",\"3\",\"--save-plots\"])  # ignore ipykernel args\n",
    "\n",
    "Tips to encourage MULTIMODAL x* (useful for your \"mode recovery\"):\n",
    "  --disagreement uniform --entropy-tau 0.02\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import heapq\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Stage-based logger with QUIET/INFO/DEBUG.\"\"\"\n",
    "\n",
    "    LEVELS = {\"QUIET\": 0, \"INFO\": 1, \"DEBUG\": 2}\n",
    "\n",
    "    def __init__(self, level: str = \"INFO\"):\n",
    "        level = level.upper()\n",
    "        if level not in self.LEVELS:\n",
    "            raise ValueError(f\"Unknown log level: {level}. Use QUIET/INFO/DEBUG\")\n",
    "        self.level = level\n",
    "        self.k = self.LEVELS[level]\n",
    "\n",
    "    def banner(self, title: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(\"\" + \"=\" * 100)\n",
    "            print(title)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "    def info(self, msg: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(msg)\n",
    "\n",
    "    def debug(self, msg: str) -> None:\n",
    "        if self.k >= 2:\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Types\n",
    "# =============================================================================\n",
    "\n",
    "Action = Tuple[int, int]  # (dx, dy)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Environment (grid + hidden sensor mode + noisy alarm observation)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GridConfig:\n",
    "    width: int\n",
    "    height: int\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    obstacles: frozenset\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SensorConfig:\n",
    "    sensors: Tuple[Tuple[int, int], ...]\n",
    "    radius: int\n",
    "    base_p: float\n",
    "    hotspot_p: float\n",
    "\n",
    "\n",
    "class GridWorldStealthEnv:\n",
    "    \"\"\"Grid world with detection risk controlled by a hidden/selected sensor 'mode'.\"\"\"\n",
    "\n",
    "    def __init__(self, grid: GridConfig, sensor_cfg: SensorConfig, fp: float = 0.05, fn: float = 0.10, seed: int = 0):\n",
    "        self.grid = grid\n",
    "        self.sensor_cfg = sensor_cfg\n",
    "        self.fp = float(fp)\n",
    "        self.fn = float(fn)\n",
    "\n",
    "        if not (0.0 <= self.fp <= 1.0 and 0.0 <= self.fn <= 1.0):\n",
    "            raise ValueError(\"fp and fn must be in [0,1].\")\n",
    "        if not (0.0 <= sensor_cfg.base_p <= 1.0 and 0.0 <= sensor_cfg.hotspot_p <= 1.0):\n",
    "            raise ValueError(\"base_p and hotspot_p must be in [0,1].\")\n",
    "        if sensor_cfg.radius < 0:\n",
    "            raise ValueError(\"radius must be >= 0\")\n",
    "        if len(sensor_cfg.sensors) == 0:\n",
    "            raise ValueError(\"Need at least one sensor center.\")\n",
    "\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.reset(sensor_mode=0)\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    def reset(self, sensor_mode: int = 0) -> Dict[str, Any]:\n",
    "        self.t = 0\n",
    "        self.pos = self.grid.start\n",
    "        self.sensor_mode = int(sensor_mode)\n",
    "        self.detected = False\n",
    "        self.total_true_risk = 0.0\n",
    "        return {\"pos\": self.pos, \"t\": self.t}\n",
    "\n",
    "    def in_bounds(self, p: Tuple[int, int]) -> bool:\n",
    "        x, y = p\n",
    "        return 0 <= x < self.grid.width and 0 <= y < self.grid.height\n",
    "\n",
    "    def is_free(self, p: Tuple[int, int]) -> bool:\n",
    "        return self.in_bounds(p) and (p not in self.grid.obstacles)\n",
    "\n",
    "    def true_detection_prob(self, p: Tuple[int, int], mode: int) -> float:\n",
    "        if not (0 <= mode < len(self.sensor_cfg.sensors)):\n",
    "            raise ValueError(f\"mode {mode} out of range\")\n",
    "        sx, sy = self.sensor_cfg.sensors[mode]\n",
    "        x, y = p\n",
    "        d = abs(x - sx) + abs(y - sy)\n",
    "        return self.sensor_cfg.hotspot_p if d <= self.sensor_cfg.radius else self.sensor_cfg.base_p\n",
    "\n",
    "    def observation_prob(self, alarm: int, p_true: float) -> float:\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        if alarm == 1:\n",
    "            return p_alarm\n",
    "        if alarm == 0:\n",
    "            return 1.0 - p_alarm\n",
    "        raise ValueError(\"alarm must be 0 or 1\")\n",
    "\n",
    "    def step(self, a: Action) -> Dict[str, Any]:\n",
    "        if self.detected:\n",
    "            return {\"pos\": self.pos, \"t\": self.t, \"alarm\": 1, \"p_true\": 1.0, \"detected\": True, \"done\": True}\n",
    "\n",
    "        self.t += 1\n",
    "        nx = self.pos[0] + int(a[0])\n",
    "        ny = self.pos[1] + int(a[1])\n",
    "        np_ = (nx, ny)\n",
    "        if self.is_free(np_):\n",
    "            self.pos = np_\n",
    "\n",
    "        p_true = float(self.true_detection_prob(self.pos, self.sensor_mode))\n",
    "        self.total_true_risk += p_true\n",
    "\n",
    "        if self.rng.random() < p_true:\n",
    "            self.detected = True\n",
    "\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        alarm = 1 if (self.rng.random() < p_alarm) else 0\n",
    "\n",
    "        done = self.detected or (self.pos == self.grid.goal) or (self.t >= 200)\n",
    "        return {\"pos\": self.pos, \"t\": self.t, \"alarm\": alarm, \"p_true\": p_true, \"detected\": self.detected, \"done\": done}\n",
    "\n",
    "\n",
    "def build_two_corridor_grid(width: int = 15, height: int = 9) -> GridConfig:\n",
    "    obstacles = set()\n",
    "    wall_x = width // 2\n",
    "    gap_ys = {2, 6}\n",
    "    for y in range(height):\n",
    "        if y not in gap_ys:\n",
    "            obstacles.add((wall_x, y))\n",
    "\n",
    "    start = (1, height - 2)\n",
    "    goal = (width - 2, 1)\n",
    "    if start in obstacles or goal in obstacles:\n",
    "        raise RuntimeError(\"Start/goal blocked unexpectedly\")\n",
    "\n",
    "    return GridConfig(width=width, height=height, start=start, goal=goal, obstacles=frozenset(obstacles))\n",
    "\n",
    "\n",
    "def print_grid_ascii(grid: GridConfig, sensor_cfg: SensorConfig) -> None:\n",
    "    W, H = grid.width, grid.height\n",
    "    obs = set(grid.obstacles)\n",
    "    sens = set(sensor_cfg.sensors)\n",
    "    for y in range(H):\n",
    "        row = []\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p == grid.start:\n",
    "                row.append(\"R\")\n",
    "            elif p == grid.goal:\n",
    "                row.append(\"G\")\n",
    "            elif p in sens:\n",
    "                row.append(\"S\")\n",
    "            elif p in obs:\n",
    "                row.append(\"#\")\n",
    "            else:\n",
    "                row.append(\".\")\n",
    "        print(\"\".join(row))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Belief over modes\n",
    "# =============================================================================\n",
    "\n",
    "class ModeBelief:\n",
    "    \"\"\"Exact belief over discrete modes m in {0..M-1}.\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, init: Optional[np.ndarray] = None):\n",
    "        self.M = int(M)\n",
    "        if self.M <= 0:\n",
    "            raise ValueError(\"M must be >= 1\")\n",
    "        if init is None:\n",
    "            self.b = np.full(self.M, 1.0 / self.M)\n",
    "        else:\n",
    "            init = np.asarray(init, dtype=float).reshape(-1)\n",
    "            if init.shape != (self.M,):\n",
    "                raise ValueError(\"init shape mismatch\")\n",
    "            if np.any(init < 0):\n",
    "                raise ValueError(\"init must be nonnegative\")\n",
    "            s = float(init.sum())\n",
    "            self.b = init / s if s > 0 else np.full(self.M, 1.0 / self.M)\n",
    "\n",
    "    def update(self, env: GridWorldStealthEnv, alarm: int, pos: Tuple[int, int], eps: float = 1e-12) -> None:\n",
    "        like = np.zeros(self.M, dtype=float)\n",
    "        for m in range(self.M):\n",
    "            p_true = env.true_detection_prob(pos, m)\n",
    "            like[m] = env.observation_prob(alarm, p_true)\n",
    "        post = self.b * like\n",
    "        Z = float(post.sum())\n",
    "        if (not np.isfinite(Z)) or Z < eps:\n",
    "            return\n",
    "        self.b = post / Z\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policies\n",
    "# =============================================================================\n",
    "\n",
    "class RobotPolicy:\n",
    "    name: str = \"RobotPolicy\"\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedPathPolicy(RobotPolicy):\n",
    "    def __init__(self, path: List[Tuple[int, int]], name: str):\n",
    "        if len(path) < 2:\n",
    "            raise ValueError(\"Path must have >=2 states\")\n",
    "        self.path = list(path)\n",
    "        self.name = str(name)\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        try:\n",
    "            self._idx = self.path.index(start_pos)\n",
    "        except ValueError:\n",
    "            self._idx = 0\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        cur = env.pos\n",
    "        if self._idx >= len(self.path) - 1:\n",
    "            return (0, 0)\n",
    "        if cur != self.path[self._idx]:\n",
    "            try:\n",
    "                self._idx = self.path.index(cur, self._idx)\n",
    "            except ValueError:\n",
    "                return (0, 0)\n",
    "        nxt = self.path[self._idx + 1]\n",
    "        dx = int(np.clip(nxt[0] - cur[0], -1, 1))\n",
    "        dy = int(np.clip(nxt[1] - cur[1], -1, 1))\n",
    "        self._idx += 1\n",
    "        return (dx, dy)\n",
    "\n",
    "\n",
    "class RandomPolicy(RobotPolicy):\n",
    "    \"\"\"Reproducible random policy using env.rng.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"R_Random\"):\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        candidates: List[Action] = []\n",
    "        x, y = env.pos\n",
    "        for a in [(1, 0), (-1, 0), (0, 1), (0, -1), (0, 0)]:\n",
    "            np_ = (x + a[0], y + a[1])\n",
    "            if env.is_free(np_):\n",
    "                candidates.append(a)\n",
    "        if not candidates:\n",
    "            return (0, 0)\n",
    "        return candidates[int(env.rng.integers(0, len(candidates)))]\n",
    "\n",
    "\n",
    "class OnlineBeliefReplanPolicy(RobotPolicy):\n",
    "    \"\"\"POMDP-ish heuristic: replan each step using risk map induced by current belief b_t.\"\"\"\n",
    "\n",
    "    def __init__(self, env: GridWorldStealthEnv, risk_weight: float = 12.0, name: str = \"R_OnlineBeliefReplan\"):\n",
    "        self.env = env\n",
    "        self.risk_weight = float(risk_weight)\n",
    "        self.name = name\n",
    "        self._cached_next: Optional[Tuple[int, int]] = None\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        self._cached_next = None\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        # Build a risk map from belief over modes.\n",
    "        mode_probs = belief.b\n",
    "\n",
    "        def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "            x, y = to\n",
    "            # expected risk at to under belief\n",
    "            r = 0.0\n",
    "            for m, pm in enumerate(mode_probs):\n",
    "                r += float(pm) * float(env.true_detection_prob(to, m))\n",
    "            return 1.0 + self.risk_weight * r\n",
    "\n",
    "        # Plan from current pos to goal (one-step receding horizon)\n",
    "        try:\n",
    "            path = astar_path(env.grid, env.pos, env.grid.goal, step_cost)\n",
    "            if len(path) < 2:\n",
    "                return (0, 0)\n",
    "            nxt = path[1]\n",
    "            dx = int(np.clip(nxt[0] - env.pos[0], -1, 1))\n",
    "            dy = int(np.clip(nxt[1] - env.pos[1], -1, 1))\n",
    "            return (dx, dy)\n",
    "        except Exception:\n",
    "            return (0, 0)\n",
    "\n",
    "\n",
    "class SensorPolicy:\n",
    "    name: str = \"SensorPolicy\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedModeSensorPolicy(SensorPolicy):\n",
    "    def __init__(self, mode: int, name: Optional[str] = None):\n",
    "        self.mode = int(mode)\n",
    "        self.name = name or f\"S_Mode{mode}\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return self.mode\n",
    "\n",
    "\n",
    "class AlternatingSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Simple sensor heuristic for benchmarks: alternate modes 0,1,0,1,...\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, name: str = \"S_Alternate\"):\n",
    "        self.M = int(M)\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(t % self.M)\n",
    "\n",
    "\n",
    "class RandomModeSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Benchmark sensor: random mode each step (uses numpy Generator for reproducibility).\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, seed: int = 0, name: str = \"S_RandomMode\"):\n",
    "        self.M = int(M)\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(self.rng.integers(0, self.M))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# A* (used for planning)\n",
    "# =============================================================================\n",
    "\n",
    "def astar_path(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    max_expansions: int = 250_000,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    if start == goal:\n",
    "        return [start]\n",
    "\n",
    "    def h(p: Tuple[int, int]) -> float:\n",
    "        return abs(p[0] - goal[0]) + abs(p[1] - goal[1])\n",
    "\n",
    "    def neighbors(p: Tuple[int, int]):\n",
    "        x, y = p\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            np_ = (x + dx, y + dy)\n",
    "            if 0 <= np_[0] < grid.width and 0 <= np_[1] < grid.height and np_ not in grid.obstacles:\n",
    "                yield np_\n",
    "\n",
    "    open_heap: List[Tuple[float, float, Tuple[int, int]]] = []\n",
    "    heapq.heappush(open_heap, (h(start), 0.0, start))\n",
    "\n",
    "    came: Dict[Tuple[int, int], Optional[Tuple[int, int]]] = {start: None}\n",
    "    gscore: Dict[Tuple[int, int], float] = {start: 0.0}\n",
    "\n",
    "    expansions = 0\n",
    "    while open_heap:\n",
    "        _, _, cur = heapq.heappop(open_heap)\n",
    "        expansions += 1\n",
    "        if cur == goal:\n",
    "            path: List[Tuple[int, int]] = []\n",
    "            while cur is not None:\n",
    "                path.append(cur)\n",
    "                cur = came[cur]\n",
    "            path.reverse()\n",
    "            return path\n",
    "        if expansions > max_expansions:\n",
    "            raise RuntimeError(\"A* exceeded max expansions\")\n",
    "\n",
    "        for nb in neighbors(cur):\n",
    "            tentative = gscore[cur] + float(step_cost(cur, nb))\n",
    "            if (nb not in gscore) or (tentative < gscore[nb] - 1e-12):\n",
    "                gscore[nb] = tentative\n",
    "                came[nb] = cur\n",
    "                heapq.heappush(open_heap, (tentative + h(nb), tentative, nb))\n",
    "\n",
    "    raise RuntimeError(\"A* failed: unreachable goal\")\n",
    "\n",
    "\n",
    "def astar_via_waypoint(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    waypoint: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    ") -> List[Tuple[int, int]]:\n",
    "    p1 = astar_path(grid, start, waypoint, step_cost)\n",
    "    p2 = astar_path(grid, waypoint, goal, step_cost)\n",
    "    return p1[:-1] + p2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rollouts + payoffs\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStats:\n",
    "    steps: int\n",
    "    reached_goal: bool\n",
    "    detected: bool\n",
    "    total_true_risk: float\n",
    "    U_R: float\n",
    "    U_S: float\n",
    "\n",
    "\n",
    "def rollout_episode(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    seed: int,\n",
    "    max_steps: int = 200,\n",
    "    lambda_risk: float = 1.0,\n",
    "    det_penalty: float = 50.0,\n",
    "    sensor_energy_per_step: float = 0.2,\n",
    "    step_debug: bool = False,\n",
    ") -> EpisodeStats:\n",
    "    env.seed(seed)\n",
    "    sensor.reset()\n",
    "    env.reset(sensor_mode=sensor.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)\n",
    "    robot.reset(env.pos)\n",
    "\n",
    "    total_risk = 0.0\n",
    "    last_alarm: Optional[int] = None\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.sensor_mode = sensor.select_mode(env.t)\n",
    "        a = robot.act(env, belief, last_alarm)\n",
    "        out = env.step(a)\n",
    "\n",
    "        total_risk += float(out[\"p_true\"])\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "        last_alarm = int(out[\"alarm\"])\n",
    "\n",
    "        if step_debug:\n",
    "            print(\n",
    "                f\"[Step] t={out['t']:3d} pos={out['pos']} a={a} p_true={out['p_true']:.3f} \"\n",
    "                f\"alarm={out['alarm']} det={out['detected']} done={out['done']} b={belief.b.round(3)}\"\n",
    "            )\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = bool(env.detected)\n",
    "    steps = int(env.t)\n",
    "\n",
    "    cost_R = steps + lambda_risk * total_risk + (det_penalty if detected else 0.0)\n",
    "    U_R = -float(cost_R)\n",
    "\n",
    "    U_S = float((det_penalty if detected else 0.0) + lambda_risk * total_risk - sensor_energy_per_step * steps)\n",
    "\n",
    "    return EpisodeStats(steps=steps, reached_goal=reached_goal, detected=detected, total_true_risk=float(total_risk), U_R=U_R, U_S=U_S)\n",
    "\n",
    "\n",
    "def evaluate_payoffs(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    debug_rollout_pair: Optional[Tuple[int, int]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[int, int], Dict[str, float]]]:\n",
    "    m, n = len(robots), len(sensors)\n",
    "    U_R = np.zeros((m, n), dtype=float)\n",
    "    U_S = np.zeros((m, n), dtype=float)\n",
    "    diag: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    log.info(f\"[Eval] Estimating payoffs: m={m}, n={n}, rollouts={rollouts}, base_seed={base_seed}\")\n",
    "\n",
    "    for i, rpol in enumerate(robots):\n",
    "        for j, spol in enumerate(sensors):\n",
    "            step_debug = (debug_rollout_pair == (i, j))\n",
    "\n",
    "            r_list: List[float] = []\n",
    "            s_list: List[float] = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            steps_list: List[int] = []\n",
    "            risk_list: List[float] = []\n",
    "\n",
    "            for k in range(rollouts):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rpol, spol, M_modes=M_modes, seed=seed, step_debug=step_debug)\n",
    "                r_list.append(st.U_R)\n",
    "                s_list.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "                steps_list.append(st.steps)\n",
    "                risk_list.append(st.total_true_risk)\n",
    "\n",
    "                if step_debug:\n",
    "                    step_debug = False  # only show one rollout\n",
    "\n",
    "            U_R[i, j] = float(np.mean(r_list))\n",
    "            U_S[i, j] = float(np.mean(s_list))\n",
    "\n",
    "            diag[(i, j)] = {\n",
    "                \"det_rate\": det / rollouts,\n",
    "                \"goal_rate\": goal / rollouts,\n",
    "                \"mean_steps\": float(np.mean(steps_list)),\n",
    "                \"mean_risk\": float(np.mean(risk_list)),\n",
    "                \"std_UR\": float(np.std(r_list)),\n",
    "                \"std_US\": float(np.std(s_list)),\n",
    "            }\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(\"[Eval] Compact payoff summary:\")\n",
    "        for i, rpol in enumerate(robots):\n",
    "            for j, spol in enumerate(sensors):\n",
    "                d = diag[(i, j)]\n",
    "                log.info(\n",
    "                    f\"  (R{i}:{rpol.name}, S{j}:{spol.name}) \"\n",
    "                    f\"UR={U_R[i,j]:8.3f}±{d['std_UR']:.2f} | \"\n",
    "                    f\"US={U_S[i,j]:8.3f}±{d['std_US']:.2f} | \"\n",
    "                    f\"det%={100*d['det_rate']:5.1f} goal%={100*d['goal_rate']:5.1f} \"\n",
    "                    f\"steps={d['mean_steps']:.1f} risk={d['mean_risk']:.2f}\"\n",
    "                )\n",
    "\n",
    "    return U_R, U_S, diag\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NBS solver (with optional entropy regularization)\n",
    "# =============================================================================\n",
    "\n",
    "def project_simplex(v: np.ndarray, z: float = 1.0) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    if v.size == 0:\n",
    "        raise ValueError(\"Empty vector\")\n",
    "    if z <= 0:\n",
    "        raise ValueError(\"z must be > 0\")\n",
    "\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, v.size + 1) > (cssv - z))[0]\n",
    "    if rho.size == 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    rho = int(rho[-1])\n",
    "    theta = (cssv[rho] - z) / (rho + 1.0)\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    s = float(w.sum())\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    return w * (z / s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NBSResult:\n",
    "    x: np.ndarray\n",
    "    obj: float\n",
    "    gains: Tuple[float, float]\n",
    "    support: int\n",
    "\n",
    "\n",
    "def solve_nbs(\n",
    "    uR: np.ndarray,\n",
    "    uS: np.ndarray,\n",
    "    log: Logger,\n",
    "    max_iters: int = 400,\n",
    "    alpha: float = 0.5,\n",
    "    tol_l1: float = 1e-6,\n",
    "    kappa: float = 1e-6,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    ") -> NBSResult:\n",
    "    uR = np.asarray(uR, dtype=float).reshape(-1)\n",
    "    uS = np.asarray(uS, dtype=float).reshape(-1)\n",
    "    if uR.shape != uS.shape:\n",
    "        raise ValueError(\"uR and uS must have same shape\")\n",
    "    d = uR.size\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Need >=2 joint actions\")\n",
    "\n",
    "    unif = np.full(d, 1.0 / d)\n",
    "\n",
    "    disagreement = disagreement.lower().strip()\n",
    "    if disagreement == \"minminus\":\n",
    "        dR = float(np.min(uR) - 1.0)\n",
    "        dS = float(np.min(uS) - 1.0)\n",
    "    elif disagreement == \"uniform\":\n",
    "        dR = float(uR @ unif)\n",
    "        dS = float(uS @ unif)\n",
    "    else:\n",
    "        raise ValueError(\"disagreement must be 'minminus' or 'uniform'\")\n",
    "\n",
    "    x = unif.copy()\n",
    "\n",
    "    def gains(xv: np.ndarray) -> Tuple[float, float]:\n",
    "        return float(uR @ xv - dR), float(uS @ xv - dS)\n",
    "\n",
    "    def entropy(xv: np.ndarray) -> float:\n",
    "        xx = np.clip(xv, 1e-12, 1.0)\n",
    "        return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "    def obj(xv: np.ndarray) -> float:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return float(np.log(gR) + np.log(gS) + entropy_tau * entropy(xv))\n",
    "\n",
    "    def grad(xv: np.ndarray) -> np.ndarray:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        g = (uR / gR) + (uS / gS)\n",
    "        if entropy_tau > 0:\n",
    "            xx = np.clip(xv, 1e-12, 1.0)\n",
    "            g += entropy_tau * (-(np.log(xx) + 1.0))\n",
    "        return g\n",
    "\n",
    "    last = obj(x)\n",
    "    log.info(f\"[NBS] d={d} disagreement=({dR:.3f},{dS:.3f}) entropy_tau={entropy_tau:.3g}\")\n",
    "\n",
    "    for t in range(1, max_iters + 1):\n",
    "        g = grad(x)\n",
    "        a = alpha\n",
    "        improved = False\n",
    "        for _ in range(30):\n",
    "            x_new = project_simplex(x + a * g)\n",
    "            new_obj = obj(x_new)\n",
    "            if new_obj >= last - 1e-12:\n",
    "                improved = True\n",
    "                break\n",
    "            a *= 0.5\n",
    "            if a < 1e-6:\n",
    "                break\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "        delta = float(np.linalg.norm(x_new - x, ord=1))\n",
    "        x = x_new\n",
    "        last = new_obj\n",
    "\n",
    "        if log.k >= 2 and (t <= 5 or t % 25 == 0):\n",
    "            gR, gS = gains(x)\n",
    "            top = np.argsort(-x)[:5]\n",
    "            top_str = \", \".join([f\"{i}:{x[i]:.3f}\" for i in top])\n",
    "            log.debug(f\"[NBS][it={t:3d}] obj={last:.6f} gains=({gR:.3f},{gS:.3f}) L1={delta:.2e} top={top_str}\")\n",
    "\n",
    "        if delta < tol_l1:\n",
    "            break\n",
    "\n",
    "    gR, gS = gains(x)\n",
    "    support = int(np.sum(x > 1e-6))\n",
    "    log.info(f\"[NBS] done: obj={last:.6f} gains=({gR:.3f},{gS:.3f}) support={support}/{d}\")\n",
    "\n",
    "    return NBSResult(x=x, obj=float(last), gains=(float(gR), float(gS)), support=support)\n",
    "\n",
    "\n",
    "def joint_to_matrix(x: np.ndarray, m: int, n: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size != m * n:\n",
    "        raise ValueError(\"x size mismatch\")\n",
    "    X = x.reshape((m, n))\n",
    "    s = float(X.sum())\n",
    "    if not np.isfinite(s) or abs(s - 1.0) > 1e-6:\n",
    "        # Renormalize defensively\n",
    "        X = X / max(s, 1e-12)\n",
    "    return X\n",
    "\n",
    "\n",
    "def marginals_from_joint(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    sigma_R = X.sum(axis=1)\n",
    "    sigma_S = X.sum(axis=0)\n",
    "    if sigma_R.sum() > 0:\n",
    "        sigma_R = sigma_R / sigma_R.sum()\n",
    "    if sigma_S.sum() > 0:\n",
    "        sigma_S = sigma_S / sigma_S.sum()\n",
    "    return sigma_R, sigma_S\n",
    "\n",
    "\n",
    "def entropy_of_joint(X: np.ndarray) -> float:\n",
    "    xx = np.clip(X.reshape(-1), 1e-12, 1.0)\n",
    "    return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Best responses (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_expected_risk_map_from_policy_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi: np.ndarray,\n",
    "    M_modes: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Expected p_true(cell) under mixture over sensor POLICIES (not modes).\n",
    "\n",
    "    For each sensor policy j, we use its mode at t=0 as its defining mode.\n",
    "    (This matches FixedModeSensorPolicy exactly.)\n",
    "    \"\"\"\n",
    "    pi = np.asarray(pi, dtype=float).reshape(-1)\n",
    "    if pi.size != len(sensors):\n",
    "        raise ValueError(\"mixture length mismatch\")\n",
    "\n",
    "    mode_probs = np.zeros(M_modes, dtype=float)\n",
    "    for j, sp in enumerate(sensors):\n",
    "        m = int(sp.select_mode(0))\n",
    "        if not (0 <= m < M_modes):\n",
    "            raise ValueError(\"invalid sensor mode\")\n",
    "        mode_probs[m] += float(pi[j])\n",
    "    if mode_probs.sum() > 0:\n",
    "        mode_probs = mode_probs / mode_probs.sum()\n",
    "\n",
    "    H, W = env.grid.height, env.grid.width\n",
    "    risk = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in env.grid.obstacles:\n",
    "                risk[y, x] = np.nan\n",
    "                continue\n",
    "            val = 0.0\n",
    "            for m in range(M_modes):\n",
    "                val += float(mode_probs[m]) * float(env.true_detection_prob((x, y), m))\n",
    "            risk[y, x] = float(val)\n",
    "\n",
    "    return risk\n",
    "\n",
    "\n",
    "def plan_risk_weighted_path(env: GridWorldStealthEnv, risk_map: np.ndarray, risk_weight: float) -> List[Tuple[int, int]]:\n",
    "    def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "        x, y = to\n",
    "        r = risk_map[y, x]\n",
    "        if not np.isfinite(r):\n",
    "            return 1e9\n",
    "        return 1.0 + float(risk_weight) * float(r)\n",
    "\n",
    "    return astar_path(env.grid, env.grid.start, env.grid.goal, step_cost)\n",
    "\n",
    "\n",
    "def robot_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi_S: np.ndarray,\n",
    "    robots: List[RobotPolicy],\n",
    "    M_modes: int,\n",
    "    risk_weight: float,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> RobotPolicy:\n",
    "    risk = compute_expected_risk_map_from_policy_mixture(env, sensors, pi_S, M_modes=M_modes)\n",
    "    try:\n",
    "        path = plan_risk_weighted_path(env, risk, risk_weight=risk_weight)\n",
    "    except Exception as e:\n",
    "        log.info(f\"[RobotBR] WARNING A* failed ({tag}): {e}\")\n",
    "        return robots[0]\n",
    "\n",
    "    path_tuple = tuple(path)\n",
    "    for p in robots:\n",
    "        if isinstance(p, FixedPathPolicy) and tuple(p.path) == path_tuple:\n",
    "            log.info(f\"[RobotBR] ({tag}) BR path already exists: {p.name}\")\n",
    "            return p\n",
    "\n",
    "    newp = FixedPathPolicy(path, name=f\"R_BR_{tag}_w{risk_weight:.1f}_len{len(path)}\")\n",
    "    log.info(f\"[RobotBR] ({tag}) Added new robot policy: {newp.name}\")\n",
    "    return newp\n",
    "\n",
    "\n",
    "def sensor_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    pi_R: np.ndarray,\n",
    "    candidate_modes: List[int],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> FixedModeSensorPolicy:\n",
    "    \"\"\"Sensor best response with common-random-numbers (CRN).\n",
    "\n",
    "    Why CRN matters: payoff variance is large (detection is a rare/threshold event).\n",
    "    If each candidate mode is evaluated on different random rollouts, you can pick\n",
    "    the wrong 'best mode' by noise, which then breaks the PSRO expansion logic.\n",
    "\n",
    "    Fix: reuse the same sampled robot indices AND the same episode seeds across all\n",
    "    candidate modes.\n",
    "    \"\"\"\n",
    "    pi_R = np.asarray(pi_R, dtype=float).reshape(-1)\n",
    "    if pi_R.size != len(robots):\n",
    "        raise ValueError(\"pi_R length mismatch\")\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    # Same robot-index samples for every mode\n",
    "    robot_idxs = rng.choice(len(robots), size=rollouts, p=pi_R, replace=True)\n",
    "\n",
    "    # Same episode seeds for every mode (common random numbers)\n",
    "    seeds = (int(base_seed) + np.arange(rollouts)).astype(int)\n",
    "\n",
    "    best_mode: Optional[int] = None\n",
    "    best_val = -1e18\n",
    "\n",
    "    for mode in candidate_modes:\n",
    "        if not (0 <= mode < M_modes):\n",
    "            continue\n",
    "        sp = FixedModeSensorPolicy(mode, name=f\"S_BR_{tag}_Mode{mode}\")\n",
    "\n",
    "        vals: List[float] = []\n",
    "        for k in range(rollouts):\n",
    "            rp = robots[int(robot_idxs[k])]\n",
    "            st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=int(seeds[k]))\n",
    "            vals.append(st.U_S)\n",
    "\n",
    "        mean_u = float(np.mean(vals))\n",
    "        if log.k >= 2:\n",
    "            log.debug(f\"[SensorBR] ({tag}) mode={mode} E[US]={mean_u:.3f} std={float(np.std(vals)):.2f}\")\n",
    "\n",
    "        if mean_u > best_val:\n",
    "            best_val = mean_u\n",
    "            best_mode = mode\n",
    "\n",
    "    if best_mode is None:\n",
    "        raise RuntimeError(\"No valid sensor BR mode found\")\n",
    "\n",
    "    log.info(f\"[SensorBR] ({tag}) Best mode={best_mode} E[US]={best_val:.3f}\")\n",
    "    return FixedModeSensorPolicy(best_mode, name=f\"S_BR_{tag}_Mode{best_mode}\")\n",
    "\n",
    "\n",
    "def conditional_sensor_given_robot(X: np.ndarray, i: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    row = np.asarray(X[i, :], dtype=float)\n",
    "    s = float(row.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(row, 1.0 / row.size)\n",
    "    return row / s\n",
    "\n",
    "\n",
    "def conditional_robot_given_sensor(X: np.ndarray, j: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    col = np.asarray(X[:, j], dtype=float)\n",
    "    s = float(col.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(col, 1.0 / col.size)\n",
    "    return col / s\n",
    "\n",
    "\n",
    "def compute_ce_regrets(U_R: np.ndarray, U_S: np.ndarray, X: np.ndarray, eps: float = 1e-12) -> Dict[str, float]:\n",
    "    \"\"\"Conditional recommendation regrets (CE-style) computed on current meta-game.\n",
    "\n",
    "    For robot (given recommendation i):\n",
    "        regret_R(i) = max_{i'} E_{j~q(.|i)}[U_R(i',j) - U_R(i,j)]\n",
    "\n",
    "    For sensor (given recommendation j):\n",
    "        regret_S(j) = max_{j'} E_{i~q(.|j)}[U_S(i,j') - U_S(i,j)]\n",
    "\n",
    "    Returns max and average regrets.\n",
    "    \"\"\"\n",
    "    m, n = U_R.shape\n",
    "    assert U_S.shape == (m, n)\n",
    "    assert X.shape == (m, n)\n",
    "\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "    reg_R = []\n",
    "    for i in range(m):\n",
    "        if sigma_R[i] <= eps:\n",
    "            continue\n",
    "        q = conditional_sensor_given_robot(X, i, eps=eps)\n",
    "        rec = float(np.dot(q, U_R[i, :]))\n",
    "        best = rec\n",
    "        for ip in range(m):\n",
    "            val = float(np.dot(q, U_R[ip, :]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_R.append(best - rec)\n",
    "\n",
    "    reg_S = []\n",
    "    for j in range(n):\n",
    "        if sigma_S[j] <= eps:\n",
    "            continue\n",
    "        q = conditional_robot_given_sensor(X, j, eps=eps)\n",
    "        rec = float(np.dot(q, U_S[:, j]))\n",
    "        best = rec\n",
    "        for jp in range(n):\n",
    "            val = float(np.dot(q, U_S[:, jp]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_S.append(best - rec)\n",
    "\n",
    "    return {\n",
    "        \"max_regret_R\": float(max(reg_R) if reg_R else 0.0),\n",
    "        \"max_regret_S\": float(max(reg_S) if reg_S else 0.0),\n",
    "        \"mean_regret_R\": float(np.mean(reg_R) if reg_R else 0.0),\n",
    "        \"mean_regret_S\": float(np.mean(reg_S) if reg_S else 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def find_sensor_by_mode(pols: List[SensorPolicy], mode: int) -> Optional[FixedModeSensorPolicy]:\n",
    "    for p in pols:\n",
    "        if isinstance(p, FixedModeSensorPolicy) and p.mode == mode:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policy initialization + evaluator for joint strategy\n",
    "# =============================================================================\n",
    "\n",
    "def build_initial_policies(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "    ]\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StrategyEval:\n",
    "    mean_U_R: float\n",
    "    mean_U_S: float\n",
    "    det_rate: float\n",
    "    goal_rate: float\n",
    "    mean_steps: float\n",
    "    mean_risk: float\n",
    "\n",
    "\n",
    "def evaluate_joint_strategy(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    X: np.ndarray,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    m, n = X.shape\n",
    "    probs = X.reshape(-1)\n",
    "    probs = probs / max(float(probs.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR = []\n",
    "    US = []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        idx = int(rng.choice(m * n, p=probs))\n",
    "        i, j = np.unravel_index(idx, (m, n))\n",
    "        seed = base_seed + k\n",
    "        st = rollout_episode(env, robots[i], sensors[j], M_modes=M_modes, seed=seed)\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training loop (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainHistoryRow:\n",
    "    outer_iter: int\n",
    "    m: int\n",
    "    n: int\n",
    "    nbs_obj: float\n",
    "    entropy_X: float\n",
    "    max_regret_R: float\n",
    "    max_regret_S: float\n",
    "    selfplay_UR: float\n",
    "    selfplay_US: float\n",
    "    selfplay_det: float\n",
    "    selfplay_goal: float\n",
    "    seconds: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    solver: str\n",
    "    env: GridWorldStealthEnv\n",
    "    robots: List[RobotPolicy]\n",
    "    sensors: List[SensorPolicy]\n",
    "    X: np.ndarray\n",
    "    history: List[TrainHistoryRow]\n",
    "\n",
    "\n",
    "def run_training(env: GridWorldStealthEnv, args: argparse.Namespace, solver: str, log: Logger) -> TrainResult:\n",
    "    t0_all = time.time()\n",
    "\n",
    "    grid = env.grid\n",
    "    sensor_cfg = env.sensor_cfg\n",
    "    M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "    robots, sensors = build_initial_policies(env, M_modes=M_modes)\n",
    "\n",
    "    # Optional: reduce initial set if you want smaller games.\n",
    "    # (We keep it as-is for benchmarks.)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(f\"[{solver}] Initial robots: \" + \", \".join([p.name for p in robots]))\n",
    "        log.info(f\"[{solver}] Initial sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "\n",
    "    debug_pair = None\n",
    "    if args.debug_rollout_pair:\n",
    "        parts = args.debug_rollout_pair.split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            debug_pair = (int(parts[0]), int(parts[1]))\n",
    "            log.info(f\"[{solver}] Will print one step-by-step rollout for pair {debug_pair} (only once).\")\n",
    "\n",
    "    history: List[TrainHistoryRow] = []\n",
    "    X = None\n",
    "\n",
    "    for it in range(1, args.outer_iters + 1):\n",
    "        t0 = time.time()\n",
    "        log.banner(f\"[{solver}] Outer iter {it}/{args.outer_iters}\")\n",
    "\n",
    "        U_R, U_S, _diag = evaluate_payoffs(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            M_modes=M_modes,\n",
    "            rollouts=args.rollouts_payoff,\n",
    "            base_seed=1000 + 100 * it,\n",
    "            log=log,\n",
    "            debug_rollout_pair=debug_pair,\n",
    "        )\n",
    "        debug_pair = None\n",
    "\n",
    "        # Solve NBS over joint actions\n",
    "        uR = U_R.reshape(-1)\n",
    "        uS = U_S.reshape(-1)\n",
    "\n",
    "        nbs = solve_nbs(\n",
    "            uR,\n",
    "            uS,\n",
    "            log=log,\n",
    "            disagreement=args.disagreement,\n",
    "            entropy_tau=args.entropy_tau,\n",
    "        )\n",
    "\n",
    "        m, n = U_R.shape\n",
    "        X = joint_to_matrix(nbs.x, m, n)\n",
    "        sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "        # Print top joint actions\n",
    "        top = np.argsort(-X.reshape(-1))[:min(5, X.size)]\n",
    "        log.info(f\"[{solver}] Top joint actions:\")\n",
    "        for k, idx in enumerate(top, start=1):\n",
    "            i, j = np.unravel_index(int(idx), (m, n))\n",
    "            log.info(f\"  #{k}: (R{i}:{robots[i].name}, S{j}:{sensors[j].name}) prob={X[i,j]:.4f}\")\n",
    "        log.info(f\"[{solver}] sigma_R={sigma_R.round(3)}\")\n",
    "        log.info(f\"[{solver}] sigma_S={sigma_S.round(3)}\")\n",
    "\n",
    "        # Stability diagnostics\n",
    "        regrets = compute_ce_regrets(U_R, U_S, X)\n",
    "        ent = entropy_of_joint(X)\n",
    "\n",
    "        # Self-play evaluation under joint X\n",
    "        sp = evaluate_joint_strategy(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            X,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.eval_episodes,\n",
    "            base_seed=9000 + 100 * it,\n",
    "        )\n",
    "\n",
    "        log.info(\n",
    "            f\"[{solver}] CE-regrets: maxR={regrets['max_regret_R']:.3f} maxS={regrets['max_regret_S']:.3f} | \"\n",
    "            f\"SelfPlay: UR={sp.mean_U_R:.2f} US={sp.mean_U_S:.2f} det%={100*sp.det_rate:.1f} goal%={100*sp.goal_rate:.1f} | \"\n",
    "            f\"H(X)={ent:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Best-response expansion\n",
    "        if solver == \"marginal\":\n",
    "            br_r = robot_best_response_to_mixture(\n",
    "                env,\n",
    "                sensors,\n",
    "                pi_S=sigma_S,\n",
    "                robots=robots,\n",
    "                M_modes=M_modes,\n",
    "                risk_weight=args.risk_weight_br,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "            br_s = sensor_best_response_to_mixture(\n",
    "                env,\n",
    "                robots,\n",
    "                pi_R=sigma_R,\n",
    "                candidate_modes=list(range(M_modes)),\n",
    "                M_modes=M_modes,\n",
    "                rollouts=args.rollouts_br,\n",
    "                base_seed=2000 + 100 * it,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "        elif solver == \"correlated\":\n",
    "            # FIX: choose conditional mixtures q(.|i) and q(.|j)\n",
    "            # We only check top-K recommendations to keep it tractable.\n",
    "            topK = max(1, int(args.cond_top_k))\n",
    "\n",
    "            # Robot: check the top-K robot recommendations by sigma_R\n",
    "            cand_i = [int(i) for i in np.argsort(-sigma_R) if sigma_R[int(i)] > 1e-8][:topK]\n",
    "            if not cand_i:\n",
    "                cand_i = [int(i) for i in np.argsort(-sigma_R)[:topK]]\n",
    "            best_gain = 0.0\n",
    "            br_r = robots[0]\n",
    "\n",
    "            for i in cand_i:\n",
    "                qS = conditional_sensor_given_robot(X, int(i))\n",
    "                tag = f\"Cond_i{i}\"\n",
    "                pol = robot_best_response_to_mixture(\n",
    "                    env,\n",
    "                    sensors,\n",
    "                    pi_S=qS,\n",
    "                    robots=robots,\n",
    "                    M_modes=M_modes,\n",
    "                    risk_weight=args.risk_weight_br,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement: E_q[U_R(pol,j)] - E_q[U_R(i,j)]\n",
    "                # If pol is already in set, its payoff exists in U_R row of that policy.\n",
    "                # Otherwise we simulate pol against each sensor policy j.\n",
    "                if pol in robots:\n",
    "                    ip = robots.index(pol)\n",
    "                    dev = float(np.dot(qS, U_R[ip, :]))\n",
    "                else:\n",
    "                    # simulate quickly vs each sensor policy\n",
    "                    dev_vals = []\n",
    "                    for j in range(n):\n",
    "                        vals = []\n",
    "                        for kk in range(args.br_eval_rollouts):\n",
    "                            seed = 777000 + 1000 * it + 100 * i + 10 * j + kk\n",
    "                            st = rollout_episode(env, pol, sensors[j], M_modes=M_modes, seed=seed)\n",
    "                            vals.append(st.U_R)\n",
    "                        dev_vals.append(float(np.mean(vals)))\n",
    "                    dev = float(np.dot(qS, np.asarray(dev_vals)))\n",
    "\n",
    "                rec = float(np.dot(qS, U_R[i, :]))\n",
    "                gain = dev - rec\n",
    "                if gain > best_gain + 1e-9:\n",
    "                    best_gain = gain\n",
    "                    br_r = pol\n",
    "\n",
    "            if best_gain > args.add_threshold:\n",
    "                if br_r in robots:\n",
    "                    log.info(f\"[{solver}] Best robot deviation already in set; est_gain={best_gain:.3f}\")\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding robot deviation; est_gain={best_gain:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No robot deviation above threshold (best_gain={best_gain:.3f}).\")\n",
    "\n",
    "            # Sensor: check top-K sensor recommendations by sigma_S\n",
    "            cand_j = [int(j) for j in np.argsort(-sigma_S) if sigma_S[int(j)] > 1e-8][:topK]\n",
    "            if not cand_j:\n",
    "                cand_j = [int(j) for j in np.argsort(-sigma_S)[:topK]]\n",
    "            best_gain_s = 0.0\n",
    "            br_s = FixedModeSensorPolicy(0, name=\"S_dummy\")\n",
    "\n",
    "            for j in cand_j:\n",
    "                qR = conditional_robot_given_sensor(X, int(j))\n",
    "                tag = f\"Cond_j{j}\"\n",
    "                polS = sensor_best_response_to_mixture(\n",
    "                    env,\n",
    "                    robots,\n",
    "                    pi_R=qR,\n",
    "                    candidate_modes=list(range(M_modes)),\n",
    "                    M_modes=M_modes,\n",
    "                    rollouts=args.rollouts_br,\n",
    "                    base_seed=333000 + 1000 * it + 10 * j,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement for sensor\n",
    "                # rec under recommendation j is E_q[U_S(i,j)]\n",
    "                recS = float(np.dot(qR, U_S[:, j]))\n",
    "\n",
    "                # dev under mode polS.mode: if already present, use its column.\n",
    "                existing_col = None\n",
    "                for jj, spj in enumerate(sensors):\n",
    "                    if isinstance(spj, FixedModeSensorPolicy) and spj.mode == polS.mode:\n",
    "                        existing_col = jj\n",
    "                        break\n",
    "\n",
    "                if existing_col is not None:\n",
    "                    devS = float(np.dot(qR, U_S[:, existing_col]))\n",
    "                else:\n",
    "                    vals = []\n",
    "                    for kk in range(args.br_eval_rollouts):\n",
    "                        i_samp = int(np.random.default_rng(444 + kk).choice(len(robots), p=qR))\n",
    "                        seed = 888000 + 1000 * it + 10 * j + kk\n",
    "                        st = rollout_episode(env, robots[i_samp], polS, M_modes=M_modes, seed=seed)\n",
    "                        vals.append(st.U_S)\n",
    "                    devS = float(np.mean(vals))\n",
    "\n",
    "                gainS = devS - recS\n",
    "                if gainS > best_gain_s + 1e-9:\n",
    "                    best_gain_s = gainS\n",
    "                    br_s = polS\n",
    "\n",
    "            if best_gain_s > args.add_threshold:\n",
    "                if (isinstance(br_s, FixedModeSensorPolicy)) and (find_sensor_by_mode(sensors, br_s.mode) is not None):\n",
    "                    log.info(f\"[{solver}] Best sensor deviation already in set (mode={br_s.mode}); est_gain={best_gain_s:.3f}\")\n",
    "                    br_s = None\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding sensor deviation; est_gain={best_gain_s:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No sensor deviation above threshold (best_gain={best_gain_s:.3f}).\")\n",
    "                br_s = None\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"solver must be marginal or correlated\")\n",
    "\n",
    "        # Add to sets (dedupe)\n",
    "        if br_r not in robots:\n",
    "            robots.append(br_r)\n",
    "\n",
    "        if isinstance(br_s, FixedModeSensorPolicy):\n",
    "            if find_sensor_by_mode(sensors, br_s.mode) is None:\n",
    "                sensors.append(br_s)\n",
    "            else:\n",
    "                log.info(f\"[{solver}] Sensor mode {br_s.mode} already present; not adding duplicate.\")\n",
    "\n",
    "        seconds = float(time.time() - t0)\n",
    "        history.append(\n",
    "            TrainHistoryRow(\n",
    "                outer_iter=it,\n",
    "                m=len(robots),\n",
    "                n=len(sensors),\n",
    "                nbs_obj=float(nbs.obj),\n",
    "                entropy_X=float(ent),\n",
    "                max_regret_R=float(regrets[\"max_regret_R\"]),\n",
    "                max_regret_S=float(regrets[\"max_regret_S\"]),\n",
    "                selfplay_UR=float(sp.mean_U_R),\n",
    "                selfplay_US=float(sp.mean_U_S),\n",
    "                selfplay_det=float(sp.det_rate),\n",
    "                selfplay_goal=float(sp.goal_rate),\n",
    "                seconds=seconds,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        log.info(f\"[{solver}] Sets: |Pi_R|={len(robots)} |Pi_S|={len(sensors)} | iter_seconds={seconds:.2f}\")\n",
    "\n",
    "    if X is None:\n",
    "        raise RuntimeError(\"Training produced no X\")\n",
    "\n",
    "    log.banner(f\"[{solver}] Finished\")\n",
    "    log.info(f\"[{solver}] Final robots: \" + \", \".join([p.name for p in robots]))\n",
    "    log.info(f\"[{solver}] Final sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "    log.info(f\"[{solver}] Total time: {time.time()-t0_all:.2f}s\")\n",
    "\n",
    "    return TrainResult(solver=solver, env=env, robots=robots, sensors=sensors, X=X, history=history)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Benchmarks + plotting\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BenchCell:\n",
    "    UR: float\n",
    "    US: float\n",
    "    det: float\n",
    "    goal: float\n",
    "\n",
    "\n",
    "def run_policy_matrix_benchmark(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_methods: List[RobotPolicy],\n",
    "    sensor_methods: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[Tuple[int, int], BenchCell]:\n",
    "    res: Dict[Tuple[int, int], BenchCell] = {}\n",
    "    for i, rp in enumerate(robot_methods):\n",
    "        for j, sp in enumerate(sensor_methods):\n",
    "            UR = []\n",
    "            US = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            for k in range(episodes):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed)\n",
    "                UR.append(st.U_R)\n",
    "                US.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "            res[(i, j)] = BenchCell(\n",
    "                UR=float(np.mean(UR)),\n",
    "                US=float(np.mean(US)),\n",
    "                det=float(det / episodes),\n",
    "                goal=float(goal / episodes),\n",
    "            )\n",
    "    return res\n",
    "\n",
    "\n",
    "def safe_makedirs(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_history_csv(hist: List[TrainHistoryRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    fields = list(TrainHistoryRow.__annotations__.keys())\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fields)\n",
    "        w.writeheader()\n",
    "        for row in hist:\n",
    "            w.writerow({k: getattr(row, k) for k in fields})\n",
    "\n",
    "\n",
    "def plot_training_curves(results: List[TrainResult], outdir: str) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # 1) NBS objective\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.nbs_obj for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"NBS objective\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_nbs_obj.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Max conditional regrets\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_R for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: maxRegret_R\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_S for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: maxRegret_S\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Max conditional regret\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_max_regrets.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 3) Entropy of X\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.entropy_X for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Entropy H(X)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_entropy_X.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 4) Self-play outcomes\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_UR for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: UR\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_US for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: US\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Expected utility under X\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_selfplay_utils.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_benchmark_bars(\n",
    "    robot_names: List[str],\n",
    "    sensor_names: List[str],\n",
    "    bench: Dict[Tuple[int, int], BenchCell],\n",
    "    outdir: str,\n",
    ") -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # For each sensor, bar chart of robot UR\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].UR for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Robot utility\")\n",
    "        plt.title(f\"Robot utility vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_UR_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # For each sensor, bar chart of robot goal rate\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].goal for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Goal rate\")\n",
    "        plt.title(f\"Robot goal rate vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_goal_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main pipeline wrapper\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(args: argparse.Namespace) -> None:\n",
    "    log = Logger(args.log_level)\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.banner(\"[PIPELINE] Stealth grid game\")\n",
    "        log.info(f\"Grid: {grid.width}x{grid.height} start={grid.start} goal={grid.goal} obstacles={len(grid.obstacles)}\")\n",
    "        log.info(f\"Sensors: {sensor_cfg.sensors} radius={sensor_cfg.radius} base_p={sensor_cfg.base_p} hotspot_p={sensor_cfg.hotspot_p}\")\n",
    "        if log.k >= 2:\n",
    "            log.debug(\"ASCII map:\")\n",
    "            print_grid_ascii(grid, sensor_cfg)\n",
    "\n",
    "    solvers: List[str]\n",
    "    if args.solver == \"both\":\n",
    "        solvers = [\"marginal\", \"correlated\"]\n",
    "    else:\n",
    "        solvers = [args.solver]\n",
    "\n",
    "    results: List[TrainResult] = []\n",
    "    for s in solvers:\n",
    "        # Use a fresh env copy per solver (to avoid RNG coupling)\n",
    "        env_s = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "        results.append(run_training(env_s, args, solver=s, log=log))\n",
    "\n",
    "    if args.results_dir:\n",
    "        safe_makedirs(args.results_dir)\n",
    "        for r in results:\n",
    "            save_history_csv(r.history, os.path.join(args.results_dir, f\"history_{r.solver}.csv\"))\n",
    "\n",
    "    if args.save_plots and args.results_dir:\n",
    "        plot_training_curves(results, outdir=args.results_dir)\n",
    "\n",
    "    # Benchmarks on the same game\n",
    "    if args.run_benchmarks:\n",
    "        log.banner(\"[BENCH] Heuristics on same game\")\n",
    "        M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "        # Robot heuristics\n",
    "        #  - fixed paths from initial set\n",
    "        robots_init, sensors_init = build_initial_policies_for_bench(env, M_modes)\n",
    "\n",
    "        # Sensor heuristics\n",
    "        sensor_methods: List[SensorPolicy] = [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=args.seed + 123, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "        bench = run_policy_matrix_benchmark(\n",
    "            env,\n",
    "            robot_methods=robots_init,\n",
    "            sensor_methods=sensor_methods,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.bench_episodes,\n",
    "            base_seed=555000,\n",
    "        )\n",
    "\n",
    "        # Print a compact table\n",
    "        robot_names = [p.name for p in robots_init]\n",
    "        sensor_names = [p.name for p in sensor_methods]\n",
    "        for j, sname in enumerate(sensor_names):\n",
    "            log.info(f\"[BENCH] Sensor={sname}\")\n",
    "            for i, rname in enumerate(robot_names):\n",
    "                cell = bench[(i, j)]\n",
    "                log.info(f\"  Robot={rname:22s} UR={cell.UR:8.2f} goal%={100*cell.goal:5.1f} det%={100*cell.det:5.1f}\")\n",
    "\n",
    "        if args.save_plots and args.results_dir:\n",
    "            plot_benchmark_bars(robot_names, sensor_names, bench, outdir=args.results_dir)\n",
    "\n",
    "\n",
    "# Helper: initial robot set for benchmark (without the solver-added BR policies)\n",
    "\n",
    "def build_initial_policies_for_bench(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    # Static risk-aware A* under UNIFORM mode belief\n",
    "    uniform_mode = np.full(M_modes, 1.0 / M_modes)\n",
    "\n",
    "    # Construct risk map directly under uniform belief\n",
    "    H, W = grid.height, grid.width\n",
    "    risk_uniform = np.zeros((H, W), dtype=float)\n",
    "    risk_worst = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in grid.obstacles:\n",
    "                risk_uniform[y, x] = np.nan\n",
    "                risk_worst[y, x] = np.nan\n",
    "                continue\n",
    "            vals = [env.true_detection_prob((x, y), m) for m in range(M_modes)]\n",
    "            risk_uniform[y, x] = float(np.dot(uniform_mode, vals))\n",
    "            risk_worst[y, x] = float(np.max(vals))\n",
    "\n",
    "    p_uniform = plan_risk_weighted_path(env, risk_uniform, risk_weight=12.0)\n",
    "    p_worst = plan_risk_weighted_path(env, risk_worst, risk_weight=12.0)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        FixedPathPolicy(p_uniform, \"R_RiskAStar_Uniform\"),\n",
    "        FixedPathPolicy(p_worst, \"R_RiskAStar_WorstCase\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "    ]\n",
    "\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLI\n",
    "# =============================================================================\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> Tuple[argparse.Namespace, List[str]]:\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "    p.add_argument(\"--log-level\", type=str, default=\"INFO\", choices=[\"QUIET\", \"INFO\", \"DEBUG\"])\n",
    "\n",
    "    p.add_argument(\"--solver\", type=str, default=\"correlated\", choices=[\"marginal\", \"correlated\", \"both\"])\n",
    "\n",
    "    p.add_argument(\"--outer-iters\", type=int, default=3)\n",
    "\n",
    "    p.add_argument(\"--rollouts-payoff\", type=int, default=20)\n",
    "    p.add_argument(\"--rollouts-br\", type=int, default=30)\n",
    "    p.add_argument(\"--risk-weight-br\", type=float, default=12.0)\n",
    "\n",
    "    # NBS knobs\n",
    "    p.add_argument(\"--disagreement\", type=str, default=\"minminus\", choices=[\"minminus\", \"uniform\"])\n",
    "    p.add_argument(\"--entropy-tau\", type=float, default=0.0)\n",
    "\n",
    "    # Fix mismatch knobs\n",
    "    p.add_argument(\"--cond-top-k\", type=int, default=2, help=\"How many top recommendations to check for conditional BRs\")\n",
    "    p.add_argument(\"--br-eval-rollouts\", type=int, default=8, help=\"Small evaluation rollouts for new deviations\")\n",
    "    p.add_argument(\"--add-threshold\", type=float, default=0.25, help=\"Minimum estimated conditional gain to add a deviation policy\")\n",
    "\n",
    "    # Eval episodes under joint X (self-play)\n",
    "    p.add_argument(\"--eval-episodes\", type=int, default=60)\n",
    "\n",
    "    # Debug\n",
    "    p.add_argument(\"--debug-rollout-pair\", type=str, default=\"\", help=\"Print one step-by-step rollout for i,j\")\n",
    "\n",
    "    # Outputs\n",
    "    p.add_argument(\"--results-dir\", type=str, default=\"results\", help=\"Directory for csv/plots\")\n",
    "    p.add_argument(\"--save-plots\", action=\"store_true\")\n",
    "\n",
    "    # Benchmarks\n",
    "    p.add_argument(\"--run-benchmarks\", action=\"store_true\")\n",
    "    p.add_argument(\"--bench-episodes\", type=int, default=80)\n",
    "\n",
    "    args, unknown = p.parse_known_args(args=argv)\n",
    "    return args, unknown\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    args, unknown = parse_args(argv=argv)\n",
    "    if unknown and (\"ipykernel\" not in sys.modules):\n",
    "        print(f\"[WARN] Ignoring unknown CLI args: {unknown}\")\n",
    "    if args.debug_rollout_pair.strip() == \"\":\n",
    "        args.debug_rollout_pair = \"\"\n",
    "    run_pipeline(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and (\"ipykernel\" not in sys.modules):\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a28352d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"approach2_robust_correlated.py\n",
    "\n",
    "Robust, reduced-log implementation of \"Approach 2\" on a stealth gridworld POMDP,\n",
    "with a FIX for the conceptual mismatch:\n",
    "\n",
    "  Old mismatch: solve a joint distribution x*(i,j) over (robot policy i, sensor policy j)\n",
    "  but then compute best responses against the MARGINALS sigma_R, sigma_S.\n",
    "\n",
    "  Fix here: treat x* as a CORRELATION DEVICE (mediator) and compute BRs against\n",
    "  CONDITIONAL distributions:\n",
    "\n",
    "      q_S(.|i) = X[i,:] / sigma_R[i]   (sensor conditional given robot recommendation i)\n",
    "      q_R(.|j) = X[:,j] / sigma_S[j]   (robot conditional given sensor recommendation j)\n",
    "\n",
    "  Then add the most profitable *deviation* policy (oracle) based on the most violated\n",
    "  conditional recommendation.\n",
    "\n",
    "This makes the PSRO-style expansion step consistent with a correlated-strategy viewpoint.\n",
    "\n",
    "Also included:\n",
    "  - Benchmark harness vs simple heuristics on the same game.\n",
    "  - Saved plots (training curves + bar charts) for presentations.\n",
    "\n",
    "Run (script):\n",
    "  python approach2_robust_correlated.py --solver correlated --outer-iters 3 --save-plots\n",
    "\n",
    "Run (notebook):\n",
    "  main(argv=[\"--solver\",\"both\",\"--outer-iters\",\"3\",\"--save-plots\"])  # ignore ipykernel args\n",
    "\n",
    "Tips to encourage MULTIMODAL x* (useful for your \"mode recovery\"):\n",
    "  --disagreement uniform --entropy-tau 0.02\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import heapq\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Stage-based logger with QUIET/INFO/DEBUG.\"\"\"\n",
    "\n",
    "    LEVELS = {\"QUIET\": 0, \"INFO\": 1, \"DEBUG\": 2}\n",
    "\n",
    "    def __init__(self, level: str = \"INFO\"):\n",
    "        level = level.upper()\n",
    "        if level not in self.LEVELS:\n",
    "            raise ValueError(f\"Unknown log level: {level}. Use QUIET/INFO/DEBUG\")\n",
    "        self.level = level\n",
    "        self.k = self.LEVELS[level]\n",
    "\n",
    "    def banner(self, title: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(\"\" + \"=\" * 100)\n",
    "            print(title)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "    def info(self, msg: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(msg)\n",
    "\n",
    "    def debug(self, msg: str) -> None:\n",
    "        if self.k >= 2:\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Types\n",
    "# =============================================================================\n",
    "\n",
    "Action = Tuple[int, int]  # (dx, dy)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Environment (grid + hidden sensor mode + noisy alarm observation)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GridConfig:\n",
    "    width: int\n",
    "    height: int\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    obstacles: frozenset\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SensorConfig:\n",
    "    sensors: Tuple[Tuple[int, int], ...]\n",
    "    radius: int\n",
    "    base_p: float\n",
    "    hotspot_p: float\n",
    "\n",
    "\n",
    "class GridWorldStealthEnv:\n",
    "    \"\"\"Grid world with detection risk controlled by a hidden/selected sensor 'mode'.\"\"\"\n",
    "\n",
    "    def __init__(self, grid: GridConfig, sensor_cfg: SensorConfig, fp: float = 0.05, fn: float = 0.10, seed: int = 0):\n",
    "        self.grid = grid\n",
    "        self.sensor_cfg = sensor_cfg\n",
    "        self.fp = float(fp)\n",
    "        self.fn = float(fn)\n",
    "\n",
    "        if not (0.0 <= self.fp <= 1.0 and 0.0 <= self.fn <= 1.0):\n",
    "            raise ValueError(\"fp and fn must be in [0,1].\")\n",
    "        if not (0.0 <= sensor_cfg.base_p <= 1.0 and 0.0 <= sensor_cfg.hotspot_p <= 1.0):\n",
    "            raise ValueError(\"base_p and hotspot_p must be in [0,1].\")\n",
    "        if sensor_cfg.radius < 0:\n",
    "            raise ValueError(\"radius must be >= 0\")\n",
    "        if len(sensor_cfg.sensors) == 0:\n",
    "            raise ValueError(\"Need at least one sensor center.\")\n",
    "\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.reset(sensor_mode=0)\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    def reset(self, sensor_mode: int = 0) -> Dict[str, Any]:\n",
    "        self.t = 0\n",
    "        self.pos = self.grid.start\n",
    "        self.sensor_mode = int(sensor_mode)\n",
    "        self.detected = False\n",
    "        self.total_true_risk = 0.0\n",
    "        return {\"pos\": self.pos, \"t\": self.t}\n",
    "\n",
    "    def in_bounds(self, p: Tuple[int, int]) -> bool:\n",
    "        x, y = p\n",
    "        return 0 <= x < self.grid.width and 0 <= y < self.grid.height\n",
    "\n",
    "    def is_free(self, p: Tuple[int, int]) -> bool:\n",
    "        return self.in_bounds(p) and (p not in self.grid.obstacles)\n",
    "\n",
    "    def true_detection_prob(self, p: Tuple[int, int], mode: int) -> float:\n",
    "        if not (0 <= mode < len(self.sensor_cfg.sensors)):\n",
    "            raise ValueError(f\"mode {mode} out of range\")\n",
    "        sx, sy = self.sensor_cfg.sensors[mode]\n",
    "        x, y = p\n",
    "        d = abs(x - sx) + abs(y - sy)\n",
    "        return self.sensor_cfg.hotspot_p if d <= self.sensor_cfg.radius else self.sensor_cfg.base_p\n",
    "\n",
    "    def observation_prob(self, alarm: int, p_true: float) -> float:\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        if alarm == 1:\n",
    "            return p_alarm\n",
    "        if alarm == 0:\n",
    "            return 1.0 - p_alarm\n",
    "        raise ValueError(\"alarm must be 0 or 1\")\n",
    "\n",
    "    def step(self, a: Action) -> Dict[str, Any]:\n",
    "        if self.detected:\n",
    "            return {\"pos\": self.pos, \"t\": self.t, \"alarm\": 1, \"p_true\": 1.0, \"detected\": True, \"done\": True}\n",
    "\n",
    "        self.t += 1\n",
    "        nx = self.pos[0] + int(a[0])\n",
    "        ny = self.pos[1] + int(a[1])\n",
    "        np_ = (nx, ny)\n",
    "        if self.is_free(np_):\n",
    "            self.pos = np_\n",
    "\n",
    "        p_true = float(self.true_detection_prob(self.pos, self.sensor_mode))\n",
    "        self.total_true_risk += p_true\n",
    "\n",
    "        if self.rng.random() < p_true:\n",
    "            self.detected = True\n",
    "\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        alarm = 1 if (self.rng.random() < p_alarm) else 0\n",
    "\n",
    "        done = self.detected or (self.pos == self.grid.goal) or (self.t >= 200)\n",
    "        return {\"pos\": self.pos, \"t\": self.t, \"alarm\": alarm, \"p_true\": p_true, \"detected\": self.detected, \"done\": done}\n",
    "\n",
    "\n",
    "def build_two_corridor_grid(width: int = 15, height: int = 9) -> GridConfig:\n",
    "    obstacles = set()\n",
    "    wall_x = width // 2\n",
    "    gap_ys = {2, 6}\n",
    "    for y in range(height):\n",
    "        if y not in gap_ys:\n",
    "            obstacles.add((wall_x, y))\n",
    "\n",
    "    start = (1, height - 2)\n",
    "    goal = (width - 2, 1)\n",
    "    if start in obstacles or goal in obstacles:\n",
    "        raise RuntimeError(\"Start/goal blocked unexpectedly\")\n",
    "\n",
    "    return GridConfig(width=width, height=height, start=start, goal=goal, obstacles=frozenset(obstacles))\n",
    "\n",
    "\n",
    "def print_grid_ascii(grid: GridConfig, sensor_cfg: SensorConfig) -> None:\n",
    "    W, H = grid.width, grid.height\n",
    "    obs = set(grid.obstacles)\n",
    "    sens = set(sensor_cfg.sensors)\n",
    "    for y in range(H):\n",
    "        row = []\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p == grid.start:\n",
    "                row.append(\"R\")\n",
    "            elif p == grid.goal:\n",
    "                row.append(\"G\")\n",
    "            elif p in sens:\n",
    "                row.append(\"S\")\n",
    "            elif p in obs:\n",
    "                row.append(\"#\")\n",
    "            else:\n",
    "                row.append(\".\")\n",
    "        print(\"\".join(row))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Belief over modes\n",
    "# =============================================================================\n",
    "\n",
    "class ModeBelief:\n",
    "    \"\"\"Exact belief over discrete modes m in {0..M-1}.\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, init: Optional[np.ndarray] = None):\n",
    "        self.M = int(M)\n",
    "        if self.M <= 0:\n",
    "            raise ValueError(\"M must be >= 1\")\n",
    "        if init is None:\n",
    "            self.b = np.full(self.M, 1.0 / self.M)\n",
    "        else:\n",
    "            init = np.asarray(init, dtype=float).reshape(-1)\n",
    "            if init.shape != (self.M,):\n",
    "                raise ValueError(\"init shape mismatch\")\n",
    "            if np.any(init < 0):\n",
    "                raise ValueError(\"init must be nonnegative\")\n",
    "            s = float(init.sum())\n",
    "            self.b = init / s if s > 0 else np.full(self.M, 1.0 / self.M)\n",
    "\n",
    "    def update(self, env: GridWorldStealthEnv, alarm: int, pos: Tuple[int, int], eps: float = 1e-12) -> None:\n",
    "        like = np.zeros(self.M, dtype=float)\n",
    "        for m in range(self.M):\n",
    "            p_true = env.true_detection_prob(pos, m)\n",
    "            like[m] = env.observation_prob(alarm, p_true)\n",
    "        post = self.b * like\n",
    "        Z = float(post.sum())\n",
    "        if (not np.isfinite(Z)) or Z < eps:\n",
    "            return\n",
    "        self.b = post / Z\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policies\n",
    "# =============================================================================\n",
    "\n",
    "class RobotPolicy:\n",
    "    name: str = \"RobotPolicy\"\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedPathPolicy(RobotPolicy):\n",
    "    def __init__(self, path: List[Tuple[int, int]], name: str):\n",
    "        if len(path) < 2:\n",
    "            raise ValueError(\"Path must have >=2 states\")\n",
    "        self.path = list(path)\n",
    "        self.name = str(name)\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        try:\n",
    "            self._idx = self.path.index(start_pos)\n",
    "        except ValueError:\n",
    "            self._idx = 0\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        cur = env.pos\n",
    "        if self._idx >= len(self.path) - 1:\n",
    "            return (0, 0)\n",
    "        if cur != self.path[self._idx]:\n",
    "            try:\n",
    "                self._idx = self.path.index(cur, self._idx)\n",
    "            except ValueError:\n",
    "                return (0, 0)\n",
    "        nxt = self.path[self._idx + 1]\n",
    "        dx = int(np.clip(nxt[0] - cur[0], -1, 1))\n",
    "        dy = int(np.clip(nxt[1] - cur[1], -1, 1))\n",
    "        self._idx += 1\n",
    "        return (dx, dy)\n",
    "\n",
    "\n",
    "class RandomPolicy(RobotPolicy):\n",
    "    \"\"\"Reproducible random policy using env.rng.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"R_Random\"):\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        candidates: List[Action] = []\n",
    "        x, y = env.pos\n",
    "        for a in [(1, 0), (-1, 0), (0, 1), (0, -1), (0, 0)]:\n",
    "            np_ = (x + a[0], y + a[1])\n",
    "            if env.is_free(np_):\n",
    "                candidates.append(a)\n",
    "        if not candidates:\n",
    "            return (0, 0)\n",
    "        return candidates[int(env.rng.integers(0, len(candidates)))]\n",
    "\n",
    "\n",
    "class OnlineBeliefReplanPolicy(RobotPolicy):\n",
    "    \"\"\"POMDP-ish heuristic: replan each step using risk map induced by current belief b_t.\"\"\"\n",
    "\n",
    "    def __init__(self, env: GridWorldStealthEnv, risk_weight: float = 12.0, name: str = \"R_OnlineBeliefReplan\"):\n",
    "        self.env = env\n",
    "        self.risk_weight = float(risk_weight)\n",
    "        self.name = name\n",
    "        self._cached_next: Optional[Tuple[int, int]] = None\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        self._cached_next = None\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        # Build a risk map from belief over modes.\n",
    "        mode_probs = belief.b\n",
    "\n",
    "        def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "            x, y = to\n",
    "            # expected risk at to under belief\n",
    "            r = 0.0\n",
    "            for m, pm in enumerate(mode_probs):\n",
    "                r += float(pm) * float(env.true_detection_prob(to, m))\n",
    "            return 1.0 + self.risk_weight * r\n",
    "\n",
    "        # Plan from current pos to goal (one-step receding horizon)\n",
    "        try:\n",
    "            path = astar_path(env.grid, env.pos, env.grid.goal, step_cost)\n",
    "            if len(path) < 2:\n",
    "                return (0, 0)\n",
    "            nxt = path[1]\n",
    "            dx = int(np.clip(nxt[0] - env.pos[0], -1, 1))\n",
    "            dy = int(np.clip(nxt[1] - env.pos[1], -1, 1))\n",
    "            return (dx, dy)\n",
    "        except Exception:\n",
    "            return (0, 0)\n",
    "\n",
    "\n",
    "class SensorPolicy:\n",
    "    name: str = \"SensorPolicy\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedModeSensorPolicy(SensorPolicy):\n",
    "    def __init__(self, mode: int, name: Optional[str] = None):\n",
    "        self.mode = int(mode)\n",
    "        self.name = name or f\"S_Mode{mode}\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return self.mode\n",
    "\n",
    "\n",
    "class AlternatingSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Simple sensor heuristic for benchmarks: alternate modes 0,1,0,1,...\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, name: str = \"S_Alternate\"):\n",
    "        self.M = int(M)\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(t % self.M)\n",
    "\n",
    "\n",
    "class RandomModeSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Benchmark sensor: random mode each step (uses numpy Generator for reproducibility).\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, seed: int = 0, name: str = \"S_RandomMode\"):\n",
    "        self.M = int(M)\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(self.rng.integers(0, self.M))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# A* (used for planning)\n",
    "# =============================================================================\n",
    "\n",
    "def astar_path(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    max_expansions: int = 250_000,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    if start == goal:\n",
    "        return [start]\n",
    "\n",
    "    def h(p: Tuple[int, int]) -> float:\n",
    "        return abs(p[0] - goal[0]) + abs(p[1] - goal[1])\n",
    "\n",
    "    def neighbors(p: Tuple[int, int]):\n",
    "        x, y = p\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            np_ = (x + dx, y + dy)\n",
    "            if 0 <= np_[0] < grid.width and 0 <= np_[1] < grid.height and np_ not in grid.obstacles:\n",
    "                yield np_\n",
    "\n",
    "    open_heap: List[Tuple[float, float, Tuple[int, int]]] = []\n",
    "    heapq.heappush(open_heap, (h(start), 0.0, start))\n",
    "\n",
    "    came: Dict[Tuple[int, int], Optional[Tuple[int, int]]] = {start: None}\n",
    "    gscore: Dict[Tuple[int, int], float] = {start: 0.0}\n",
    "\n",
    "    expansions = 0\n",
    "    while open_heap:\n",
    "        _, _, cur = heapq.heappop(open_heap)\n",
    "        expansions += 1\n",
    "        if cur == goal:\n",
    "            path: List[Tuple[int, int]] = []\n",
    "            while cur is not None:\n",
    "                path.append(cur)\n",
    "                cur = came[cur]\n",
    "            path.reverse()\n",
    "            return path\n",
    "        if expansions > max_expansions:\n",
    "            raise RuntimeError(\"A* exceeded max expansions\")\n",
    "\n",
    "        for nb in neighbors(cur):\n",
    "            tentative = gscore[cur] + float(step_cost(cur, nb))\n",
    "            if (nb not in gscore) or (tentative < gscore[nb] - 1e-12):\n",
    "                gscore[nb] = tentative\n",
    "                came[nb] = cur\n",
    "                heapq.heappush(open_heap, (tentative + h(nb), tentative, nb))\n",
    "\n",
    "    raise RuntimeError(\"A* failed: unreachable goal\")\n",
    "\n",
    "\n",
    "def astar_via_waypoint(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    waypoint: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    ") -> List[Tuple[int, int]]:\n",
    "    p1 = astar_path(grid, start, waypoint, step_cost)\n",
    "    p2 = astar_path(grid, waypoint, goal, step_cost)\n",
    "    return p1[:-1] + p2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rollouts + payoffs\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStats:\n",
    "    steps: int\n",
    "    reached_goal: bool\n",
    "    detected: bool\n",
    "    total_true_risk: float\n",
    "    U_R: float\n",
    "    U_S: float\n",
    "\n",
    "\n",
    "def rollout_episode(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    seed: int,\n",
    "    max_steps: int = 200,\n",
    "    lambda_risk: float = 1.0,\n",
    "    det_penalty: float = 50.0,\n",
    "    sensor_energy_per_step: float = 0.2,\n",
    "    step_debug: bool = False,\n",
    ") -> EpisodeStats:\n",
    "    env.seed(seed)\n",
    "    sensor.reset()\n",
    "    env.reset(sensor_mode=sensor.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)\n",
    "    robot.reset(env.pos)\n",
    "\n",
    "    total_risk = 0.0\n",
    "    last_alarm: Optional[int] = None\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.sensor_mode = sensor.select_mode(env.t)\n",
    "        a = robot.act(env, belief, last_alarm)\n",
    "        out = env.step(a)\n",
    "\n",
    "        total_risk += float(out[\"p_true\"])\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "        last_alarm = int(out[\"alarm\"])\n",
    "\n",
    "        if step_debug:\n",
    "            print(\n",
    "                f\"[Step] t={out['t']:3d} pos={out['pos']} a={a} p_true={out['p_true']:.3f} \"\n",
    "                f\"alarm={out['alarm']} det={out['detected']} done={out['done']} b={belief.b.round(3)}\"\n",
    "            )\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = bool(env.detected)\n",
    "    steps = int(env.t)\n",
    "\n",
    "    cost_R = steps + lambda_risk * total_risk + (det_penalty if detected else 0.0)\n",
    "    U_R = -float(cost_R)\n",
    "\n",
    "    U_S = float((det_penalty if detected else 0.0) + lambda_risk * total_risk - sensor_energy_per_step * steps)\n",
    "\n",
    "    return EpisodeStats(steps=steps, reached_goal=reached_goal, detected=detected, total_true_risk=float(total_risk), U_R=U_R, U_S=U_S)\n",
    "\n",
    "\n",
    "def evaluate_payoffs(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    debug_rollout_pair: Optional[Tuple[int, int]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[int, int], Dict[str, float]]]:\n",
    "    m, n = len(robots), len(sensors)\n",
    "    U_R = np.zeros((m, n), dtype=float)\n",
    "    U_S = np.zeros((m, n), dtype=float)\n",
    "    diag: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    log.info(f\"[Eval] Estimating payoffs: m={m}, n={n}, rollouts={rollouts}, base_seed={base_seed}\")\n",
    "\n",
    "    for i, rpol in enumerate(robots):\n",
    "        for j, spol in enumerate(sensors):\n",
    "            step_debug = (debug_rollout_pair == (i, j))\n",
    "\n",
    "            r_list: List[float] = []\n",
    "            s_list: List[float] = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            steps_list: List[int] = []\n",
    "            risk_list: List[float] = []\n",
    "\n",
    "            for k in range(rollouts):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rpol, spol, M_modes=M_modes, seed=seed, step_debug=step_debug)\n",
    "                r_list.append(st.U_R)\n",
    "                s_list.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "                steps_list.append(st.steps)\n",
    "                risk_list.append(st.total_true_risk)\n",
    "\n",
    "                if step_debug:\n",
    "                    step_debug = False  # only show one rollout\n",
    "\n",
    "            U_R[i, j] = float(np.mean(r_list))\n",
    "            U_S[i, j] = float(np.mean(s_list))\n",
    "\n",
    "            diag[(i, j)] = {\n",
    "                \"det_rate\": det / rollouts,\n",
    "                \"goal_rate\": goal / rollouts,\n",
    "                \"mean_steps\": float(np.mean(steps_list)),\n",
    "                \"mean_risk\": float(np.mean(risk_list)),\n",
    "                \"std_UR\": float(np.std(r_list)),\n",
    "                \"std_US\": float(np.std(s_list)),\n",
    "            }\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(\"[Eval] Compact payoff summary:\")\n",
    "        for i, rpol in enumerate(robots):\n",
    "            for j, spol in enumerate(sensors):\n",
    "                d = diag[(i, j)]\n",
    "                log.info(\n",
    "                    f\"  (R{i}:{rpol.name}, S{j}:{spol.name}) \"\n",
    "                    f\"UR={U_R[i,j]:8.3f}±{d['std_UR']:.2f} | \"\n",
    "                    f\"US={U_S[i,j]:8.3f}±{d['std_US']:.2f} | \"\n",
    "                    f\"det%={100*d['det_rate']:5.1f} goal%={100*d['goal_rate']:5.1f} \"\n",
    "                    f\"steps={d['mean_steps']:.1f} risk={d['mean_risk']:.2f}\"\n",
    "                )\n",
    "\n",
    "    return U_R, U_S, diag\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NBS solver (with optional entropy regularization)\n",
    "# =============================================================================\n",
    "\n",
    "def project_simplex(v: np.ndarray, z: float = 1.0) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    if v.size == 0:\n",
    "        raise ValueError(\"Empty vector\")\n",
    "    if z <= 0:\n",
    "        raise ValueError(\"z must be > 0\")\n",
    "\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, v.size + 1) > (cssv - z))[0]\n",
    "    if rho.size == 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    rho = int(rho[-1])\n",
    "    theta = (cssv[rho] - z) / (rho + 1.0)\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    s = float(w.sum())\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    return w * (z / s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NBSResult:\n",
    "    x: np.ndarray\n",
    "    obj: float\n",
    "    gains: Tuple[float, float]\n",
    "    support: int\n",
    "\n",
    "\n",
    "def solve_nbs(\n",
    "    uR: np.ndarray,\n",
    "    uS: np.ndarray,\n",
    "    log: Logger,\n",
    "    max_iters: int = 400,\n",
    "    alpha: float = 0.5,\n",
    "    tol_l1: float = 1e-6,\n",
    "    kappa: float = 1e-6,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    ") -> NBSResult:\n",
    "    uR = np.asarray(uR, dtype=float).reshape(-1)\n",
    "    uS = np.asarray(uS, dtype=float).reshape(-1)\n",
    "    if uR.shape != uS.shape:\n",
    "        raise ValueError(\"uR and uS must have same shape\")\n",
    "    d = uR.size\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Need >=2 joint actions\")\n",
    "\n",
    "    unif = np.full(d, 1.0 / d)\n",
    "\n",
    "    disagreement = disagreement.lower().strip()\n",
    "    if disagreement == \"minminus\":\n",
    "        dR = float(np.min(uR) - 1.0)\n",
    "        dS = float(np.min(uS) - 1.0)\n",
    "    elif disagreement == \"uniform\":\n",
    "        dR = float(uR @ unif)\n",
    "        dS = float(uS @ unif)\n",
    "    else:\n",
    "        raise ValueError(\"disagreement must be 'minminus' or 'uniform'\")\n",
    "\n",
    "    x = unif.copy()\n",
    "\n",
    "    def gains(xv: np.ndarray) -> Tuple[float, float]:\n",
    "        return float(uR @ xv - dR), float(uS @ xv - dS)\n",
    "\n",
    "    def entropy(xv: np.ndarray) -> float:\n",
    "        xx = np.clip(xv, 1e-12, 1.0)\n",
    "        return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "    def obj(xv: np.ndarray) -> float:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return float(np.log(gR) + np.log(gS) + entropy_tau * entropy(xv))\n",
    "\n",
    "    def grad(xv: np.ndarray) -> np.ndarray:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        g = (uR / gR) + (uS / gS)\n",
    "        if entropy_tau > 0:\n",
    "            xx = np.clip(xv, 1e-12, 1.0)\n",
    "            g += entropy_tau * (-(np.log(xx) + 1.0))\n",
    "        return g\n",
    "\n",
    "    last = obj(x)\n",
    "    log.info(f\"[NBS] d={d} disagreement=({dR:.3f},{dS:.3f}) entropy_tau={entropy_tau:.3g}\")\n",
    "\n",
    "    for t in range(1, max_iters + 1):\n",
    "        g = grad(x)\n",
    "        a = alpha\n",
    "        improved = False\n",
    "        for _ in range(30):\n",
    "            x_new = project_simplex(x + a * g)\n",
    "            new_obj = obj(x_new)\n",
    "            if new_obj >= last - 1e-12:\n",
    "                improved = True\n",
    "                break\n",
    "            a *= 0.5\n",
    "            if a < 1e-6:\n",
    "                break\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "        delta = float(np.linalg.norm(x_new - x, ord=1))\n",
    "        x = x_new\n",
    "        last = new_obj\n",
    "\n",
    "        if log.k >= 2 and (t <= 5 or t % 25 == 0):\n",
    "            gR, gS = gains(x)\n",
    "            top = np.argsort(-x)[:5]\n",
    "            top_str = \", \".join([f\"{i}:{x[i]:.3f}\" for i in top])\n",
    "            log.debug(f\"[NBS][it={t:3d}] obj={last:.6f} gains=({gR:.3f},{gS:.3f}) L1={delta:.2e} top={top_str}\")\n",
    "\n",
    "        if delta < tol_l1:\n",
    "            break\n",
    "\n",
    "    gR, gS = gains(x)\n",
    "    support = int(np.sum(x > 1e-6))\n",
    "    log.info(f\"[NBS] done: obj={last:.6f} gains=({gR:.3f},{gS:.3f}) support={support}/{d}\")\n",
    "\n",
    "    return NBSResult(x=x, obj=float(last), gains=(float(gR), float(gS)), support=support)\n",
    "\n",
    "\n",
    "def joint_to_matrix(x: np.ndarray, m: int, n: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size != m * n:\n",
    "        raise ValueError(\"x size mismatch\")\n",
    "    X = x.reshape((m, n))\n",
    "    s = float(X.sum())\n",
    "    if not np.isfinite(s) or abs(s - 1.0) > 1e-6:\n",
    "        # Renormalize defensively\n",
    "        X = X / max(s, 1e-12)\n",
    "    return X\n",
    "\n",
    "\n",
    "def marginals_from_joint(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    sigma_R = X.sum(axis=1)\n",
    "    sigma_S = X.sum(axis=0)\n",
    "    if sigma_R.sum() > 0:\n",
    "        sigma_R = sigma_R / sigma_R.sum()\n",
    "    if sigma_S.sum() > 0:\n",
    "        sigma_S = sigma_S / sigma_S.sum()\n",
    "    return sigma_R, sigma_S\n",
    "\n",
    "\n",
    "def entropy_of_joint(X: np.ndarray) -> float:\n",
    "    xx = np.clip(X.reshape(-1), 1e-12, 1.0)\n",
    "    return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Best responses (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_expected_risk_map_from_policy_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi: np.ndarray,\n",
    "    M_modes: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Expected p_true(cell) under mixture over sensor POLICIES (not modes).\n",
    "\n",
    "    For each sensor policy j, we use its mode at t=0 as its defining mode.\n",
    "    (This matches FixedModeSensorPolicy exactly.)\n",
    "    \"\"\"\n",
    "    pi = np.asarray(pi, dtype=float).reshape(-1)\n",
    "    if pi.size != len(sensors):\n",
    "        raise ValueError(\"mixture length mismatch\")\n",
    "\n",
    "    mode_probs = np.zeros(M_modes, dtype=float)\n",
    "    for j, sp in enumerate(sensors):\n",
    "        m = int(sp.select_mode(0))\n",
    "        if not (0 <= m < M_modes):\n",
    "            raise ValueError(\"invalid sensor mode\")\n",
    "        mode_probs[m] += float(pi[j])\n",
    "    if mode_probs.sum() > 0:\n",
    "        mode_probs = mode_probs / mode_probs.sum()\n",
    "\n",
    "    H, W = env.grid.height, env.grid.width\n",
    "    risk = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in env.grid.obstacles:\n",
    "                risk[y, x] = np.nan\n",
    "                continue\n",
    "            val = 0.0\n",
    "            for m in range(M_modes):\n",
    "                val += float(mode_probs[m]) * float(env.true_detection_prob((x, y), m))\n",
    "            risk[y, x] = float(val)\n",
    "\n",
    "    return risk\n",
    "\n",
    "\n",
    "def plan_risk_weighted_path(env: GridWorldStealthEnv, risk_map: np.ndarray, risk_weight: float) -> List[Tuple[int, int]]:\n",
    "    def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "        x, y = to\n",
    "        r = risk_map[y, x]\n",
    "        if not np.isfinite(r):\n",
    "            return 1e9\n",
    "        return 1.0 + float(risk_weight) * float(r)\n",
    "\n",
    "    return astar_path(env.grid, env.grid.start, env.grid.goal, step_cost)\n",
    "\n",
    "\n",
    "def robot_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi_S: np.ndarray,\n",
    "    robots: List[RobotPolicy],\n",
    "    M_modes: int,\n",
    "    risk_weight: float,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> RobotPolicy:\n",
    "    risk = compute_expected_risk_map_from_policy_mixture(env, sensors, pi_S, M_modes=M_modes)\n",
    "    try:\n",
    "        path = plan_risk_weighted_path(env, risk, risk_weight=risk_weight)\n",
    "    except Exception as e:\n",
    "        log.info(f\"[RobotBR] WARNING A* failed ({tag}): {e}\")\n",
    "        return robots[0]\n",
    "\n",
    "    path_tuple = tuple(path)\n",
    "    for p in robots:\n",
    "        if isinstance(p, FixedPathPolicy) and tuple(p.path) == path_tuple:\n",
    "            log.info(f\"[RobotBR] ({tag}) BR path already exists: {p.name}\")\n",
    "            return p\n",
    "\n",
    "    newp = FixedPathPolicy(path, name=f\"R_BR_{tag}_w{risk_weight:.1f}_len{len(path)}\")\n",
    "    log.info(f\"[RobotBR] ({tag}) Added new robot policy: {newp.name}\")\n",
    "    return newp\n",
    "\n",
    "\n",
    "def sensor_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    pi_R: np.ndarray,\n",
    "    candidate_modes: List[int],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> FixedModeSensorPolicy:\n",
    "    \"\"\"Sensor best response with common-random-numbers (CRN).\n",
    "\n",
    "    Why CRN matters: payoff variance is large (detection is a rare/threshold event).\n",
    "    If each candidate mode is evaluated on different random rollouts, you can pick\n",
    "    the wrong 'best mode' by noise, which then breaks the PSRO expansion logic.\n",
    "\n",
    "    Fix: reuse the same sampled robot indices AND the same episode seeds across all\n",
    "    candidate modes.\n",
    "    \"\"\"\n",
    "    pi_R = np.asarray(pi_R, dtype=float).reshape(-1)\n",
    "    if pi_R.size != len(robots):\n",
    "        raise ValueError(\"pi_R length mismatch\")\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    # Same robot-index samples for every mode\n",
    "    robot_idxs = rng.choice(len(robots), size=rollouts, p=pi_R, replace=True)\n",
    "\n",
    "    # Same episode seeds for every mode (common random numbers)\n",
    "    seeds = (int(base_seed) + np.arange(rollouts)).astype(int)\n",
    "\n",
    "    best_mode: Optional[int] = None\n",
    "    best_val = -1e18\n",
    "\n",
    "    for mode in candidate_modes:\n",
    "        if not (0 <= mode < M_modes):\n",
    "            continue\n",
    "        sp = FixedModeSensorPolicy(mode, name=f\"S_BR_{tag}_Mode{mode}\")\n",
    "\n",
    "        vals: List[float] = []\n",
    "        for k in range(rollouts):\n",
    "            rp = robots[int(robot_idxs[k])]\n",
    "            st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=int(seeds[k]))\n",
    "            vals.append(st.U_S)\n",
    "\n",
    "        mean_u = float(np.mean(vals))\n",
    "        if log.k >= 2:\n",
    "            log.debug(f\"[SensorBR] ({tag}) mode={mode} E[US]={mean_u:.3f} std={float(np.std(vals)):.2f}\")\n",
    "\n",
    "        if mean_u > best_val:\n",
    "            best_val = mean_u\n",
    "            best_mode = mode\n",
    "\n",
    "    if best_mode is None:\n",
    "        raise RuntimeError(\"No valid sensor BR mode found\")\n",
    "\n",
    "    log.info(f\"[SensorBR] ({tag}) Best mode={best_mode} E[US]={best_val:.3f}\")\n",
    "    return FixedModeSensorPolicy(best_mode, name=f\"S_BR_{tag}_Mode{best_mode}\")\n",
    "\n",
    "\n",
    "def conditional_sensor_given_robot(X: np.ndarray, i: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    row = np.asarray(X[i, :], dtype=float)\n",
    "    s = float(row.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(row, 1.0 / row.size)\n",
    "    return row / s\n",
    "\n",
    "\n",
    "def conditional_robot_given_sensor(X: np.ndarray, j: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    col = np.asarray(X[:, j], dtype=float)\n",
    "    s = float(col.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(col, 1.0 / col.size)\n",
    "    return col / s\n",
    "\n",
    "\n",
    "def compute_ce_regrets(U_R: np.ndarray, U_S: np.ndarray, X: np.ndarray, eps: float = 1e-12) -> Dict[str, float]:\n",
    "    \"\"\"Conditional recommendation regrets (CE-style) computed on current meta-game.\n",
    "\n",
    "    For robot (given recommendation i):\n",
    "        regret_R(i) = max_{i'} E_{j~q(.|i)}[U_R(i',j) - U_R(i,j)]\n",
    "\n",
    "    For sensor (given recommendation j):\n",
    "        regret_S(j) = max_{j'} E_{i~q(.|j)}[U_S(i,j') - U_S(i,j)]\n",
    "\n",
    "    Returns max and average regrets.\n",
    "    \"\"\"\n",
    "    m, n = U_R.shape\n",
    "    assert U_S.shape == (m, n)\n",
    "    assert X.shape == (m, n)\n",
    "\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "    reg_R = []\n",
    "    for i in range(m):\n",
    "        if sigma_R[i] <= eps:\n",
    "            continue\n",
    "        q = conditional_sensor_given_robot(X, i, eps=eps)\n",
    "        rec = float(np.dot(q, U_R[i, :]))\n",
    "        best = rec\n",
    "        for ip in range(m):\n",
    "            val = float(np.dot(q, U_R[ip, :]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_R.append(best - rec)\n",
    "\n",
    "    reg_S = []\n",
    "    for j in range(n):\n",
    "        if sigma_S[j] <= eps:\n",
    "            continue\n",
    "        q = conditional_robot_given_sensor(X, j, eps=eps)\n",
    "        rec = float(np.dot(q, U_S[:, j]))\n",
    "        best = rec\n",
    "        for jp in range(n):\n",
    "            val = float(np.dot(q, U_S[:, jp]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_S.append(best - rec)\n",
    "\n",
    "    return {\n",
    "        \"max_regret_R\": float(max(reg_R) if reg_R else 0.0),\n",
    "        \"max_regret_S\": float(max(reg_S) if reg_S else 0.0),\n",
    "        \"mean_regret_R\": float(np.mean(reg_R) if reg_R else 0.0),\n",
    "        \"mean_regret_S\": float(np.mean(reg_S) if reg_S else 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def find_sensor_by_mode(pols: List[SensorPolicy], mode: int) -> Optional[FixedModeSensorPolicy]:\n",
    "    for p in pols:\n",
    "        if isinstance(p, FixedModeSensorPolicy) and p.mode == mode:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policy initialization + evaluator for joint strategy\n",
    "# =============================================================================\n",
    "\n",
    "def build_initial_policies(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "    ]\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StrategyEval:\n",
    "    mean_U_R: float\n",
    "    mean_U_S: float\n",
    "    det_rate: float\n",
    "    goal_rate: float\n",
    "    mean_steps: float\n",
    "    mean_risk: float\n",
    "\n",
    "\n",
    "def evaluate_joint_strategy(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    X: np.ndarray,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    m, n = X.shape\n",
    "    probs = X.reshape(-1)\n",
    "    probs = probs / max(float(probs.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR = []\n",
    "    US = []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        idx = int(rng.choice(m * n, p=probs))\n",
    "        i, j = np.unravel_index(idx, (m, n))\n",
    "        seed = base_seed + k\n",
    "        st = rollout_episode(env, robots[i], sensors[j], M_modes=M_modes, seed=seed)\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training loop (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainHistoryRow:\n",
    "    outer_iter: int\n",
    "    m: int\n",
    "    n: int\n",
    "    nbs_obj: float\n",
    "    entropy_X: float\n",
    "    max_regret_R: float\n",
    "    max_regret_S: float\n",
    "    selfplay_UR: float\n",
    "    selfplay_US: float\n",
    "    selfplay_det: float\n",
    "    selfplay_goal: float\n",
    "    seconds: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    solver: str\n",
    "    env: GridWorldStealthEnv\n",
    "    robots: List[RobotPolicy]\n",
    "    sensors: List[SensorPolicy]\n",
    "    X: np.ndarray\n",
    "    history: List[TrainHistoryRow]\n",
    "\n",
    "\n",
    "def run_training(env: GridWorldStealthEnv, args: argparse.Namespace, solver: str, log: Logger) -> TrainResult:\n",
    "    t0_all = time.time()\n",
    "\n",
    "    grid = env.grid\n",
    "    sensor_cfg = env.sensor_cfg\n",
    "    M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "    robots, sensors = build_initial_policies(env, M_modes=M_modes)\n",
    "\n",
    "    # Optional: reduce initial set if you want smaller games.\n",
    "    # (We keep it as-is for benchmarks.)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(f\"[{solver}] Initial robots: \" + \", \".join([p.name for p in robots]))\n",
    "        log.info(f\"[{solver}] Initial sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "\n",
    "    debug_pair = None\n",
    "    if args.debug_rollout_pair:\n",
    "        parts = args.debug_rollout_pair.split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            debug_pair = (int(parts[0]), int(parts[1]))\n",
    "            log.info(f\"[{solver}] Will print one step-by-step rollout for pair {debug_pair} (only once).\")\n",
    "\n",
    "    history: List[TrainHistoryRow] = []\n",
    "    X = None\n",
    "\n",
    "    for it in range(1, args.outer_iters + 1):\n",
    "        t0 = time.time()\n",
    "        log.banner(f\"[{solver}] Outer iter {it}/{args.outer_iters}\")\n",
    "\n",
    "        U_R, U_S, _diag = evaluate_payoffs(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            M_modes=M_modes,\n",
    "            rollouts=args.rollouts_payoff,\n",
    "            base_seed=1000 + 100 * it,\n",
    "            log=log,\n",
    "            debug_rollout_pair=debug_pair,\n",
    "        )\n",
    "        debug_pair = None\n",
    "\n",
    "        # Solve NBS over joint actions\n",
    "        uR = U_R.reshape(-1)\n",
    "        uS = U_S.reshape(-1)\n",
    "\n",
    "        nbs = solve_nbs(\n",
    "            uR,\n",
    "            uS,\n",
    "            log=log,\n",
    "            disagreement=args.disagreement,\n",
    "            entropy_tau=args.entropy_tau,\n",
    "        )\n",
    "\n",
    "        m, n = U_R.shape\n",
    "        X = joint_to_matrix(nbs.x, m, n)\n",
    "        sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "        # Print top joint actions\n",
    "        top = np.argsort(-X.reshape(-1))[:min(5, X.size)]\n",
    "        log.info(f\"[{solver}] Top joint actions:\")\n",
    "        for k, idx in enumerate(top, start=1):\n",
    "            i, j = np.unravel_index(int(idx), (m, n))\n",
    "            log.info(f\"  #{k}: (R{i}:{robots[i].name}, S{j}:{sensors[j].name}) prob={X[i,j]:.4f}\")\n",
    "        log.info(f\"[{solver}] sigma_R={sigma_R.round(3)}\")\n",
    "        log.info(f\"[{solver}] sigma_S={sigma_S.round(3)}\")\n",
    "\n",
    "        # Stability diagnostics\n",
    "        regrets = compute_ce_regrets(U_R, U_S, X)\n",
    "        ent = entropy_of_joint(X)\n",
    "\n",
    "        # Self-play evaluation under joint X\n",
    "        sp = evaluate_joint_strategy(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            X,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.eval_episodes,\n",
    "            base_seed=9000 + 100 * it,\n",
    "        )\n",
    "\n",
    "        log.info(\n",
    "            f\"[{solver}] CE-regrets: maxR={regrets['max_regret_R']:.3f} maxS={regrets['max_regret_S']:.3f} | \"\n",
    "            f\"SelfPlay: UR={sp.mean_U_R:.2f} US={sp.mean_U_S:.2f} det%={100*sp.det_rate:.1f} goal%={100*sp.goal_rate:.1f} | \"\n",
    "            f\"H(X)={ent:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Best-response expansion\n",
    "        if solver == \"marginal\":\n",
    "            br_r = robot_best_response_to_mixture(\n",
    "                env,\n",
    "                sensors,\n",
    "                pi_S=sigma_S,\n",
    "                robots=robots,\n",
    "                M_modes=M_modes,\n",
    "                risk_weight=args.risk_weight_br,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "            br_s = sensor_best_response_to_mixture(\n",
    "                env,\n",
    "                robots,\n",
    "                pi_R=sigma_R,\n",
    "                candidate_modes=list(range(M_modes)),\n",
    "                M_modes=M_modes,\n",
    "                rollouts=args.rollouts_br,\n",
    "                base_seed=2000 + 100 * it,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "        elif solver == \"correlated\":\n",
    "            # FIX: choose conditional mixtures q(.|i) and q(.|j)\n",
    "            # We only check top-K recommendations to keep it tractable.\n",
    "            topK = max(1, int(args.cond_top_k))\n",
    "\n",
    "            # Robot: check the top-K robot recommendations by sigma_R\n",
    "            cand_i = [int(i) for i in np.argsort(-sigma_R) if sigma_R[int(i)] > 1e-8][:topK]\n",
    "            if not cand_i:\n",
    "                cand_i = [int(i) for i in np.argsort(-sigma_R)[:topK]]\n",
    "            best_gain = 0.0\n",
    "            br_r = robots[0]\n",
    "\n",
    "            for i in cand_i:\n",
    "                qS = conditional_sensor_given_robot(X, int(i))\n",
    "                tag = f\"Cond_i{i}\"\n",
    "                pol = robot_best_response_to_mixture(\n",
    "                    env,\n",
    "                    sensors,\n",
    "                    pi_S=qS,\n",
    "                    robots=robots,\n",
    "                    M_modes=M_modes,\n",
    "                    risk_weight=args.risk_weight_br,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement: E_q[U_R(pol,j)] - E_q[U_R(i,j)]\n",
    "                # If pol is already in set, its payoff exists in U_R row of that policy.\n",
    "                # Otherwise we simulate pol against each sensor policy j.\n",
    "                if pol in robots:\n",
    "                    ip = robots.index(pol)\n",
    "                    dev = float(np.dot(qS, U_R[ip, :]))\n",
    "                else:\n",
    "                    # simulate quickly vs each sensor policy\n",
    "                    dev_vals = []\n",
    "                    for j in range(n):\n",
    "                        vals = []\n",
    "                        for kk in range(args.br_eval_rollouts):\n",
    "                            seed = 777000 + 1000 * it + 100 * i + 10 * j + kk\n",
    "                            st = rollout_episode(env, pol, sensors[j], M_modes=M_modes, seed=seed)\n",
    "                            vals.append(st.U_R)\n",
    "                        dev_vals.append(float(np.mean(vals)))\n",
    "                    dev = float(np.dot(qS, np.asarray(dev_vals)))\n",
    "\n",
    "                rec = float(np.dot(qS, U_R[i, :]))\n",
    "                gain = dev - rec\n",
    "                if gain > best_gain + 1e-9:\n",
    "                    best_gain = gain\n",
    "                    br_r = pol\n",
    "\n",
    "            if best_gain > args.add_threshold:\n",
    "                if br_r in robots:\n",
    "                    log.info(f\"[{solver}] Best robot deviation already in set; est_gain={best_gain:.3f}\")\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding robot deviation; est_gain={best_gain:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No robot deviation above threshold (best_gain={best_gain:.3f}).\")\n",
    "\n",
    "            # Sensor: check top-K sensor recommendations by sigma_S\n",
    "            cand_j = [int(j) for j in np.argsort(-sigma_S) if sigma_S[int(j)] > 1e-8][:topK]\n",
    "            if not cand_j:\n",
    "                cand_j = [int(j) for j in np.argsort(-sigma_S)[:topK]]\n",
    "            best_gain_s = 0.0\n",
    "            br_s = FixedModeSensorPolicy(0, name=\"S_dummy\")\n",
    "\n",
    "            for j in cand_j:\n",
    "                qR = conditional_robot_given_sensor(X, int(j))\n",
    "                tag = f\"Cond_j{j}\"\n",
    "                polS = sensor_best_response_to_mixture(\n",
    "                    env,\n",
    "                    robots,\n",
    "                    pi_R=qR,\n",
    "                    candidate_modes=list(range(M_modes)),\n",
    "                    M_modes=M_modes,\n",
    "                    rollouts=args.rollouts_br,\n",
    "                    base_seed=333000 + 1000 * it + 10 * j,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement for sensor\n",
    "                # rec under recommendation j is E_q[U_S(i,j)]\n",
    "                recS = float(np.dot(qR, U_S[:, j]))\n",
    "\n",
    "                # dev under mode polS.mode: if already present, use its column.\n",
    "                existing_col = None\n",
    "                for jj, spj in enumerate(sensors):\n",
    "                    if isinstance(spj, FixedModeSensorPolicy) and spj.mode == polS.mode:\n",
    "                        existing_col = jj\n",
    "                        break\n",
    "\n",
    "                if existing_col is not None:\n",
    "                    devS = float(np.dot(qR, U_S[:, existing_col]))\n",
    "                else:\n",
    "                    vals = []\n",
    "                    for kk in range(args.br_eval_rollouts):\n",
    "                        i_samp = int(np.random.default_rng(444 + kk).choice(len(robots), p=qR))\n",
    "                        seed = 888000 + 1000 * it + 10 * j + kk\n",
    "                        st = rollout_episode(env, robots[i_samp], polS, M_modes=M_modes, seed=seed)\n",
    "                        vals.append(st.U_S)\n",
    "                    devS = float(np.mean(vals))\n",
    "\n",
    "                gainS = devS - recS\n",
    "                if gainS > best_gain_s + 1e-9:\n",
    "                    best_gain_s = gainS\n",
    "                    br_s = polS\n",
    "\n",
    "            if best_gain_s > args.add_threshold:\n",
    "                if (isinstance(br_s, FixedModeSensorPolicy)) and (find_sensor_by_mode(sensors, br_s.mode) is not None):\n",
    "                    log.info(f\"[{solver}] Best sensor deviation already in set (mode={br_s.mode}); est_gain={best_gain_s:.3f}\")\n",
    "                    br_s = None\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding sensor deviation; est_gain={best_gain_s:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No sensor deviation above threshold (best_gain={best_gain_s:.3f}).\")\n",
    "                br_s = None\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"solver must be marginal or correlated\")\n",
    "\n",
    "        # Add to sets (dedupe)\n",
    "        if br_r not in robots:\n",
    "            robots.append(br_r)\n",
    "\n",
    "        if isinstance(br_s, FixedModeSensorPolicy):\n",
    "            if find_sensor_by_mode(sensors, br_s.mode) is None:\n",
    "                sensors.append(br_s)\n",
    "            else:\n",
    "                log.info(f\"[{solver}] Sensor mode {br_s.mode} already present; not adding duplicate.\")\n",
    "\n",
    "        seconds = float(time.time() - t0)\n",
    "        history.append(\n",
    "            TrainHistoryRow(\n",
    "                outer_iter=it,\n",
    "                m=len(robots),\n",
    "                n=len(sensors),\n",
    "                nbs_obj=float(nbs.obj),\n",
    "                entropy_X=float(ent),\n",
    "                max_regret_R=float(regrets[\"max_regret_R\"]),\n",
    "                max_regret_S=float(regrets[\"max_regret_S\"]),\n",
    "                selfplay_UR=float(sp.mean_U_R),\n",
    "                selfplay_US=float(sp.mean_U_S),\n",
    "                selfplay_det=float(sp.det_rate),\n",
    "                selfplay_goal=float(sp.goal_rate),\n",
    "                seconds=seconds,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        log.info(f\"[{solver}] Sets: |Pi_R|={len(robots)} |Pi_S|={len(sensors)} | iter_seconds={seconds:.2f}\")\n",
    "\n",
    "    if X is None:\n",
    "        raise RuntimeError(\"Training produced no X\")\n",
    "\n",
    "    log.banner(f\"[{solver}] Finished\")\n",
    "    log.info(f\"[{solver}] Final robots: \" + \", \".join([p.name for p in robots]))\n",
    "    log.info(f\"[{solver}] Final sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "    log.info(f\"[{solver}] Total time: {time.time()-t0_all:.2f}s\")\n",
    "\n",
    "    return TrainResult(solver=solver, env=env, robots=robots, sensors=sensors, X=X, history=history)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Benchmarks + plotting\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BenchCell:\n",
    "    UR: float\n",
    "    US: float\n",
    "    det: float\n",
    "    goal: float\n",
    "\n",
    "\n",
    "def run_policy_matrix_benchmark(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_methods: List[RobotPolicy],\n",
    "    sensor_methods: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[Tuple[int, int], BenchCell]:\n",
    "    res: Dict[Tuple[int, int], BenchCell] = {}\n",
    "    for i, rp in enumerate(robot_methods):\n",
    "        for j, sp in enumerate(sensor_methods):\n",
    "            UR = []\n",
    "            US = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            for k in range(episodes):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed)\n",
    "                UR.append(st.U_R)\n",
    "                US.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "            res[(i, j)] = BenchCell(\n",
    "                UR=float(np.mean(UR)),\n",
    "                US=float(np.mean(US)),\n",
    "                det=float(det / episodes),\n",
    "                goal=float(goal / episodes),\n",
    "            )\n",
    "    return res\n",
    "\n",
    "\n",
    "def safe_makedirs(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_history_csv(hist: List[TrainHistoryRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    fields = list(TrainHistoryRow.__annotations__.keys())\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fields)\n",
    "        w.writeheader()\n",
    "        for row in hist:\n",
    "            w.writerow({k: getattr(row, k) for k in fields})\n",
    "\n",
    "\n",
    "def plot_training_curves(results: List[TrainResult], outdir: str) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # 1) NBS objective\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.nbs_obj for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"NBS objective\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_nbs_obj.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Max conditional regrets\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_R for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: maxRegret_R\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_S for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: maxRegret_S\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Max conditional regret\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_max_regrets.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 3) Entropy of X\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.entropy_X for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Entropy H(X)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_entropy_X.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 4) Self-play outcomes\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_UR for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: UR\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_US for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: US\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Expected utility under X\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_selfplay_utils.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_benchmark_bars(\n",
    "    robot_names: List[str],\n",
    "    sensor_names: List[str],\n",
    "    bench: Dict[Tuple[int, int], BenchCell],\n",
    "    outdir: str,\n",
    ") -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # For each sensor, bar chart of robot UR\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].UR for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Robot utility\")\n",
    "        plt.title(f\"Robot utility vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_UR_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # For each sensor, bar chart of robot goal rate\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].goal for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Goal rate\")\n",
    "        plt.title(f\"Robot goal rate vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_goal_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main pipeline wrapper\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(args: argparse.Namespace) -> None:\n",
    "    log = Logger(args.log_level)\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.banner(\"[PIPELINE] Stealth grid game\")\n",
    "        log.info(f\"Grid: {grid.width}x{grid.height} start={grid.start} goal={grid.goal} obstacles={len(grid.obstacles)}\")\n",
    "        log.info(f\"Sensors: {sensor_cfg.sensors} radius={sensor_cfg.radius} base_p={sensor_cfg.base_p} hotspot_p={sensor_cfg.hotspot_p}\")\n",
    "        if log.k >= 2:\n",
    "            log.debug(\"ASCII map:\")\n",
    "            print_grid_ascii(grid, sensor_cfg)\n",
    "\n",
    "    solvers: List[str]\n",
    "    if args.solver == \"both\":\n",
    "        solvers = [\"marginal\", \"correlated\"]\n",
    "    else:\n",
    "        solvers = [args.solver]\n",
    "\n",
    "    results: List[TrainResult] = []\n",
    "    for s in solvers:\n",
    "        # Use a fresh env copy per solver (to avoid RNG coupling)\n",
    "        env_s = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "        results.append(run_training(env_s, args, solver=s, log=log))\n",
    "\n",
    "    if args.results_dir:\n",
    "        safe_makedirs(args.results_dir)\n",
    "        for r in results:\n",
    "            save_history_csv(r.history, os.path.join(args.results_dir, f\"history_{r.solver}.csv\"))\n",
    "\n",
    "    if args.save_plots and args.results_dir:\n",
    "        plot_training_curves(results, outdir=args.results_dir)\n",
    "\n",
    "    # Benchmarks on the same game\n",
    "    if args.run_benchmarks:\n",
    "        log.banner(\"[BENCH] Heuristics on same game\")\n",
    "        M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "        # Robot heuristics\n",
    "        #  - fixed paths from initial set\n",
    "        robots_init, sensors_init = build_initial_policies_for_bench(env, M_modes)\n",
    "\n",
    "        # Sensor heuristics\n",
    "        sensor_methods: List[SensorPolicy] = [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=args.seed + 123, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "        bench = run_policy_matrix_benchmark(\n",
    "            env,\n",
    "            robot_methods=robots_init,\n",
    "            sensor_methods=sensor_methods,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.bench_episodes,\n",
    "            base_seed=555000,\n",
    "        )\n",
    "\n",
    "        # Print a compact table\n",
    "        robot_names = [p.name for p in robots_init]\n",
    "        sensor_names = [p.name for p in sensor_methods]\n",
    "        for j, sname in enumerate(sensor_names):\n",
    "            log.info(f\"[BENCH] Sensor={sname}\")\n",
    "            for i, rname in enumerate(robot_names):\n",
    "                cell = bench[(i, j)]\n",
    "                log.info(f\"  Robot={rname:22s} UR={cell.UR:8.2f} goal%={100*cell.goal:5.1f} det%={100*cell.det:5.1f}\")\n",
    "\n",
    "        if args.save_plots and args.results_dir:\n",
    "            plot_benchmark_bars(robot_names, sensor_names, bench, outdir=args.results_dir)\n",
    "\n",
    "\n",
    "# Helper: initial robot set for benchmark (without the solver-added BR policies)\n",
    "\n",
    "def build_initial_policies_for_bench(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    # Static risk-aware A* under UNIFORM mode belief\n",
    "    uniform_mode = np.full(M_modes, 1.0 / M_modes)\n",
    "\n",
    "    # Construct risk map directly under uniform belief\n",
    "    H, W = grid.height, grid.width\n",
    "    risk_uniform = np.zeros((H, W), dtype=float)\n",
    "    risk_worst = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in grid.obstacles:\n",
    "                risk_uniform[y, x] = np.nan\n",
    "                risk_worst[y, x] = np.nan\n",
    "                continue\n",
    "            vals = [env.true_detection_prob((x, y), m) for m in range(M_modes)]\n",
    "            risk_uniform[y, x] = float(np.dot(uniform_mode, vals))\n",
    "            risk_worst[y, x] = float(np.max(vals))\n",
    "\n",
    "    p_uniform = plan_risk_weighted_path(env, risk_uniform, risk_weight=12.0)\n",
    "    p_worst = plan_risk_weighted_path(env, risk_worst, risk_weight=12.0)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        FixedPathPolicy(p_uniform, \"R_RiskAStar_Uniform\"),\n",
    "        FixedPathPolicy(p_worst, \"R_RiskAStar_WorstCase\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "    ]\n",
    "\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLI\n",
    "# =============================================================================\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> Tuple[argparse.Namespace, List[str]]:\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "    p.add_argument(\"--log-level\", type=str, default=\"INFO\", choices=[\"QUIET\", \"INFO\", \"DEBUG\"])\n",
    "\n",
    "    p.add_argument(\"--solver\", type=str, default=\"correlated\", choices=[\"marginal\", \"correlated\", \"both\"])\n",
    "\n",
    "    p.add_argument(\"--outer-iters\", type=int, default=3)\n",
    "\n",
    "    p.add_argument(\"--rollouts-payoff\", type=int, default=20)\n",
    "    p.add_argument(\"--rollouts-br\", type=int, default=30)\n",
    "    p.add_argument(\"--risk-weight-br\", type=float, default=12.0)\n",
    "\n",
    "    # NBS knobs\n",
    "    p.add_argument(\"--disagreement\", type=str, default=\"minminus\", choices=[\"minminus\", \"uniform\"])\n",
    "    p.add_argument(\"--entropy-tau\", type=float, default=0.0)\n",
    "\n",
    "    # Fix mismatch knobs\n",
    "    p.add_argument(\"--cond-top-k\", type=int, default=2, help=\"How many top recommendations to check for conditional BRs\")\n",
    "    p.add_argument(\"--br-eval-rollouts\", type=int, default=8, help=\"Small evaluation rollouts for new deviations\")\n",
    "    p.add_argument(\"--add-threshold\", type=float, default=0.25, help=\"Minimum estimated conditional gain to add a deviation policy\")\n",
    "\n",
    "    # Eval episodes under joint X (self-play)\n",
    "    p.add_argument(\"--eval-episodes\", type=int, default=60)\n",
    "\n",
    "    # Debug\n",
    "    p.add_argument(\"--debug-rollout-pair\", type=str, default=\"\", help=\"Print one step-by-step rollout for i,j\")\n",
    "\n",
    "    # Outputs\n",
    "    p.add_argument(\"--results-dir\", type=str, default=\"results\", help=\"Directory for csv/plots\")\n",
    "    p.add_argument(\"--save-plots\", action=\"store_true\")\n",
    "\n",
    "    # Benchmarks\n",
    "    p.add_argument(\"--run-benchmarks\", action=\"store_true\")\n",
    "    p.add_argument(\"--bench-episodes\", type=int, default=80)\n",
    "\n",
    "    args, unknown = p.parse_known_args(args=argv)\n",
    "    return args, unknown\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    args, unknown = parse_args(argv=argv)\n",
    "    if unknown and (\"ipykernel\" not in sys.modules):\n",
    "        print(f\"[WARN] Ignoring unknown CLI args: {unknown}\")\n",
    "    if args.debug_rollout_pair.strip() == \"\":\n",
    "        args.debug_rollout_pair = \"\"\n",
    "    run_pipeline(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and (\"ipykernel\" not in sys.modules):\n",
    "    main()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENTS (fixed game set + baselines + tests + presentation plots)\n",
    "# =============================================================================\n",
    "#\n",
    "# Keep the training code above exactly as-is.\n",
    "# This section adds:\n",
    "#   (A) sanity tests (quick asserts)\n",
    "#   (B) a fixed game-set generator (deterministic)\n",
    "#   (C) an experiment runner that compares solvers + baselines on the same games\n",
    "#   (D) plots + CSV outputs for slides\n",
    "#\n",
    "# Notebook usage:\n",
    "#   run_sanity_tests()\n",
    "#   run_fixed_games_experiment(outdir=\"results_exp\", n_games=6, seeds=[0,1,2])\n",
    "#\n",
    "\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GameInstance:\n",
    "    game_id: str\n",
    "    grid: GridConfig\n",
    "    sensor_cfg: SensorConfig\n",
    "    desc: str\n",
    "\n",
    "\n",
    "def _is_reachable(grid: GridConfig) -> bool:\n",
    "    try:\n",
    "        _ = astar_path(grid, grid.start, grid.goal, step_cost=lambda a, b: 1.0, max_expansions=250_000)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def build_fixed_game_set(\n",
    "    n_games: int = 6,\n",
    "    seed: int = 123,\n",
    "    width: int = 15,\n",
    "    height: int = 9,\n",
    "    base_radius: int = 2,\n",
    "    base_p: float = 0.02,\n",
    "    hotspot_p: float = 0.60,\n",
    ") -> List[GameInstance]:\n",
    "    \"\"\"Deterministic *fixed* game set for fair comparisons.\n",
    "\n",
    "    We generate corridor-variant grids by adding a small number of extra obstacles\n",
    "    (without breaking reachability), while keeping the base corridor wall.\n",
    "\n",
    "    Notes:\n",
    "      - Uses a fixed RNG seed => same games every run.\n",
    "      - Guarantees start->goal reachability (ignoring detection risk).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    base_grid = build_two_corridor_grid(width=width, height=height)\n",
    "    wall_x = width // 2\n",
    "\n",
    "    # Keep the two sensor centers near the corridor gaps for interpretability.\n",
    "    base_sensors = ((wall_x, 2), (wall_x, 6))\n",
    "\n",
    "    games: List[GameInstance] = []\n",
    "    attempts = 0\n",
    "\n",
    "    # Increasing difficulty: more extra obstacles.\n",
    "    # (Chosen small so it stays tractable and doesn’t accidentally block corridors.)\n",
    "    obstacle_budgets = [0, 2, 4, 6, 8, 10]\n",
    "    while len(games) < n_games:\n",
    "        attempts += 1\n",
    "        if attempts > 4000:\n",
    "            raise RuntimeError(\"Could not generate enough solvable game instances\")\n",
    "\n",
    "        k = len(games)\n",
    "        extra_obs_target = obstacle_budgets[min(k, len(obstacle_budgets) - 1)]\n",
    "\n",
    "        # Candidate cells for extra obstacles (avoid start/goal/sensors and the wall column).\n",
    "        candidates: List[Tuple[int, int]] = []\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                p = (x, y)\n",
    "                if p in base_grid.obstacles:\n",
    "                    continue\n",
    "                if p == base_grid.start or p == base_grid.goal:\n",
    "                    continue\n",
    "                if p in base_sensors:\n",
    "                    continue\n",
    "                if x == wall_x:\n",
    "                    continue\n",
    "                candidates.append(p)\n",
    "\n",
    "        rng.shuffle(candidates)\n",
    "        extra = set(candidates[:extra_obs_target])\n",
    "\n",
    "        grid = GridConfig(\n",
    "            width=base_grid.width,\n",
    "            height=base_grid.height,\n",
    "            start=base_grid.start,\n",
    "            goal=base_grid.goal,\n",
    "            obstacles=frozenset(set(base_grid.obstacles) | extra),\n",
    "        )\n",
    "        if not _is_reachable(grid):\n",
    "            continue\n",
    "\n",
    "        sensor_cfg = SensorConfig(\n",
    "            sensors=base_sensors,\n",
    "            radius=int(base_radius),\n",
    "            base_p=float(base_p),\n",
    "            hotspot_p=float(hotspot_p),\n",
    "        )\n",
    "\n",
    "        game_id = f\"G{k:02d}_extraObs{extra_obs_target}\"\n",
    "        desc = f\"corridors + {extra_obs_target} extra obstacles\"\n",
    "        games.append(GameInstance(game_id=game_id, grid=grid, sensor_cfg=sensor_cfg, desc=desc))\n",
    "\n",
    "    return games\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Sanity tests\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_sanity_tests() -> None:\n",
    "    \"\"\"Fast, high-signal checks so you can trust comparisons.\"\"\"\n",
    "    print(\"[TEST] Running sanity tests...\")\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=0)\n",
    "\n",
    "    # A* reachability\n",
    "    p = astar_path(grid, grid.start, grid.goal, step_cost=lambda a, b: 1.0)\n",
    "    assert p[0] == grid.start and p[-1] == grid.goal\n",
    "\n",
    "    # Belief update preserves simplex\n",
    "    b = ModeBelief(M=2)\n",
    "    env.reset(sensor_mode=0)\n",
    "    out = env.step((1, 0))\n",
    "    b.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "    assert np.isfinite(b.b).all()\n",
    "    assert abs(float(b.b.sum()) - 1.0) < 1e-6\n",
    "    assert (b.b >= -1e-12).all()\n",
    "\n",
    "    # NBS returns simplex\n",
    "    uR = np.array([-1.0, -2.0, -3.0, -4.0])\n",
    "    uS = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "    nbs = solve_nbs(uR, uS, log=Logger(\"QUIET\"), max_iters=50)\n",
    "    x = nbs.x\n",
    "    assert np.isfinite(x).all()\n",
    "    assert (x >= -1e-10).all()\n",
    "    assert abs(float(x.sum()) - 1.0) < 1e-6\n",
    "\n",
    "    # Joint/marginals/conditionals are sane\n",
    "    X = joint_to_matrix(x, m=2, n=2)\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "    assert abs(float(sigma_R.sum()) - 1.0) < 1e-6\n",
    "    assert abs(float(sigma_S.sum()) - 1.0) < 1e-6\n",
    "    qS = conditional_sensor_given_robot(X, 0)\n",
    "    qR = conditional_robot_given_sensor(X, 0)\n",
    "    assert abs(float(qS.sum()) - 1.0) < 1e-6\n",
    "    assert abs(float(qR.sum()) - 1.0) < 1e-6\n",
    "\n",
    "    # CE regrets should be >= 0 (numerical)\n",
    "    UR = np.random.default_rng(0).normal(size=(3, 2))\n",
    "    US = np.random.default_rng(1).normal(size=(3, 2))\n",
    "    Xr = np.full((3, 2), 1.0 / 6)\n",
    "    reg = compute_ce_regrets(UR, US, Xr)\n",
    "    assert reg[\"max_regret_R\"] >= -1e-9\n",
    "    assert reg[\"max_regret_S\"] >= -1e-9\n",
    "\n",
    "    # Rollout returns finite stats\n",
    "    robots, sensors = build_initial_policies(env, M_modes=2)\n",
    "    st = rollout_episode(env, robots[0], sensors[0], M_modes=2, seed=0)\n",
    "    assert np.isfinite(st.U_R) and np.isfinite(st.U_S)\n",
    "\n",
    "    print(\"[TEST] All sanity tests passed ✅\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Fair comparisons on the same fixed game set\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ExperimentRow:\n",
    "    alg: str\n",
    "    solver: str\n",
    "    game_id: str\n",
    "    seed: int\n",
    "    metric: str\n",
    "    value: float\n",
    "\n",
    "\n",
    "def _evaluate_robot_mixture_against_sensor(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sigma_R: np.ndarray,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    sigma_R = np.asarray(sigma_R, dtype=float).reshape(-1)\n",
    "    sigma_R = sigma_R / max(float(sigma_R.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR, US = [], []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        i = int(rng.choice(len(robots), p=sigma_R))\n",
    "        st = rollout_episode(env, robots[i], sensor, M_modes=M_modes, seed=int(base_seed + k))\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "def _summarize_against_sensor_suite(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sigma_R: np.ndarray,\n",
    "    sensors_suite: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes_per_sensor: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[str, float]:\n",
    "    vals_UR = []\n",
    "    vals_goal = []\n",
    "    vals_det = []\n",
    "\n",
    "    for j, sp in enumerate(sensors_suite):\n",
    "        ev = _evaluate_robot_mixture_against_sensor(\n",
    "            env,\n",
    "            robots,\n",
    "            sigma_R=sigma_R,\n",
    "            sensor=sp,\n",
    "            M_modes=M_modes,\n",
    "            episodes=episodes_per_sensor,\n",
    "            base_seed=base_seed + 10000 * j,\n",
    "        )\n",
    "        vals_UR.append(ev.mean_U_R)\n",
    "        vals_goal.append(ev.goal_rate)\n",
    "        vals_det.append(ev.det_rate)\n",
    "\n",
    "    # \"Robust\" = worst case over the sensor suite.\n",
    "    return {\n",
    "        \"mean_UR\": float(np.mean(vals_UR)),\n",
    "        \"robust_UR\": float(np.min(vals_UR)),\n",
    "        \"mean_goal\": float(np.mean(vals_goal)),\n",
    "        \"robust_goal\": float(np.min(vals_goal)),\n",
    "        \"mean_det\": float(np.mean(vals_det)),\n",
    "        \"robust_det\": float(np.max(vals_det)),\n",
    "    }\n",
    "\n",
    "\n",
    "def _write_experiment_csv(rows: List[ExperimentRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    safe_makedirs(os.path.dirname(path) or \".\")\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"alg\", \"solver\", \"game_id\", \"seed\", \"metric\", \"value\"])\n",
    "        for r in rows:\n",
    "            w.writerow([r.alg, r.solver, r.game_id, r.seed, r.metric, f\"{r.value:.8f}\"])\n",
    "\n",
    "\n",
    "def _group_stats(values: List[float]) -> Dict[str, float]:\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    return {\n",
    "        \"mean\": float(np.mean(arr)) if arr.size else float(\"nan\"),\n",
    "        \"std\": float(np.std(arr)) if arr.size else float(\"nan\"),\n",
    "        \"n\": int(arr.size),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_experiment_summary(rows: List[ExperimentRow], outdir: str) -> None:\n",
    "    \"\"\"Creates high-signal graphs for your presentation.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # Collect per-alg stats for a few key metrics.\n",
    "    metrics = [\n",
    "        \"robust_UR\",\n",
    "        \"mean_UR\",\n",
    "        \"robust_goal\",\n",
    "        \"mean_goal\",\n",
    "        \"robust_det\",\n",
    "        \"runtime_s\",\n",
    "        \"entropy_X\",\n",
    "        \"max_regret_R\",\n",
    "        \"max_regret_S\",\n",
    "    ]\n",
    "\n",
    "    algs = sorted({r.alg for r in rows})\n",
    "\n",
    "    # Build dict: metric -> alg -> list of values\n",
    "    bucket: Dict[str, Dict[str, List[float]]] = {m: {a: [] for a in algs} for m in metrics}\n",
    "    for r in rows:\n",
    "        if r.metric in bucket:\n",
    "            bucket[r.metric][r.alg].append(float(r.value))\n",
    "\n",
    "    # Helper: bar plot with error bars.\n",
    "    def bar_with_std(metric: str, ylabel: str, filename: str) -> None:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for a in algs:\n",
    "            st = _group_stats(bucket[metric][a])\n",
    "            means.append(st[\"mean\"])\n",
    "            stds.append(st[\"std\"])\n",
    "        xs = np.arange(len(algs))\n",
    "        plt.bar(xs, means, yerr=stds, capsize=4)\n",
    "        plt.xticks(xs, algs, rotation=25, ha=\"right\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, filename), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    bar_with_std(\"robust_UR\", \"Robot utility (worst-case over sensor suite)\", \"exp_bar_robust_UR.png\")\n",
    "    bar_with_std(\"mean_UR\", \"Robot utility (mean over sensor suite)\", \"exp_bar_mean_UR.png\")\n",
    "    bar_with_std(\"robust_goal\", \"Goal rate (worst-case over sensor suite)\", \"exp_bar_robust_goal.png\")\n",
    "    bar_with_std(\"robust_det\", \"Detection rate (worst-case over sensor suite)\", \"exp_bar_robust_det.png\")\n",
    "    bar_with_std(\"runtime_s\", \"Runtime (seconds)\", \"exp_bar_runtime.png\")\n",
    "\n",
    "    # Scatter: robustness vs (robot) regret to show the stability/optimality tradeoff.\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    for a in algs:\n",
    "        xs = bucket[\"max_regret_R\"][a]\n",
    "        ys = bucket[\"robust_UR\"][a]\n",
    "        if xs and ys:\n",
    "            plt.scatter(xs, ys, label=a)\n",
    "    plt.xlabel(\"Max conditional regret (robot)\")\n",
    "    plt.ylabel(\"Robust robot utility\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"exp_scatter_robustUR_vs_regretR.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def run_fixed_games_experiment(\n",
    "    outdir: str = \"results_exp\",\n",
    "    n_games: int = 6,\n",
    "    seeds: Optional[List[int]] = None,\n",
    "    outer_iters: int = 3,\n",
    "    rollouts_payoff: int = 20,\n",
    "    rollouts_br: int = 30,\n",
    "    eval_episodes: int = 120,\n",
    "    episodes_per_sensor: int = 80,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    "    cond_top_k: int = 2,\n",
    "    br_eval_rollouts: int = 8,\n",
    "    add_threshold: float = 0.25,\n",
    "    risk_weight_br: float = 12.0,\n",
    "    include_solvers: Optional[List[str]] = None,\n",
    "    include_baselines: bool = True,\n",
    "    log_level: str = \"QUIET\",\n",
    ") -> None:\n",
    "    \"\"\"Run a clean comparison on a deterministic game set.\n",
    "\n",
    "    What you get in outdir:\n",
    "      - experiments.csv  (all raw numbers)\n",
    "      - exp_bar_*.png    (summary bars)\n",
    "      - exp_scatter_*.png\n",
    "\n",
    "    Recommended presentation story:\n",
    "      1) Robust UR and robust goal rate vs baselines\n",
    "      2) Runtime vs baselines\n",
    "      3) Scatter showing tradeoff: regret (stability) vs robust UR (performance)\n",
    "\n",
    "    NOTE: NBS is not a Nash/CE solver; higher regret is expected. That’s *part of the story*.\n",
    "    \"\"\"\n",
    "\n",
    "    if seeds is None:\n",
    "        seeds = [0, 1, 2]\n",
    "    if include_solvers is None:\n",
    "        include_solvers = [\"correlated\", \"marginal\"]\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    games = build_fixed_game_set(n_games=n_games)\n",
    "\n",
    "    rows: List[ExperimentRow] = []\n",
    "\n",
    "    # Sensor suite for robust evaluation (adversarial-ish)\n",
    "    # You can add Alternate/Random to show generalization, but the key is Mode0/Mode1.\n",
    "    def make_sensor_suite(M_modes: int, seed0: int) -> List[SensorPolicy]:\n",
    "        return [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=seed0 + 999, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "    for g in games:\n",
    "        for s in seeds:\n",
    "            env = GridWorldStealthEnv(g.grid, g.sensor_cfg, fp=0.05, fn=0.10, seed=s)\n",
    "            M_modes = len(g.sensor_cfg.sensors)\n",
    "            sensors_suite = make_sensor_suite(M_modes=M_modes, seed0=s)\n",
    "\n",
    "            # -----------------\n",
    "            # (1) Run solvers\n",
    "            # -----------------\n",
    "            for solver in include_solvers:\n",
    "                # Build an args object compatible with run_training()\n",
    "                args = argparse.Namespace(\n",
    "                    seed=s,\n",
    "                    log_level=log_level,\n",
    "                    solver=solver,\n",
    "                    outer_iters=int(outer_iters),\n",
    "                    rollouts_payoff=int(rollouts_payoff),\n",
    "                    rollouts_br=int(rollouts_br),\n",
    "                    risk_weight_br=float(risk_weight_br),\n",
    "                    disagreement=str(disagreement),\n",
    "                    entropy_tau=float(entropy_tau),\n",
    "                    cond_top_k=int(cond_top_k),\n",
    "                    br_eval_rollouts=int(br_eval_rollouts),\n",
    "                    add_threshold=float(add_threshold),\n",
    "                    eval_episodes=int(eval_episodes),\n",
    "                    debug_rollout_pair=\"\",\n",
    "                    results_dir=\"\",\n",
    "                    save_plots=False,\n",
    "                    run_benchmarks=False,\n",
    "                    bench_episodes=0,\n",
    "                )\n",
    "\n",
    "                t0 = time.time()\n",
    "                log = Logger(log_level)\n",
    "                tr = run_training(env, args, solver=solver, log=log)\n",
    "                runtime_s = float(time.time() - t0)\n",
    "\n",
    "                # Extract sigma_R from final X\n",
    "                sigma_R, _sigma_S = marginals_from_joint(tr.X)\n",
    "                suite_summary = _summarize_against_sensor_suite(\n",
    "                    env,\n",
    "                    robots=tr.robots,\n",
    "                    sigma_R=sigma_R,\n",
    "                    sensors_suite=sensors_suite,\n",
    "                    M_modes=M_modes,\n",
    "                    episodes_per_sensor=int(episodes_per_sensor),\n",
    "                    base_seed=100_000 + 1000 * s,\n",
    "                )\n",
    "\n",
    "                # Also keep solver-internal diagnostics (from last history row)\n",
    "                last = tr.history[-1]\n",
    "\n",
    "                alg = f\"Solver:{solver}\"\n",
    "                for k, v in suite_summary.items():\n",
    "                    rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=k, value=float(v)))\n",
    "\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"runtime_s\", value=runtime_s))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"entropy_X\", value=float(last.entropy_X)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"max_regret_R\", value=float(last.max_regret_R)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"max_regret_S\", value=float(last.max_regret_S)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"selfplay_UR\", value=float(last.selfplay_UR)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"selfplay_US\", value=float(last.selfplay_US)))\n",
    "\n",
    "            # -----------------\n",
    "            # (2) Baseline robots (no training)\n",
    "            # -----------------\n",
    "            if include_baselines:\n",
    "                robots_base, _ = build_initial_policies_for_bench(env, M_modes=M_modes)\n",
    "\n",
    "                for rp in robots_base:\n",
    "                    sigma = np.zeros(len(robots_base), dtype=float)\n",
    "                    sigma[robots_base.index(rp)] = 1.0\n",
    "\n",
    "                    suite_summary = _summarize_against_sensor_suite(\n",
    "                        env,\n",
    "                        robots=robots_base,\n",
    "                        sigma_R=sigma,\n",
    "                        sensors_suite=sensors_suite,\n",
    "                        M_modes=M_modes,\n",
    "                        episodes_per_sensor=int(episodes_per_sensor),\n",
    "                        base_seed=200_000 + 1000 * s,\n",
    "                    )\n",
    "\n",
    "                    alg = f\"Baseline:{rp.name}\"\n",
    "                    for k, v in suite_summary.items():\n",
    "                        rows.append(ExperimentRow(alg=alg, solver=\"baseline\", game_id=g.game_id, seed=s, metric=k, value=float(v)))\n",
    "\n",
    "    # Save raw numbers\n",
    "    csv_path = os.path.join(outdir, \"experiments.csv\")\n",
    "    _write_experiment_csv(rows, csv_path)\n",
    "\n",
    "    # Produce summary plots\n",
    "    plot_experiment_summary(rows, outdir=outdir)\n",
    "\n",
    "    print(f\"[EXP] Done. Wrote: {csv_path}\")\n",
    "    print(f\"[EXP] Plots saved to: {outdir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0898db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Running sanity tests...\n",
      "[TEST] All sanity tests passed ✅\n",
      "[EXP] Done. Wrote: results_exp\\experiments.csv\n",
      "[EXP] Plots saved to: results_exp\n"
     ]
    }
   ],
   "source": [
    "run_sanity_tests()\n",
    "\n",
    "run_fixed_games_experiment(\n",
    "    outdir=\"results_exp\",\n",
    "    n_games=6,\n",
    "    seeds=[0,1,2],\n",
    "    outer_iters=3,\n",
    "    include_solvers=[\"correlated\",\"marginal\"],\n",
    "    include_baselines=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b20adc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"approach2_robust_correlated.py\n",
    "\n",
    "Robust, reduced-log implementation of \"Approach 2\" on a stealth gridworld POMDP,\n",
    "with a FIX for the conceptual mismatch:\n",
    "\n",
    "  Old mismatch: solve a joint distribution x*(i,j) over (robot policy i, sensor policy j)\n",
    "  but then compute best responses against the MARGINALS sigma_R, sigma_S.\n",
    "\n",
    "  Fix here: treat x* as a CORRELATION DEVICE (mediator) and compute BRs against\n",
    "  CONDITIONAL distributions:\n",
    "\n",
    "      q_S(.|i) = X[i,:] / sigma_R[i]   (sensor conditional given robot recommendation i)\n",
    "      q_R(.|j) = X[:,j] / sigma_S[j]   (robot conditional given sensor recommendation j)\n",
    "\n",
    "  Then add the most profitable *deviation* policy (oracle) based on the most violated\n",
    "  conditional recommendation.\n",
    "\n",
    "This makes the PSRO-style expansion step consistent with a correlated-strategy viewpoint.\n",
    "\n",
    "Also included:\n",
    "  - Benchmark harness vs simple heuristics on the same game.\n",
    "  - Saved plots (training curves + bar charts) for presentations.\n",
    "\n",
    "Run (script):\n",
    "  python approach2_robust_correlated.py --solver correlated --outer-iters 3 --save-plots\n",
    "\n",
    "Run (notebook):\n",
    "  main(argv=[\"--solver\",\"both\",\"--outer-iters\",\"3\",\"--save-plots\"])  # ignore ipykernel args\n",
    "\n",
    "Tips to encourage MULTIMODAL x* (useful for your \"mode recovery\"):\n",
    "  --disagreement uniform --entropy-tau 0.02\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import heapq\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Stage-based logger with QUIET/INFO/DEBUG.\"\"\"\n",
    "\n",
    "    LEVELS = {\"QUIET\": 0, \"INFO\": 1, \"DEBUG\": 2}\n",
    "\n",
    "    def __init__(self, level: str = \"INFO\"):\n",
    "        level = level.upper()\n",
    "        if level not in self.LEVELS:\n",
    "            raise ValueError(f\"Unknown log level: {level}. Use QUIET/INFO/DEBUG\")\n",
    "        self.level = level\n",
    "        self.k = self.LEVELS[level]\n",
    "\n",
    "    def banner(self, title: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(\"\" + \"=\" * 100)\n",
    "            print(title)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "    def info(self, msg: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(msg)\n",
    "\n",
    "    def debug(self, msg: str) -> None:\n",
    "        if self.k >= 2:\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Types\n",
    "# =============================================================================\n",
    "\n",
    "Action = Tuple[int, int]  # (dx, dy)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Environment (grid + hidden sensor mode + noisy alarm observation)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GridConfig:\n",
    "    width: int\n",
    "    height: int\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    obstacles: frozenset\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SensorConfig:\n",
    "    sensors: Tuple[Tuple[int, int], ...]\n",
    "    radius: int\n",
    "    base_p: float\n",
    "    hotspot_p: float\n",
    "\n",
    "\n",
    "class GridWorldStealthEnv:\n",
    "    \"\"\"Grid world with detection risk controlled by a hidden/selected sensor 'mode'.\"\"\"\n",
    "\n",
    "    def __init__(self, grid: GridConfig, sensor_cfg: SensorConfig, fp: float = 0.05, fn: float = 0.10, seed: int = 0):\n",
    "        self.grid = grid\n",
    "        self.sensor_cfg = sensor_cfg\n",
    "        self.fp = float(fp)\n",
    "        self.fn = float(fn)\n",
    "\n",
    "        if not (0.0 <= self.fp <= 1.0 and 0.0 <= self.fn <= 1.0):\n",
    "            raise ValueError(\"fp and fn must be in [0,1].\")\n",
    "        if not (0.0 <= sensor_cfg.base_p <= 1.0 and 0.0 <= sensor_cfg.hotspot_p <= 1.0):\n",
    "            raise ValueError(\"base_p and hotspot_p must be in [0,1].\")\n",
    "        if sensor_cfg.radius < 0:\n",
    "            raise ValueError(\"radius must be >= 0\")\n",
    "        if len(sensor_cfg.sensors) == 0:\n",
    "            raise ValueError(\"Need at least one sensor center.\")\n",
    "\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.reset(sensor_mode=0)\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    def reset(self, sensor_mode: int = 0) -> Dict[str, Any]:\n",
    "        self.t = 0\n",
    "        self.pos = self.grid.start\n",
    "        self.sensor_mode = int(sensor_mode)\n",
    "        self.detected = False\n",
    "        self.total_true_risk = 0.0\n",
    "        return {\"pos\": self.pos, \"t\": self.t}\n",
    "\n",
    "    def in_bounds(self, p: Tuple[int, int]) -> bool:\n",
    "        x, y = p\n",
    "        return 0 <= x < self.grid.width and 0 <= y < self.grid.height\n",
    "\n",
    "    def is_free(self, p: Tuple[int, int]) -> bool:\n",
    "        return self.in_bounds(p) and (p not in self.grid.obstacles)\n",
    "\n",
    "    def true_detection_prob(self, p: Tuple[int, int], mode: int) -> float:\n",
    "        if not (0 <= mode < len(self.sensor_cfg.sensors)):\n",
    "            raise ValueError(f\"mode {mode} out of range\")\n",
    "        sx, sy = self.sensor_cfg.sensors[mode]\n",
    "        x, y = p\n",
    "        d = abs(x - sx) + abs(y - sy)\n",
    "        return self.sensor_cfg.hotspot_p if d <= self.sensor_cfg.radius else self.sensor_cfg.base_p\n",
    "\n",
    "    def observation_prob(self, alarm: int, p_true: float) -> float:\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        if alarm == 1:\n",
    "            return p_alarm\n",
    "        if alarm == 0:\n",
    "            return 1.0 - p_alarm\n",
    "        raise ValueError(\"alarm must be 0 or 1\")\n",
    "\n",
    "    def step(self, a: Action) -> Dict[str, Any]:\n",
    "        if self.detected:\n",
    "            return {\"pos\": self.pos, \"t\": self.t, \"alarm\": 1, \"p_true\": 1.0, \"detected\": True, \"done\": True}\n",
    "\n",
    "        self.t += 1\n",
    "        nx = self.pos[0] + int(a[0])\n",
    "        ny = self.pos[1] + int(a[1])\n",
    "        np_ = (nx, ny)\n",
    "        if self.is_free(np_):\n",
    "            self.pos = np_\n",
    "\n",
    "        p_true = float(self.true_detection_prob(self.pos, self.sensor_mode))\n",
    "        self.total_true_risk += p_true\n",
    "\n",
    "        if self.rng.random() < p_true:\n",
    "            self.detected = True\n",
    "\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        alarm = 1 if (self.rng.random() < p_alarm) else 0\n",
    "\n",
    "        done = self.detected or (self.pos == self.grid.goal) or (self.t >= 200)\n",
    "        return {\"pos\": self.pos, \"t\": self.t, \"alarm\": alarm, \"p_true\": p_true, \"detected\": self.detected, \"done\": done}\n",
    "\n",
    "\n",
    "def build_two_corridor_grid(width: int = 15, height: int = 9) -> GridConfig:\n",
    "    obstacles = set()\n",
    "    wall_x = width // 2\n",
    "    gap_ys = {2, 6}\n",
    "    for y in range(height):\n",
    "        if y not in gap_ys:\n",
    "            obstacles.add((wall_x, y))\n",
    "\n",
    "    start = (1, height - 2)\n",
    "    goal = (width - 2, 1)\n",
    "    if start in obstacles or goal in obstacles:\n",
    "        raise RuntimeError(\"Start/goal blocked unexpectedly\")\n",
    "\n",
    "    return GridConfig(width=width, height=height, start=start, goal=goal, obstacles=frozenset(obstacles))\n",
    "\n",
    "\n",
    "def print_grid_ascii(grid: GridConfig, sensor_cfg: SensorConfig) -> None:\n",
    "    W, H = grid.width, grid.height\n",
    "    obs = set(grid.obstacles)\n",
    "    sens = set(sensor_cfg.sensors)\n",
    "    for y in range(H):\n",
    "        row = []\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p == grid.start:\n",
    "                row.append(\"R\")\n",
    "            elif p == grid.goal:\n",
    "                row.append(\"G\")\n",
    "            elif p in sens:\n",
    "                row.append(\"S\")\n",
    "            elif p in obs:\n",
    "                row.append(\"#\")\n",
    "            else:\n",
    "                row.append(\".\")\n",
    "        print(\"\".join(row))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Belief over modes\n",
    "# =============================================================================\n",
    "\n",
    "class ModeBelief:\n",
    "    \"\"\"Exact belief over discrete modes m in {0..M-1}.\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, init: Optional[np.ndarray] = None):\n",
    "        self.M = int(M)\n",
    "        if self.M <= 0:\n",
    "            raise ValueError(\"M must be >= 1\")\n",
    "        if init is None:\n",
    "            self.b = np.full(self.M, 1.0 / self.M)\n",
    "        else:\n",
    "            init = np.asarray(init, dtype=float).reshape(-1)\n",
    "            if init.shape != (self.M,):\n",
    "                raise ValueError(\"init shape mismatch\")\n",
    "            if np.any(init < 0):\n",
    "                raise ValueError(\"init must be nonnegative\")\n",
    "            s = float(init.sum())\n",
    "            self.b = init / s if s > 0 else np.full(self.M, 1.0 / self.M)\n",
    "\n",
    "    def update(self, env: GridWorldStealthEnv, alarm: int, pos: Tuple[int, int], eps: float = 1e-12) -> None:\n",
    "        like = np.zeros(self.M, dtype=float)\n",
    "        for m in range(self.M):\n",
    "            p_true = env.true_detection_prob(pos, m)\n",
    "            like[m] = env.observation_prob(alarm, p_true)\n",
    "        post = self.b * like\n",
    "        Z = float(post.sum())\n",
    "        if (not np.isfinite(Z)) or Z < eps:\n",
    "            return\n",
    "        self.b = post / Z\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policies\n",
    "# =============================================================================\n",
    "\n",
    "class RobotPolicy:\n",
    "    name: str = \"RobotPolicy\"\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedPathPolicy(RobotPolicy):\n",
    "    def __init__(self, path: List[Tuple[int, int]], name: str):\n",
    "        if len(path) < 2:\n",
    "            raise ValueError(\"Path must have >=2 states\")\n",
    "        self.path = list(path)\n",
    "        self.name = str(name)\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        try:\n",
    "            self._idx = self.path.index(start_pos)\n",
    "        except ValueError:\n",
    "            self._idx = 0\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        cur = env.pos\n",
    "        if self._idx >= len(self.path) - 1:\n",
    "            return (0, 0)\n",
    "        if cur != self.path[self._idx]:\n",
    "            try:\n",
    "                self._idx = self.path.index(cur, self._idx)\n",
    "            except ValueError:\n",
    "                return (0, 0)\n",
    "        nxt = self.path[self._idx + 1]\n",
    "        dx = int(np.clip(nxt[0] - cur[0], -1, 1))\n",
    "        dy = int(np.clip(nxt[1] - cur[1], -1, 1))\n",
    "        self._idx += 1\n",
    "        return (dx, dy)\n",
    "\n",
    "\n",
    "class RandomPolicy(RobotPolicy):\n",
    "    \"\"\"Reproducible random policy using env.rng.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"R_Random\"):\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        candidates: List[Action] = []\n",
    "        x, y = env.pos\n",
    "        for a in [(1, 0), (-1, 0), (0, 1), (0, -1), (0, 0)]:\n",
    "            np_ = (x + a[0], y + a[1])\n",
    "            if env.is_free(np_):\n",
    "                candidates.append(a)\n",
    "        if not candidates:\n",
    "            return (0, 0)\n",
    "        return candidates[int(env.rng.integers(0, len(candidates)))]\n",
    "\n",
    "\n",
    "class OnlineBeliefReplanPolicy(RobotPolicy):\n",
    "    \"\"\"POMDP-ish heuristic: replan each step using risk map induced by current belief b_t.\"\"\"\n",
    "\n",
    "    def __init__(self, env: GridWorldStealthEnv, risk_weight: float = 12.0, name: str = \"R_OnlineBeliefReplan\"):\n",
    "        self.env = env\n",
    "        self.risk_weight = float(risk_weight)\n",
    "        self.name = name\n",
    "        self._cached_next: Optional[Tuple[int, int]] = None\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        self._cached_next = None\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        # Build a risk map from belief over modes.\n",
    "        mode_probs = belief.b\n",
    "\n",
    "        def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "            x, y = to\n",
    "            # expected risk at to under belief\n",
    "            r = 0.0\n",
    "            for m, pm in enumerate(mode_probs):\n",
    "                r += float(pm) * float(env.true_detection_prob(to, m))\n",
    "            return 1.0 + self.risk_weight * r\n",
    "\n",
    "        # Plan from current pos to goal (one-step receding horizon)\n",
    "        try:\n",
    "            path = astar_path(env.grid, env.pos, env.grid.goal, step_cost)\n",
    "            if len(path) < 2:\n",
    "                return (0, 0)\n",
    "            nxt = path[1]\n",
    "            dx = int(np.clip(nxt[0] - env.pos[0], -1, 1))\n",
    "            dy = int(np.clip(nxt[1] - env.pos[1], -1, 1))\n",
    "            return (dx, dy)\n",
    "        except Exception:\n",
    "            return (0, 0)\n",
    "\n",
    "\n",
    "class SensorPolicy:\n",
    "    name: str = \"SensorPolicy\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedModeSensorPolicy(SensorPolicy):\n",
    "    def __init__(self, mode: int, name: Optional[str] = None):\n",
    "        self.mode = int(mode)\n",
    "        self.name = name or f\"S_Mode{mode}\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return self.mode\n",
    "\n",
    "\n",
    "class AlternatingSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Simple sensor heuristic for benchmarks: alternate modes 0,1,0,1,...\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, name: str = \"S_Alternate\"):\n",
    "        self.M = int(M)\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(t % self.M)\n",
    "\n",
    "\n",
    "class RandomModeSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Benchmark sensor: random mode each step (uses numpy Generator for reproducibility).\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, seed: int = 0, name: str = \"S_RandomMode\"):\n",
    "        self.M = int(M)\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(self.rng.integers(0, self.M))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# A* (used for planning)\n",
    "# =============================================================================\n",
    "\n",
    "def astar_path(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    max_expansions: int = 250_000,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    if start == goal:\n",
    "        return [start]\n",
    "\n",
    "    def h(p: Tuple[int, int]) -> float:\n",
    "        return abs(p[0] - goal[0]) + abs(p[1] - goal[1])\n",
    "\n",
    "    def neighbors(p: Tuple[int, int]):\n",
    "        x, y = p\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            np_ = (x + dx, y + dy)\n",
    "            if 0 <= np_[0] < grid.width and 0 <= np_[1] < grid.height and np_ not in grid.obstacles:\n",
    "                yield np_\n",
    "\n",
    "    open_heap: List[Tuple[float, float, Tuple[int, int]]] = []\n",
    "    heapq.heappush(open_heap, (h(start), 0.0, start))\n",
    "\n",
    "    came: Dict[Tuple[int, int], Optional[Tuple[int, int]]] = {start: None}\n",
    "    gscore: Dict[Tuple[int, int], float] = {start: 0.0}\n",
    "\n",
    "    expansions = 0\n",
    "    while open_heap:\n",
    "        _, _, cur = heapq.heappop(open_heap)\n",
    "        expansions += 1\n",
    "        if cur == goal:\n",
    "            path: List[Tuple[int, int]] = []\n",
    "            while cur is not None:\n",
    "                path.append(cur)\n",
    "                cur = came[cur]\n",
    "            path.reverse()\n",
    "            return path\n",
    "        if expansions > max_expansions:\n",
    "            raise RuntimeError(\"A* exceeded max expansions\")\n",
    "\n",
    "        for nb in neighbors(cur):\n",
    "            tentative = gscore[cur] + float(step_cost(cur, nb))\n",
    "            if (nb not in gscore) or (tentative < gscore[nb] - 1e-12):\n",
    "                gscore[nb] = tentative\n",
    "                came[nb] = cur\n",
    "                heapq.heappush(open_heap, (tentative + h(nb), tentative, nb))\n",
    "\n",
    "    raise RuntimeError(\"A* failed: unreachable goal\")\n",
    "\n",
    "\n",
    "def astar_via_waypoint(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    waypoint: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    ") -> List[Tuple[int, int]]:\n",
    "    p1 = astar_path(grid, start, waypoint, step_cost)\n",
    "    p2 = astar_path(grid, waypoint, goal, step_cost)\n",
    "    return p1[:-1] + p2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rollouts + payoffs\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStats:\n",
    "    steps: int\n",
    "    reached_goal: bool\n",
    "    detected: bool\n",
    "    total_true_risk: float\n",
    "    U_R: float\n",
    "    U_S: float\n",
    "\n",
    "\n",
    "def rollout_episode(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    seed: int,\n",
    "    max_steps: int = 200,\n",
    "    lambda_risk: float = 1.0,\n",
    "    det_penalty: float = 50.0,\n",
    "    sensor_energy_per_step: float = 0.2,\n",
    "    step_debug: bool = False,\n",
    ") -> EpisodeStats:\n",
    "    env.seed(seed)\n",
    "    sensor.reset()\n",
    "    env.reset(sensor_mode=sensor.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)\n",
    "    robot.reset(env.pos)\n",
    "\n",
    "    total_risk = 0.0\n",
    "    last_alarm: Optional[int] = None\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.sensor_mode = sensor.select_mode(env.t)\n",
    "        a = robot.act(env, belief, last_alarm)\n",
    "        out = env.step(a)\n",
    "\n",
    "        total_risk += float(out[\"p_true\"])\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "        last_alarm = int(out[\"alarm\"])\n",
    "\n",
    "        if step_debug:\n",
    "            print(\n",
    "                f\"[Step] t={out['t']:3d} pos={out['pos']} a={a} p_true={out['p_true']:.3f} \"\n",
    "                f\"alarm={out['alarm']} det={out['detected']} done={out['done']} b={belief.b.round(3)}\"\n",
    "            )\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = bool(env.detected)\n",
    "    steps = int(env.t)\n",
    "\n",
    "    cost_R = steps + lambda_risk * total_risk + (det_penalty if detected else 0.0)\n",
    "    U_R = -float(cost_R)\n",
    "\n",
    "    U_S = float((det_penalty if detected else 0.0) + lambda_risk * total_risk - sensor_energy_per_step * steps)\n",
    "\n",
    "    return EpisodeStats(steps=steps, reached_goal=reached_goal, detected=detected, total_true_risk=float(total_risk), U_R=U_R, U_S=U_S)\n",
    "\n",
    "\n",
    "def evaluate_payoffs(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    debug_rollout_pair: Optional[Tuple[int, int]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[int, int], Dict[str, float]]]:\n",
    "    m, n = len(robots), len(sensors)\n",
    "    U_R = np.zeros((m, n), dtype=float)\n",
    "    U_S = np.zeros((m, n), dtype=float)\n",
    "    diag: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    log.info(f\"[Eval] Estimating payoffs: m={m}, n={n}, rollouts={rollouts}, base_seed={base_seed}\")\n",
    "\n",
    "    for i, rpol in enumerate(robots):\n",
    "        for j, spol in enumerate(sensors):\n",
    "            step_debug = (debug_rollout_pair == (i, j))\n",
    "\n",
    "            r_list: List[float] = []\n",
    "            s_list: List[float] = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            steps_list: List[int] = []\n",
    "            risk_list: List[float] = []\n",
    "\n",
    "            for k in range(rollouts):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rpol, spol, M_modes=M_modes, seed=seed, step_debug=step_debug)\n",
    "                r_list.append(st.U_R)\n",
    "                s_list.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "                steps_list.append(st.steps)\n",
    "                risk_list.append(st.total_true_risk)\n",
    "\n",
    "                if step_debug:\n",
    "                    step_debug = False  # only show one rollout\n",
    "\n",
    "            U_R[i, j] = float(np.mean(r_list))\n",
    "            U_S[i, j] = float(np.mean(s_list))\n",
    "\n",
    "            diag[(i, j)] = {\n",
    "                \"det_rate\": det / rollouts,\n",
    "                \"goal_rate\": goal / rollouts,\n",
    "                \"mean_steps\": float(np.mean(steps_list)),\n",
    "                \"mean_risk\": float(np.mean(risk_list)),\n",
    "                \"std_UR\": float(np.std(r_list)),\n",
    "                \"std_US\": float(np.std(s_list)),\n",
    "            }\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(\"[Eval] Compact payoff summary:\")\n",
    "        for i, rpol in enumerate(robots):\n",
    "            for j, spol in enumerate(sensors):\n",
    "                d = diag[(i, j)]\n",
    "                log.info(\n",
    "                    f\"  (R{i}:{rpol.name}, S{j}:{spol.name}) \"\n",
    "                    f\"UR={U_R[i,j]:8.3f}±{d['std_UR']:.2f} | \"\n",
    "                    f\"US={U_S[i,j]:8.3f}±{d['std_US']:.2f} | \"\n",
    "                    f\"det%={100*d['det_rate']:5.1f} goal%={100*d['goal_rate']:5.1f} \"\n",
    "                    f\"steps={d['mean_steps']:.1f} risk={d['mean_risk']:.2f}\"\n",
    "                )\n",
    "\n",
    "    return U_R, U_S, diag\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NBS solver (with optional entropy regularization)\n",
    "# =============================================================================\n",
    "\n",
    "def project_simplex(v: np.ndarray, z: float = 1.0) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    if v.size == 0:\n",
    "        raise ValueError(\"Empty vector\")\n",
    "    if z <= 0:\n",
    "        raise ValueError(\"z must be > 0\")\n",
    "\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, v.size + 1) > (cssv - z))[0]\n",
    "    if rho.size == 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    rho = int(rho[-1])\n",
    "    theta = (cssv[rho] - z) / (rho + 1.0)\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    s = float(w.sum())\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    return w * (z / s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NBSResult:\n",
    "    x: np.ndarray\n",
    "    obj: float\n",
    "    gains: Tuple[float, float]\n",
    "    support: int\n",
    "\n",
    "\n",
    "def solve_nbs(\n",
    "    uR: np.ndarray,\n",
    "    uS: np.ndarray,\n",
    "    log: Logger,\n",
    "    max_iters: int = 400,\n",
    "    alpha: float = 0.5,\n",
    "    tol_l1: float = 1e-6,\n",
    "    kappa: float = 1e-6,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    ") -> NBSResult:\n",
    "    uR = np.asarray(uR, dtype=float).reshape(-1)\n",
    "    uS = np.asarray(uS, dtype=float).reshape(-1)\n",
    "    if uR.shape != uS.shape:\n",
    "        raise ValueError(\"uR and uS must have same shape\")\n",
    "    d = uR.size\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Need >=2 joint actions\")\n",
    "\n",
    "    unif = np.full(d, 1.0 / d)\n",
    "\n",
    "    disagreement = disagreement.lower().strip()\n",
    "    if disagreement == \"minminus\":\n",
    "        dR = float(np.min(uR) - 1.0)\n",
    "        dS = float(np.min(uS) - 1.0)\n",
    "    elif disagreement == \"uniform\":\n",
    "        dR = float(uR @ unif)\n",
    "        dS = float(uS @ unif)\n",
    "    else:\n",
    "        raise ValueError(\"disagreement must be 'minminus' or 'uniform'\")\n",
    "\n",
    "    x = unif.copy()\n",
    "\n",
    "    def gains(xv: np.ndarray) -> Tuple[float, float]:\n",
    "        return float(uR @ xv - dR), float(uS @ xv - dS)\n",
    "\n",
    "    def entropy(xv: np.ndarray) -> float:\n",
    "        xx = np.clip(xv, 1e-12, 1.0)\n",
    "        return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "    def obj(xv: np.ndarray) -> float:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return float(np.log(gR) + np.log(gS) + entropy_tau * entropy(xv))\n",
    "\n",
    "    def grad(xv: np.ndarray) -> np.ndarray:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        g = (uR / gR) + (uS / gS)\n",
    "        if entropy_tau > 0:\n",
    "            xx = np.clip(xv, 1e-12, 1.0)\n",
    "            g += entropy_tau * (-(np.log(xx) + 1.0))\n",
    "        return g\n",
    "\n",
    "    last = obj(x)\n",
    "    log.info(f\"[NBS] d={d} disagreement=({dR:.3f},{dS:.3f}) entropy_tau={entropy_tau:.3g}\")\n",
    "\n",
    "    for t in range(1, max_iters + 1):\n",
    "        g = grad(x)\n",
    "        a = alpha\n",
    "        improved = False\n",
    "        for _ in range(30):\n",
    "            x_new = project_simplex(x + a * g)\n",
    "            new_obj = obj(x_new)\n",
    "            if new_obj >= last - 1e-12:\n",
    "                improved = True\n",
    "                break\n",
    "            a *= 0.5\n",
    "            if a < 1e-6:\n",
    "                break\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "        delta = float(np.linalg.norm(x_new - x, ord=1))\n",
    "        x = x_new\n",
    "        last = new_obj\n",
    "\n",
    "        if log.k >= 2 and (t <= 5 or t % 25 == 0):\n",
    "            gR, gS = gains(x)\n",
    "            top = np.argsort(-x)[:5]\n",
    "            top_str = \", \".join([f\"{i}:{x[i]:.3f}\" for i in top])\n",
    "            log.debug(f\"[NBS][it={t:3d}] obj={last:.6f} gains=({gR:.3f},{gS:.3f}) L1={delta:.2e} top={top_str}\")\n",
    "\n",
    "        if delta < tol_l1:\n",
    "            break\n",
    "\n",
    "    gR, gS = gains(x)\n",
    "    support = int(np.sum(x > 1e-6))\n",
    "    log.info(f\"[NBS] done: obj={last:.6f} gains=({gR:.3f},{gS:.3f}) support={support}/{d}\")\n",
    "\n",
    "    return NBSResult(x=x, obj=float(last), gains=(float(gR), float(gS)), support=support)\n",
    "\n",
    "\n",
    "def joint_to_matrix(x: np.ndarray, m: int, n: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size != m * n:\n",
    "        raise ValueError(\"x size mismatch\")\n",
    "    X = x.reshape((m, n))\n",
    "    s = float(X.sum())\n",
    "    if not np.isfinite(s) or abs(s - 1.0) > 1e-6:\n",
    "        # Renormalize defensively\n",
    "        X = X / max(s, 1e-12)\n",
    "    return X\n",
    "\n",
    "\n",
    "def marginals_from_joint(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    sigma_R = X.sum(axis=1)\n",
    "    sigma_S = X.sum(axis=0)\n",
    "    if sigma_R.sum() > 0:\n",
    "        sigma_R = sigma_R / sigma_R.sum()\n",
    "    if sigma_S.sum() > 0:\n",
    "        sigma_S = sigma_S / sigma_S.sum()\n",
    "    return sigma_R, sigma_S\n",
    "\n",
    "\n",
    "def entropy_of_joint(X: np.ndarray) -> float:\n",
    "    xx = np.clip(X.reshape(-1), 1e-12, 1.0)\n",
    "    return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Best responses (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_expected_risk_map_from_policy_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi: np.ndarray,\n",
    "    M_modes: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Expected p_true(cell) under mixture over sensor POLICIES (not modes).\n",
    "\n",
    "    For each sensor policy j, we use its mode at t=0 as its defining mode.\n",
    "    (This matches FixedModeSensorPolicy exactly.)\n",
    "    \"\"\"\n",
    "    pi = np.asarray(pi, dtype=float).reshape(-1)\n",
    "    if pi.size != len(sensors):\n",
    "        raise ValueError(\"mixture length mismatch\")\n",
    "\n",
    "    mode_probs = np.zeros(M_modes, dtype=float)\n",
    "    for j, sp in enumerate(sensors):\n",
    "        m = int(sp.select_mode(0))\n",
    "        if not (0 <= m < M_modes):\n",
    "            raise ValueError(\"invalid sensor mode\")\n",
    "        mode_probs[m] += float(pi[j])\n",
    "    if mode_probs.sum() > 0:\n",
    "        mode_probs = mode_probs / mode_probs.sum()\n",
    "\n",
    "    H, W = env.grid.height, env.grid.width\n",
    "    risk = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in env.grid.obstacles:\n",
    "                risk[y, x] = np.nan\n",
    "                continue\n",
    "            val = 0.0\n",
    "            for m in range(M_modes):\n",
    "                val += float(mode_probs[m]) * float(env.true_detection_prob((x, y), m))\n",
    "            risk[y, x] = float(val)\n",
    "\n",
    "    return risk\n",
    "\n",
    "\n",
    "def plan_risk_weighted_path(env: GridWorldStealthEnv, risk_map: np.ndarray, risk_weight: float) -> List[Tuple[int, int]]:\n",
    "    def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "        x, y = to\n",
    "        r = risk_map[y, x]\n",
    "        if not np.isfinite(r):\n",
    "            return 1e9\n",
    "        return 1.0 + float(risk_weight) * float(r)\n",
    "\n",
    "    return astar_path(env.grid, env.grid.start, env.grid.goal, step_cost)\n",
    "\n",
    "\n",
    "def robot_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi_S: np.ndarray,\n",
    "    robots: List[RobotPolicy],\n",
    "    M_modes: int,\n",
    "    risk_weight: float,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> RobotPolicy:\n",
    "    risk = compute_expected_risk_map_from_policy_mixture(env, sensors, pi_S, M_modes=M_modes)\n",
    "    try:\n",
    "        path = plan_risk_weighted_path(env, risk, risk_weight=risk_weight)\n",
    "    except Exception as e:\n",
    "        log.info(f\"[RobotBR] WARNING A* failed ({tag}): {e}\")\n",
    "        return robots[0]\n",
    "\n",
    "    path_tuple = tuple(path)\n",
    "    for p in robots:\n",
    "        if isinstance(p, FixedPathPolicy) and tuple(p.path) == path_tuple:\n",
    "            log.info(f\"[RobotBR] ({tag}) BR path already exists: {p.name}\")\n",
    "            return p\n",
    "\n",
    "    newp = FixedPathPolicy(path, name=f\"R_BR_{tag}_w{risk_weight:.1f}_len{len(path)}\")\n",
    "    log.info(f\"[RobotBR] ({tag}) Added new robot policy: {newp.name}\")\n",
    "    return newp\n",
    "\n",
    "\n",
    "def sensor_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    pi_R: np.ndarray,\n",
    "    candidate_modes: List[int],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> FixedModeSensorPolicy:\n",
    "    \"\"\"Sensor best response with common-random-numbers (CRN).\n",
    "\n",
    "    Why CRN matters: payoff variance is large (detection is a rare/threshold event).\n",
    "    If each candidate mode is evaluated on different random rollouts, you can pick\n",
    "    the wrong 'best mode' by noise, which then breaks the PSRO expansion logic.\n",
    "\n",
    "    Fix: reuse the same sampled robot indices AND the same episode seeds across all\n",
    "    candidate modes.\n",
    "    \"\"\"\n",
    "    pi_R = np.asarray(pi_R, dtype=float).reshape(-1)\n",
    "    if pi_R.size != len(robots):\n",
    "        raise ValueError(\"pi_R length mismatch\")\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    # Same robot-index samples for every mode\n",
    "    robot_idxs = rng.choice(len(robots), size=rollouts, p=pi_R, replace=True)\n",
    "\n",
    "    # Same episode seeds for every mode (common random numbers)\n",
    "    seeds = (int(base_seed) + np.arange(rollouts)).astype(int)\n",
    "\n",
    "    best_mode: Optional[int] = None\n",
    "    best_val = -1e18\n",
    "\n",
    "    for mode in candidate_modes:\n",
    "        if not (0 <= mode < M_modes):\n",
    "            continue\n",
    "        sp = FixedModeSensorPolicy(mode, name=f\"S_BR_{tag}_Mode{mode}\")\n",
    "\n",
    "        vals: List[float] = []\n",
    "        for k in range(rollouts):\n",
    "            rp = robots[int(robot_idxs[k])]\n",
    "            st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=int(seeds[k]))\n",
    "            vals.append(st.U_S)\n",
    "\n",
    "        mean_u = float(np.mean(vals))\n",
    "        if log.k >= 2:\n",
    "            log.debug(f\"[SensorBR] ({tag}) mode={mode} E[US]={mean_u:.3f} std={float(np.std(vals)):.2f}\")\n",
    "\n",
    "        if mean_u > best_val:\n",
    "            best_val = mean_u\n",
    "            best_mode = mode\n",
    "\n",
    "    if best_mode is None:\n",
    "        raise RuntimeError(\"No valid sensor BR mode found\")\n",
    "\n",
    "    log.info(f\"[SensorBR] ({tag}) Best mode={best_mode} E[US]={best_val:.3f}\")\n",
    "    return FixedModeSensorPolicy(best_mode, name=f\"S_BR_{tag}_Mode{best_mode}\")\n",
    "\n",
    "\n",
    "def conditional_sensor_given_robot(X: np.ndarray, i: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    row = np.asarray(X[i, :], dtype=float)\n",
    "    s = float(row.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(row, 1.0 / row.size)\n",
    "    return row / s\n",
    "\n",
    "\n",
    "def conditional_robot_given_sensor(X: np.ndarray, j: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    col = np.asarray(X[:, j], dtype=float)\n",
    "    s = float(col.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(col, 1.0 / col.size)\n",
    "    return col / s\n",
    "\n",
    "\n",
    "def compute_ce_regrets(U_R: np.ndarray, U_S: np.ndarray, X: np.ndarray, eps: float = 1e-12) -> Dict[str, float]:\n",
    "    \"\"\"Conditional recommendation regrets (CE-style) computed on current meta-game.\n",
    "\n",
    "    For robot (given recommendation i):\n",
    "        regret_R(i) = max_{i'} E_{j~q(.|i)}[U_R(i',j) - U_R(i,j)]\n",
    "\n",
    "    For sensor (given recommendation j):\n",
    "        regret_S(j) = max_{j'} E_{i~q(.|j)}[U_S(i,j') - U_S(i,j)]\n",
    "\n",
    "    Returns max and average regrets.\n",
    "    \"\"\"\n",
    "    m, n = U_R.shape\n",
    "    assert U_S.shape == (m, n)\n",
    "    assert X.shape == (m, n)\n",
    "\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "    reg_R = []\n",
    "    for i in range(m):\n",
    "        if sigma_R[i] <= eps:\n",
    "            continue\n",
    "        q = conditional_sensor_given_robot(X, i, eps=eps)\n",
    "        rec = float(np.dot(q, U_R[i, :]))\n",
    "        best = rec\n",
    "        for ip in range(m):\n",
    "            val = float(np.dot(q, U_R[ip, :]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_R.append(best - rec)\n",
    "\n",
    "    reg_S = []\n",
    "    for j in range(n):\n",
    "        if sigma_S[j] <= eps:\n",
    "            continue\n",
    "        q = conditional_robot_given_sensor(X, j, eps=eps)\n",
    "        rec = float(np.dot(q, U_S[:, j]))\n",
    "        best = rec\n",
    "        for jp in range(n):\n",
    "            val = float(np.dot(q, U_S[:, jp]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_S.append(best - rec)\n",
    "\n",
    "    return {\n",
    "        \"max_regret_R\": float(max(reg_R) if reg_R else 0.0),\n",
    "        \"max_regret_S\": float(max(reg_S) if reg_S else 0.0),\n",
    "        \"mean_regret_R\": float(np.mean(reg_R) if reg_R else 0.0),\n",
    "        \"mean_regret_S\": float(np.mean(reg_S) if reg_S else 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def find_sensor_by_mode(pols: List[SensorPolicy], mode: int) -> Optional[FixedModeSensorPolicy]:\n",
    "    for p in pols:\n",
    "        if isinstance(p, FixedModeSensorPolicy) and p.mode == mode:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policy initialization + evaluator for joint strategy\n",
    "# =============================================================================\n",
    "\n",
    "def build_initial_policies(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "    ]\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StrategyEval:\n",
    "    mean_U_R: float\n",
    "    mean_U_S: float\n",
    "    det_rate: float\n",
    "    goal_rate: float\n",
    "    mean_steps: float\n",
    "    mean_risk: float\n",
    "\n",
    "\n",
    "def evaluate_joint_strategy(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    X: np.ndarray,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    m, n = X.shape\n",
    "    probs = X.reshape(-1)\n",
    "    probs = probs / max(float(probs.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR = []\n",
    "    US = []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        idx = int(rng.choice(m * n, p=probs))\n",
    "        i, j = np.unravel_index(idx, (m, n))\n",
    "        seed = base_seed + k\n",
    "        st = rollout_episode(env, robots[i], sensors[j], M_modes=M_modes, seed=seed)\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training loop (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainHistoryRow:\n",
    "    outer_iter: int\n",
    "    m: int\n",
    "    n: int\n",
    "    nbs_obj: float\n",
    "    entropy_X: float\n",
    "    max_regret_R: float\n",
    "    max_regret_S: float\n",
    "    selfplay_UR: float\n",
    "    selfplay_US: float\n",
    "    selfplay_det: float\n",
    "    selfplay_goal: float\n",
    "    seconds: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    solver: str\n",
    "    env: GridWorldStealthEnv\n",
    "    robots: List[RobotPolicy]\n",
    "    sensors: List[SensorPolicy]\n",
    "    X: np.ndarray\n",
    "    history: List[TrainHistoryRow]\n",
    "\n",
    "\n",
    "def run_training(env: GridWorldStealthEnv, args: argparse.Namespace, solver: str, log: Logger) -> TrainResult:\n",
    "    t0_all = time.time()\n",
    "\n",
    "    grid = env.grid\n",
    "    sensor_cfg = env.sensor_cfg\n",
    "    M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "    robots, sensors = build_initial_policies(env, M_modes=M_modes)\n",
    "\n",
    "    # Optional: reduce initial set if you want smaller games.\n",
    "    # (We keep it as-is for benchmarks.)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(f\"[{solver}] Initial robots: \" + \", \".join([p.name for p in robots]))\n",
    "        log.info(f\"[{solver}] Initial sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "\n",
    "    debug_pair = None\n",
    "    if args.debug_rollout_pair:\n",
    "        parts = args.debug_rollout_pair.split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            debug_pair = (int(parts[0]), int(parts[1]))\n",
    "            log.info(f\"[{solver}] Will print one step-by-step rollout for pair {debug_pair} (only once).\")\n",
    "\n",
    "    history: List[TrainHistoryRow] = []\n",
    "    X = None\n",
    "\n",
    "    for it in range(1, args.outer_iters + 1):\n",
    "        t0 = time.time()\n",
    "        log.banner(f\"[{solver}] Outer iter {it}/{args.outer_iters}\")\n",
    "\n",
    "        U_R, U_S, _diag = evaluate_payoffs(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            M_modes=M_modes,\n",
    "            rollouts=args.rollouts_payoff,\n",
    "            base_seed=1000 + 100 * it,\n",
    "            log=log,\n",
    "            debug_rollout_pair=debug_pair,\n",
    "        )\n",
    "        debug_pair = None\n",
    "\n",
    "        # Solve NBS over joint actions\n",
    "        uR = U_R.reshape(-1)\n",
    "        uS = U_S.reshape(-1)\n",
    "\n",
    "        nbs = solve_nbs(\n",
    "            uR,\n",
    "            uS,\n",
    "            log=log,\n",
    "            disagreement=args.disagreement,\n",
    "            entropy_tau=args.entropy_tau,\n",
    "        )\n",
    "\n",
    "        m, n = U_R.shape\n",
    "        X = joint_to_matrix(nbs.x, m, n)\n",
    "        sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "        # Print top joint actions\n",
    "        top = np.argsort(-X.reshape(-1))[:min(5, X.size)]\n",
    "        log.info(f\"[{solver}] Top joint actions:\")\n",
    "        for k, idx in enumerate(top, start=1):\n",
    "            i, j = np.unravel_index(int(idx), (m, n))\n",
    "            log.info(f\"  #{k}: (R{i}:{robots[i].name}, S{j}:{sensors[j].name}) prob={X[i,j]:.4f}\")\n",
    "        log.info(f\"[{solver}] sigma_R={sigma_R.round(3)}\")\n",
    "        log.info(f\"[{solver}] sigma_S={sigma_S.round(3)}\")\n",
    "\n",
    "        # Stability diagnostics\n",
    "        regrets = compute_ce_regrets(U_R, U_S, X)\n",
    "        ent = entropy_of_joint(X)\n",
    "\n",
    "        # Self-play evaluation under joint X\n",
    "        sp = evaluate_joint_strategy(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            X,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.eval_episodes,\n",
    "            base_seed=9000 + 100 * it,\n",
    "        )\n",
    "\n",
    "        log.info(\n",
    "            f\"[{solver}] CE-regrets: maxR={regrets['max_regret_R']:.3f} maxS={regrets['max_regret_S']:.3f} | \"\n",
    "            f\"SelfPlay: UR={sp.mean_U_R:.2f} US={sp.mean_U_S:.2f} det%={100*sp.det_rate:.1f} goal%={100*sp.goal_rate:.1f} | \"\n",
    "            f\"H(X)={ent:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Best-response expansion\n",
    "        if solver == \"marginal\":\n",
    "            br_r = robot_best_response_to_mixture(\n",
    "                env,\n",
    "                sensors,\n",
    "                pi_S=sigma_S,\n",
    "                robots=robots,\n",
    "                M_modes=M_modes,\n",
    "                risk_weight=args.risk_weight_br,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "            br_s = sensor_best_response_to_mixture(\n",
    "                env,\n",
    "                robots,\n",
    "                pi_R=sigma_R,\n",
    "                candidate_modes=list(range(M_modes)),\n",
    "                M_modes=M_modes,\n",
    "                rollouts=args.rollouts_br,\n",
    "                base_seed=2000 + 100 * it,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "        elif solver == \"correlated\":\n",
    "            # FIX: choose conditional mixtures q(.|i) and q(.|j)\n",
    "            # We only check top-K recommendations to keep it tractable.\n",
    "            topK = max(1, int(args.cond_top_k))\n",
    "\n",
    "            # Robot: check the top-K robot recommendations by sigma_R\n",
    "            cand_i = [int(i) for i in np.argsort(-sigma_R) if sigma_R[int(i)] > 1e-8][:topK]\n",
    "            if not cand_i:\n",
    "                cand_i = [int(i) for i in np.argsort(-sigma_R)[:topK]]\n",
    "            best_gain = 0.0\n",
    "            br_r = robots[0]\n",
    "\n",
    "            for i in cand_i:\n",
    "                qS = conditional_sensor_given_robot(X, int(i))\n",
    "                tag = f\"Cond_i{i}\"\n",
    "                pol = robot_best_response_to_mixture(\n",
    "                    env,\n",
    "                    sensors,\n",
    "                    pi_S=qS,\n",
    "                    robots=robots,\n",
    "                    M_modes=M_modes,\n",
    "                    risk_weight=args.risk_weight_br,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement: E_q[U_R(pol,j)] - E_q[U_R(i,j)]\n",
    "                # If pol is already in set, its payoff exists in U_R row of that policy.\n",
    "                # Otherwise we simulate pol against each sensor policy j.\n",
    "                if pol in robots:\n",
    "                    ip = robots.index(pol)\n",
    "                    dev = float(np.dot(qS, U_R[ip, :]))\n",
    "                else:\n",
    "                    # simulate quickly vs each sensor policy\n",
    "                    dev_vals = []\n",
    "                    for j in range(n):\n",
    "                        vals = []\n",
    "                        for kk in range(args.br_eval_rollouts):\n",
    "                            seed = 777000 + 1000 * it + 100 * i + 10 * j + kk\n",
    "                            st = rollout_episode(env, pol, sensors[j], M_modes=M_modes, seed=seed)\n",
    "                            vals.append(st.U_R)\n",
    "                        dev_vals.append(float(np.mean(vals)))\n",
    "                    dev = float(np.dot(qS, np.asarray(dev_vals)))\n",
    "\n",
    "                rec = float(np.dot(qS, U_R[i, :]))\n",
    "                gain = dev - rec\n",
    "                if gain > best_gain + 1e-9:\n",
    "                    best_gain = gain\n",
    "                    br_r = pol\n",
    "\n",
    "            if best_gain > args.add_threshold:\n",
    "                if br_r in robots:\n",
    "                    log.info(f\"[{solver}] Best robot deviation already in set; est_gain={best_gain:.3f}\")\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding robot deviation; est_gain={best_gain:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No robot deviation above threshold (best_gain={best_gain:.3f}).\")\n",
    "\n",
    "            # Sensor: check top-K sensor recommendations by sigma_S\n",
    "            cand_j = [int(j) for j in np.argsort(-sigma_S) if sigma_S[int(j)] > 1e-8][:topK]\n",
    "            if not cand_j:\n",
    "                cand_j = [int(j) for j in np.argsort(-sigma_S)[:topK]]\n",
    "            best_gain_s = 0.0\n",
    "            br_s = FixedModeSensorPolicy(0, name=\"S_dummy\")\n",
    "\n",
    "            for j in cand_j:\n",
    "                qR = conditional_robot_given_sensor(X, int(j))\n",
    "                tag = f\"Cond_j{j}\"\n",
    "                polS = sensor_best_response_to_mixture(\n",
    "                    env,\n",
    "                    robots,\n",
    "                    pi_R=qR,\n",
    "                    candidate_modes=list(range(M_modes)),\n",
    "                    M_modes=M_modes,\n",
    "                    rollouts=args.rollouts_br,\n",
    "                    base_seed=333000 + 1000 * it + 10 * j,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement for sensor\n",
    "                # rec under recommendation j is E_q[U_S(i,j)]\n",
    "                recS = float(np.dot(qR, U_S[:, j]))\n",
    "\n",
    "                # dev under mode polS.mode: if already present, use its column.\n",
    "                existing_col = None\n",
    "                for jj, spj in enumerate(sensors):\n",
    "                    if isinstance(spj, FixedModeSensorPolicy) and spj.mode == polS.mode:\n",
    "                        existing_col = jj\n",
    "                        break\n",
    "\n",
    "                if existing_col is not None:\n",
    "                    devS = float(np.dot(qR, U_S[:, existing_col]))\n",
    "                else:\n",
    "                    vals = []\n",
    "                    for kk in range(args.br_eval_rollouts):\n",
    "                        i_samp = int(np.random.default_rng(444 + kk).choice(len(robots), p=qR))\n",
    "                        seed = 888000 + 1000 * it + 10 * j + kk\n",
    "                        st = rollout_episode(env, robots[i_samp], polS, M_modes=M_modes, seed=seed)\n",
    "                        vals.append(st.U_S)\n",
    "                    devS = float(np.mean(vals))\n",
    "\n",
    "                gainS = devS - recS\n",
    "                if gainS > best_gain_s + 1e-9:\n",
    "                    best_gain_s = gainS\n",
    "                    br_s = polS\n",
    "\n",
    "            if best_gain_s > args.add_threshold:\n",
    "                if (isinstance(br_s, FixedModeSensorPolicy)) and (find_sensor_by_mode(sensors, br_s.mode) is not None):\n",
    "                    log.info(f\"[{solver}] Best sensor deviation already in set (mode={br_s.mode}); est_gain={best_gain_s:.3f}\")\n",
    "                    br_s = None\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding sensor deviation; est_gain={best_gain_s:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No sensor deviation above threshold (best_gain={best_gain_s:.3f}).\")\n",
    "                br_s = None\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"solver must be marginal or correlated\")\n",
    "\n",
    "        # Add to sets (dedupe)\n",
    "        if br_r not in robots:\n",
    "            robots.append(br_r)\n",
    "\n",
    "        if isinstance(br_s, FixedModeSensorPolicy):\n",
    "            if find_sensor_by_mode(sensors, br_s.mode) is None:\n",
    "                sensors.append(br_s)\n",
    "            else:\n",
    "                log.info(f\"[{solver}] Sensor mode {br_s.mode} already present; not adding duplicate.\")\n",
    "\n",
    "        seconds = float(time.time() - t0)\n",
    "        history.append(\n",
    "            TrainHistoryRow(\n",
    "                outer_iter=it,\n",
    "                m=len(robots),\n",
    "                n=len(sensors),\n",
    "                nbs_obj=float(nbs.obj),\n",
    "                entropy_X=float(ent),\n",
    "                max_regret_R=float(regrets[\"max_regret_R\"]),\n",
    "                max_regret_S=float(regrets[\"max_regret_S\"]),\n",
    "                selfplay_UR=float(sp.mean_U_R),\n",
    "                selfplay_US=float(sp.mean_U_S),\n",
    "                selfplay_det=float(sp.det_rate),\n",
    "                selfplay_goal=float(sp.goal_rate),\n",
    "                seconds=seconds,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        log.info(f\"[{solver}] Sets: |Pi_R|={len(robots)} |Pi_S|={len(sensors)} | iter_seconds={seconds:.2f}\")\n",
    "\n",
    "    if X is None:\n",
    "        raise RuntimeError(\"Training produced no X\")\n",
    "\n",
    "    log.banner(f\"[{solver}] Finished\")\n",
    "    log.info(f\"[{solver}] Final robots: \" + \", \".join([p.name for p in robots]))\n",
    "    log.info(f\"[{solver}] Final sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "    log.info(f\"[{solver}] Total time: {time.time()-t0_all:.2f}s\")\n",
    "\n",
    "    return TrainResult(solver=solver, env=env, robots=robots, sensors=sensors, X=X, history=history)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Benchmarks + plotting\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BenchCell:\n",
    "    UR: float\n",
    "    US: float\n",
    "    det: float\n",
    "    goal: float\n",
    "\n",
    "\n",
    "def run_policy_matrix_benchmark(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_methods: List[RobotPolicy],\n",
    "    sensor_methods: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[Tuple[int, int], BenchCell]:\n",
    "    res: Dict[Tuple[int, int], BenchCell] = {}\n",
    "    for i, rp in enumerate(robot_methods):\n",
    "        for j, sp in enumerate(sensor_methods):\n",
    "            UR = []\n",
    "            US = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            for k in range(episodes):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed)\n",
    "                UR.append(st.U_R)\n",
    "                US.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "            res[(i, j)] = BenchCell(\n",
    "                UR=float(np.mean(UR)),\n",
    "                US=float(np.mean(US)),\n",
    "                det=float(det / episodes),\n",
    "                goal=float(goal / episodes),\n",
    "            )\n",
    "    return res\n",
    "\n",
    "\n",
    "def safe_makedirs(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_history_csv(hist: List[TrainHistoryRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    fields = list(TrainHistoryRow.__annotations__.keys())\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fields)\n",
    "        w.writeheader()\n",
    "        for row in hist:\n",
    "            w.writerow({k: getattr(row, k) for k in fields})\n",
    "\n",
    "\n",
    "def plot_training_curves(results: List[TrainResult], outdir: str) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # 1) NBS objective\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.nbs_obj for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"NBS objective\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_nbs_obj.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Max conditional regrets\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_R for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: maxRegret_R\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_S for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: maxRegret_S\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Max conditional regret\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_max_regrets.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 3) Entropy of X\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.entropy_X for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Entropy H(X)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_entropy_X.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 4) Self-play outcomes\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_UR for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: UR\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_US for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: US\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Expected utility under X\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_selfplay_utils.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_benchmark_bars(\n",
    "    robot_names: List[str],\n",
    "    sensor_names: List[str],\n",
    "    bench: Dict[Tuple[int, int], BenchCell],\n",
    "    outdir: str,\n",
    ") -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # For each sensor, bar chart of robot UR\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].UR for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Robot utility\")\n",
    "        plt.title(f\"Robot utility vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_UR_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # For each sensor, bar chart of robot goal rate\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].goal for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Goal rate\")\n",
    "        plt.title(f\"Robot goal rate vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_goal_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main pipeline wrapper\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(args: argparse.Namespace) -> None:\n",
    "    log = Logger(args.log_level)\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.banner(\"[PIPELINE] Stealth grid game\")\n",
    "        log.info(f\"Grid: {grid.width}x{grid.height} start={grid.start} goal={grid.goal} obstacles={len(grid.obstacles)}\")\n",
    "        log.info(f\"Sensors: {sensor_cfg.sensors} radius={sensor_cfg.radius} base_p={sensor_cfg.base_p} hotspot_p={sensor_cfg.hotspot_p}\")\n",
    "        if log.k >= 2:\n",
    "            log.debug(\"ASCII map:\")\n",
    "            print_grid_ascii(grid, sensor_cfg)\n",
    "\n",
    "    solvers: List[str]\n",
    "    if args.solver == \"both\":\n",
    "        solvers = [\"marginal\", \"correlated\"]\n",
    "    else:\n",
    "        solvers = [args.solver]\n",
    "\n",
    "    results: List[TrainResult] = []\n",
    "    for s in solvers:\n",
    "        # Use a fresh env copy per solver (to avoid RNG coupling)\n",
    "        env_s = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "        results.append(run_training(env_s, args, solver=s, log=log))\n",
    "\n",
    "    if args.results_dir:\n",
    "        safe_makedirs(args.results_dir)\n",
    "        for r in results:\n",
    "            save_history_csv(r.history, os.path.join(args.results_dir, f\"history_{r.solver}.csv\"))\n",
    "\n",
    "    if args.save_plots and args.results_dir:\n",
    "        plot_training_curves(results, outdir=args.results_dir)\n",
    "\n",
    "    # Benchmarks on the same game\n",
    "    if args.run_benchmarks:\n",
    "        log.banner(\"[BENCH] Heuristics on same game\")\n",
    "        M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "        # Robot heuristics\n",
    "        #  - fixed paths from initial set\n",
    "        robots_init, sensors_init = build_initial_policies_for_bench(env, M_modes)\n",
    "\n",
    "        # Sensor heuristics\n",
    "        sensor_methods: List[SensorPolicy] = [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=args.seed + 123, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "        bench = run_policy_matrix_benchmark(\n",
    "            env,\n",
    "            robot_methods=robots_init,\n",
    "            sensor_methods=sensor_methods,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.bench_episodes,\n",
    "            base_seed=555000,\n",
    "        )\n",
    "\n",
    "        # Print a compact table\n",
    "        robot_names = [p.name for p in robots_init]\n",
    "        sensor_names = [p.name for p in sensor_methods]\n",
    "        for j, sname in enumerate(sensor_names):\n",
    "            log.info(f\"[BENCH] Sensor={sname}\")\n",
    "            for i, rname in enumerate(robot_names):\n",
    "                cell = bench[(i, j)]\n",
    "                log.info(f\"  Robot={rname:22s} UR={cell.UR:8.2f} goal%={100*cell.goal:5.1f} det%={100*cell.det:5.1f}\")\n",
    "\n",
    "        if args.save_plots and args.results_dir:\n",
    "            plot_benchmark_bars(robot_names, sensor_names, bench, outdir=args.results_dir)\n",
    "\n",
    "\n",
    "# Helper: initial robot set for benchmark (without the solver-added BR policies)\n",
    "\n",
    "def build_initial_policies_for_bench(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    # Static risk-aware A* under UNIFORM mode belief\n",
    "    uniform_mode = np.full(M_modes, 1.0 / M_modes)\n",
    "\n",
    "    # Construct risk map directly under uniform belief\n",
    "    H, W = grid.height, grid.width\n",
    "    risk_uniform = np.zeros((H, W), dtype=float)\n",
    "    risk_worst = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in grid.obstacles:\n",
    "                risk_uniform[y, x] = np.nan\n",
    "                risk_worst[y, x] = np.nan\n",
    "                continue\n",
    "            vals = [env.true_detection_prob((x, y), m) for m in range(M_modes)]\n",
    "            risk_uniform[y, x] = float(np.dot(uniform_mode, vals))\n",
    "            risk_worst[y, x] = float(np.max(vals))\n",
    "\n",
    "    p_uniform = plan_risk_weighted_path(env, risk_uniform, risk_weight=12.0)\n",
    "    p_worst = plan_risk_weighted_path(env, risk_worst, risk_weight=12.0)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        FixedPathPolicy(p_uniform, \"R_RiskAStar_Uniform\"),\n",
    "        FixedPathPolicy(p_worst, \"R_RiskAStar_WorstCase\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "    ]\n",
    "\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLI\n",
    "# =============================================================================\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> Tuple[argparse.Namespace, List[str]]:\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "    p.add_argument(\"--log-level\", type=str, default=\"INFO\", choices=[\"QUIET\", \"INFO\", \"DEBUG\"])\n",
    "\n",
    "    p.add_argument(\"--solver\", type=str, default=\"correlated\", choices=[\"marginal\", \"correlated\", \"both\"])\n",
    "\n",
    "    p.add_argument(\"--outer-iters\", type=int, default=3)\n",
    "\n",
    "    p.add_argument(\"--rollouts-payoff\", type=int, default=20)\n",
    "    p.add_argument(\"--rollouts-br\", type=int, default=30)\n",
    "    p.add_argument(\"--risk-weight-br\", type=float, default=12.0)\n",
    "\n",
    "    # NBS knobs\n",
    "    p.add_argument(\"--disagreement\", type=str, default=\"minminus\", choices=[\"minminus\", \"uniform\"])\n",
    "    p.add_argument(\"--entropy-tau\", type=float, default=0.0)\n",
    "\n",
    "    # Fix mismatch knobs\n",
    "    p.add_argument(\"--cond-top-k\", type=int, default=2, help=\"How many top recommendations to check for conditional BRs\")\n",
    "    p.add_argument(\"--br-eval-rollouts\", type=int, default=8, help=\"Small evaluation rollouts for new deviations\")\n",
    "    p.add_argument(\"--add-threshold\", type=float, default=0.25, help=\"Minimum estimated conditional gain to add a deviation policy\")\n",
    "\n",
    "    # Eval episodes under joint X (self-play)\n",
    "    p.add_argument(\"--eval-episodes\", type=int, default=60)\n",
    "\n",
    "    # Debug\n",
    "    p.add_argument(\"--debug-rollout-pair\", type=str, default=\"\", help=\"Print one step-by-step rollout for i,j\")\n",
    "\n",
    "    # Outputs\n",
    "    p.add_argument(\"--results-dir\", type=str, default=\"results\", help=\"Directory for csv/plots\")\n",
    "    p.add_argument(\"--save-plots\", action=\"store_true\")\n",
    "\n",
    "    # Benchmarks\n",
    "    p.add_argument(\"--run-benchmarks\", action=\"store_true\")\n",
    "    p.add_argument(\"--bench-episodes\", type=int, default=80)\n",
    "\n",
    "    args, unknown = p.parse_known_args(args=argv)\n",
    "    return args, unknown\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    args, unknown = parse_args(argv=argv)\n",
    "    if unknown and (\"ipykernel\" not in sys.modules):\n",
    "        print(f\"[WARN] Ignoring unknown CLI args: {unknown}\")\n",
    "    if args.debug_rollout_pair.strip() == \"\":\n",
    "        args.debug_rollout_pair = \"\"\n",
    "    run_pipeline(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and (\"ipykernel\" not in sys.modules):\n",
    "    main()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENTS (fixed game set + baselines + tests + presentation plots)\n",
    "# =============================================================================\n",
    "#\n",
    "# Keep the training code above exactly as-is.\n",
    "# This section adds:\n",
    "#   (A) sanity tests (quick asserts)\n",
    "#   (B) a fixed game-set generator (deterministic)\n",
    "#   (C) an experiment runner that compares solvers + baselines on the same games\n",
    "#   (D) plots + CSV outputs for slides\n",
    "#\n",
    "# Notebook usage:\n",
    "#   run_sanity_tests()\n",
    "#   run_fixed_games_experiment(outdir=\"results_exp\", n_games=6, seeds=[0,1,2])\n",
    "#\n",
    "\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GameInstance:\n",
    "    game_id: str\n",
    "    grid: GridConfig\n",
    "    sensor_cfg: SensorConfig\n",
    "    desc: str\n",
    "\n",
    "\n",
    "def _is_reachable(grid: GridConfig) -> bool:\n",
    "    try:\n",
    "        _ = astar_path(grid, grid.start, grid.goal, step_cost=lambda a, b: 1.0, max_expansions=250_000)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def build_fixed_game_set(\n",
    "    n_games: int = 6,\n",
    "    seed: int = 123,\n",
    "    width: int = 15,\n",
    "    height: int = 9,\n",
    "    base_radius: int = 2,\n",
    "    base_p: float = 0.02,\n",
    "    hotspot_p: float = 0.60,\n",
    ") -> List[GameInstance]:\n",
    "    \"\"\"Deterministic *fixed* game set for fair comparisons.\n",
    "\n",
    "    We generate corridor-variant grids by adding a small number of extra obstacles\n",
    "    (without breaking reachability), while keeping the base corridor wall.\n",
    "\n",
    "    Notes:\n",
    "      - Uses a fixed RNG seed => same games every run.\n",
    "      - Guarantees start->goal reachability (ignoring detection risk).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    base_grid = build_two_corridor_grid(width=width, height=height)\n",
    "    wall_x = width // 2\n",
    "\n",
    "    # Keep the two sensor centers near the corridor gaps for interpretability.\n",
    "    base_sensors = ((wall_x, 2), (wall_x, 6))\n",
    "\n",
    "    games: List[GameInstance] = []\n",
    "    attempts = 0\n",
    "\n",
    "    # Increasing difficulty: more extra obstacles.\n",
    "    # (Chosen small so it stays tractable and doesn’t accidentally block corridors.)\n",
    "    obstacle_budgets = [0, 2, 4, 6, 8, 10]\n",
    "    while len(games) < n_games:\n",
    "        attempts += 1\n",
    "        if attempts > 4000:\n",
    "            raise RuntimeError(\"Could not generate enough solvable game instances\")\n",
    "\n",
    "        k = len(games)\n",
    "        extra_obs_target = obstacle_budgets[min(k, len(obstacle_budgets) - 1)]\n",
    "\n",
    "        # Candidate cells for extra obstacles (avoid start/goal/sensors and the wall column).\n",
    "        candidates: List[Tuple[int, int]] = []\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                p = (x, y)\n",
    "                if p in base_grid.obstacles:\n",
    "                    continue\n",
    "                if p == base_grid.start or p == base_grid.goal:\n",
    "                    continue\n",
    "                if p in base_sensors:\n",
    "                    continue\n",
    "                if x == wall_x:\n",
    "                    continue\n",
    "                candidates.append(p)\n",
    "\n",
    "        rng.shuffle(candidates)\n",
    "        extra = set(candidates[:extra_obs_target])\n",
    "\n",
    "        grid = GridConfig(\n",
    "            width=base_grid.width,\n",
    "            height=base_grid.height,\n",
    "            start=base_grid.start,\n",
    "            goal=base_grid.goal,\n",
    "            obstacles=frozenset(set(base_grid.obstacles) | extra),\n",
    "        )\n",
    "        if not _is_reachable(grid):\n",
    "            continue\n",
    "\n",
    "        sensor_cfg = SensorConfig(\n",
    "            sensors=base_sensors,\n",
    "            radius=int(base_radius),\n",
    "            base_p=float(base_p),\n",
    "            hotspot_p=float(hotspot_p),\n",
    "        )\n",
    "\n",
    "        game_id = f\"G{k:02d}_extraObs{extra_obs_target}\"\n",
    "        desc = f\"corridors + {extra_obs_target} extra obstacles\"\n",
    "        games.append(GameInstance(game_id=game_id, grid=grid, sensor_cfg=sensor_cfg, desc=desc))\n",
    "\n",
    "    return games\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Sanity tests\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_sanity_tests() -> None:\n",
    "    \"\"\"Fast, high-signal checks so you can trust comparisons.\"\"\"\n",
    "    print(\"[TEST] Running sanity tests...\")\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=0)\n",
    "\n",
    "    # A* reachability\n",
    "    p = astar_path(grid, grid.start, grid.goal, step_cost=lambda a, b: 1.0)\n",
    "    assert p[0] == grid.start and p[-1] == grid.goal\n",
    "\n",
    "    # Belief update preserves simplex\n",
    "    b = ModeBelief(M=2)\n",
    "    env.reset(sensor_mode=0)\n",
    "    out = env.step((1, 0))\n",
    "    b.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "    assert np.isfinite(b.b).all()\n",
    "    assert abs(float(b.b.sum()) - 1.0) < 1e-6\n",
    "    assert (b.b >= -1e-12).all()\n",
    "\n",
    "    # NBS returns simplex\n",
    "    uR = np.array([-1.0, -2.0, -3.0, -4.0])\n",
    "    uS = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "    nbs = solve_nbs(uR, uS, log=Logger(\"QUIET\"), max_iters=50)\n",
    "    x = nbs.x\n",
    "    assert np.isfinite(x).all()\n",
    "    assert (x >= -1e-10).all()\n",
    "    assert abs(float(x.sum()) - 1.0) < 1e-6\n",
    "\n",
    "    # Joint/marginals/conditionals are sane\n",
    "    X = joint_to_matrix(x, m=2, n=2)\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "    assert abs(float(sigma_R.sum()) - 1.0) < 1e-6\n",
    "    assert abs(float(sigma_S.sum()) - 1.0) < 1e-6\n",
    "    qS = conditional_sensor_given_robot(X, 0)\n",
    "    qR = conditional_robot_given_sensor(X, 0)\n",
    "    assert abs(float(qS.sum()) - 1.0) < 1e-6\n",
    "    assert abs(float(qR.sum()) - 1.0) < 1e-6\n",
    "\n",
    "    # CE regrets should be >= 0 (numerical)\n",
    "    UR = np.random.default_rng(0).normal(size=(3, 2))\n",
    "    US = np.random.default_rng(1).normal(size=(3, 2))\n",
    "    Xr = np.full((3, 2), 1.0 / 6)\n",
    "    reg = compute_ce_regrets(UR, US, Xr)\n",
    "    assert reg[\"max_regret_R\"] >= -1e-9\n",
    "    assert reg[\"max_regret_S\"] >= -1e-9\n",
    "\n",
    "    # Rollout returns finite stats\n",
    "    robots, sensors = build_initial_policies(env, M_modes=2)\n",
    "    st = rollout_episode(env, robots[0], sensors[0], M_modes=2, seed=0)\n",
    "    assert np.isfinite(st.U_R) and np.isfinite(st.U_S)\n",
    "\n",
    "    print(\"[TEST] All sanity tests passed ✅\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Fair comparisons on the same fixed game set\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ExperimentRow:\n",
    "    alg: str\n",
    "    solver: str\n",
    "    game_id: str\n",
    "    seed: int\n",
    "    metric: str\n",
    "    value: float\n",
    "\n",
    "\n",
    "def _evaluate_robot_mixture_against_sensor(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sigma_R: np.ndarray,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    sigma_R = np.asarray(sigma_R, dtype=float).reshape(-1)\n",
    "    sigma_R = sigma_R / max(float(sigma_R.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR, US = [], []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        i = int(rng.choice(len(robots), p=sigma_R))\n",
    "        st = rollout_episode(env, robots[i], sensor, M_modes=M_modes, seed=int(base_seed + k))\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "def _summarize_against_sensor_suite(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sigma_R: np.ndarray,\n",
    "    sensors_suite: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes_per_sensor: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[str, float]:\n",
    "    vals_UR = []\n",
    "    vals_goal = []\n",
    "    vals_det = []\n",
    "\n",
    "    for j, sp in enumerate(sensors_suite):\n",
    "        ev = _evaluate_robot_mixture_against_sensor(\n",
    "            env,\n",
    "            robots,\n",
    "            sigma_R=sigma_R,\n",
    "            sensor=sp,\n",
    "            M_modes=M_modes,\n",
    "            episodes=episodes_per_sensor,\n",
    "            base_seed=base_seed + 10000 * j,\n",
    "        )\n",
    "        vals_UR.append(ev.mean_U_R)\n",
    "        vals_goal.append(ev.goal_rate)\n",
    "        vals_det.append(ev.det_rate)\n",
    "\n",
    "    # \"Robust\" = worst case over the sensor suite.\n",
    "    return {\n",
    "        \"mean_UR\": float(np.mean(vals_UR)),\n",
    "        \"robust_UR\": float(np.min(vals_UR)),\n",
    "        \"mean_goal\": float(np.mean(vals_goal)),\n",
    "        \"robust_goal\": float(np.min(vals_goal)),\n",
    "        \"mean_det\": float(np.mean(vals_det)),\n",
    "        \"robust_det\": float(np.max(vals_det)),\n",
    "    }\n",
    "\n",
    "\n",
    "def _write_experiment_csv(rows: List[ExperimentRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    safe_makedirs(os.path.dirname(path) or \".\")\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"alg\", \"solver\", \"game_id\", \"seed\", \"metric\", \"value\"])\n",
    "        for r in rows:\n",
    "            w.writerow([r.alg, r.solver, r.game_id, r.seed, r.metric, f\"{r.value:.8f}\"])\n",
    "\n",
    "\n",
    "def _group_stats(values: List[float]) -> Dict[str, float]:\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    return {\n",
    "        \"mean\": float(np.mean(arr)) if arr.size else float(\"nan\"),\n",
    "        \"std\": float(np.std(arr)) if arr.size else float(\"nan\"),\n",
    "        \"n\": int(arr.size),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_experiment_summary(rows: List[ExperimentRow], outdir: str) -> None:\n",
    "    \"\"\"Create *paper-style* figures from ExperimentRow logs.\n",
    "\n",
    "    Why these figures are meaningful:\n",
    "      - We plot performance *per game instance* (difficulty sweep) so you can see\n",
    "        robustness trends, not just an average.\n",
    "      - We summarize distributions across (game, seed) with boxplots.\n",
    "      - We show trade-offs (goal vs detection, runtime vs performance) with scatter.\n",
    "\n",
    "    Outputs (PNG + PDF):\n",
    "      fig_robustUR_by_game, fig_goal_det_tradeoff, fig_runtime_vs_robustUR,\n",
    "      fig_robust_goal_by_game, fig_robust_det_by_game,\n",
    "      fig_box_robustUR, fig_box_goal, fig_box_det,\n",
    "      fig_stability_regret_vs_robustUR\n",
    "\n",
    "    Notes:\n",
    "      - Uses matplotlib defaults (no manual colors), saves high DPI.\n",
    "      - Assumes metric semantics:\n",
    "          robust_UR: higher is better\n",
    "          robust_goal: higher is better\n",
    "          robust_det: lower is better (worst-case detection rate)\n",
    "          runtime_s, regrets: lower is better\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Collect + aggregate utilities\n",
    "    # ----------------------------\n",
    "    algs = sorted({r.alg for r in rows})\n",
    "    games = sorted({r.game_id for r in rows})\n",
    "\n",
    "    # Map: (alg, game, metric) -> [values across seeds]\n",
    "    bucket: Dict[Tuple[str, str, str], List[float]] = {}\n",
    "    for r in rows:\n",
    "        key = (r.alg, r.game_id, r.metric)\n",
    "        bucket.setdefault(key, []).append(float(r.value))\n",
    "\n",
    "    def get_vals(alg: str, game_id: str, metric: str) -> List[float]:\n",
    "        return bucket.get((alg, game_id, metric), [])\n",
    "\n",
    "    def mean_ci95(vals: List[float]) -> Tuple[float, float]:\n",
    "        arr = np.asarray(vals, dtype=float)\n",
    "        if arr.size == 0:\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mu = float(np.mean(arr))\n",
    "        if arr.size == 1:\n",
    "            return mu, 0.0\n",
    "        se = float(np.std(arr, ddof=1) / np.sqrt(arr.size))\n",
    "        return mu, 1.96 * se\n",
    "\n",
    "    def savefig(base: str) -> None:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, base + \".png\"), dpi=300)\n",
    "        plt.savefig(os.path.join(outdir, base + \".pdf\"))\n",
    "        plt.close()\n",
    "\n",
    "    # ----------------------------\n",
    "    # (1) Metric vs game difficulty (line + CI)\n",
    "    # ----------------------------\n",
    "    def plot_by_game(metric: str, ylabel: str, base: str) -> None:\n",
    "        plt.figure(figsize=(9.0, 4.2))\n",
    "        x = np.arange(len(games))\n",
    "        for alg in algs:\n",
    "            ys = []\n",
    "            es = []\n",
    "            for gid in games:\n",
    "                mu, ci = mean_ci95(get_vals(alg, gid, metric))\n",
    "                ys.append(mu)\n",
    "                es.append(ci)\n",
    "            ys = np.asarray(ys, dtype=float)\n",
    "            es = np.asarray(es, dtype=float)\n",
    "            plt.plot(x, ys, marker=\"o\", linewidth=2.0, label=alg)\n",
    "            # CI band (skip if nan)\n",
    "            ok = np.isfinite(ys) & np.isfinite(es)\n",
    "            if np.any(ok):\n",
    "                plt.fill_between(x[ok], (ys - es)[ok], (ys + es)[ok], alpha=0.15)\n",
    "        plt.xticks(x, games, rotation=25, ha=\"right\")\n",
    "        plt.xlabel(\"Game instance (increasing obstacle perturbations)\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True, alpha=0.25)\n",
    "        plt.legend(ncol=2, fontsize=9)\n",
    "        savefig(base)\n",
    "\n",
    "    plot_by_game(\"robust_UR\", \"Robust robot utility (worst-case over sensor suite) ↑\", \"fig_robustUR_by_game\")\n",
    "    plot_by_game(\"robust_goal\", \"Robust goal rate (worst-case over sensor suite) ↑\", \"fig_robust_goal_by_game\")\n",
    "    plot_by_game(\"robust_det\", \"Worst-case detection rate over sensor suite ↓\", \"fig_robust_det_by_game\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # (2) Overall distribution boxplots (across games+seeds)\n",
    "    # ----------------------------\n",
    "    def plot_box(metric: str, ylabel: str, base: str) -> None:\n",
    "        plt.figure(figsize=(9.0, 4.2))\n",
    "        data = []\n",
    "        for alg in algs:\n",
    "            vals = [float(r.value) for r in rows if r.alg == alg and r.metric == metric]\n",
    "            data.append(vals)\n",
    "        plt.boxplot(data, labels=algs, showfliers=False)\n",
    "        plt.xticks(rotation=25, ha=\"right\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True, axis=\"y\", alpha=0.25)\n",
    "        savefig(base)\n",
    "\n",
    "    plot_box(\"robust_UR\", \"Robust robot utility (worst-case over sensor suite) ↑\", \"fig_box_robustUR\")\n",
    "    plot_box(\"robust_goal\", \"Robust goal rate (worst-case over sensor suite) ↑\", \"fig_box_goal\")\n",
    "    plot_box(\"robust_det\", \"Worst-case detection rate over sensor suite ↓\", \"fig_box_det\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # (3) Goal vs detection trade-off (Pareto-ish view)\n",
    "    #     Each point is (game, mean over seeds).\n",
    "    # ----------------------------\n",
    "    plt.figure(figsize=(6.0, 5.2))\n",
    "    for alg in algs:\n",
    "        xs = []  # detection\n",
    "        ys = []  # goal\n",
    "        for gid in games:\n",
    "            mu_det, _ = mean_ci95(get_vals(alg, gid, \"robust_det\"))\n",
    "            mu_goal, _ = mean_ci95(get_vals(alg, gid, \"robust_goal\"))\n",
    "            if np.isfinite(mu_det) and np.isfinite(mu_goal):\n",
    "                xs.append(mu_det)\n",
    "                ys.append(mu_goal)\n",
    "        if xs and ys:\n",
    "            plt.scatter(xs, ys, label=alg)\n",
    "    plt.xlabel(\"Worst-case detection rate (lower is better)\")\n",
    "    plt.ylabel(\"Worst-case goal rate (higher is better)\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(fontsize=9)\n",
    "    savefig(\"fig_goal_det_tradeoff\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # (4) Runtime vs robust utility (efficiency vs performance)\n",
    "    #     Use per-run points (game, seed).\n",
    "    # ----------------------------\n",
    "    plt.figure(figsize=(6.0, 5.2))\n",
    "    for alg in algs:\n",
    "        xs = [float(r.value) for r in rows if r.alg == alg and r.metric == \"runtime_s\"]\n",
    "        ys = [float(r.value) for r in rows if r.alg == alg and r.metric == \"robust_UR\"]\n",
    "        if xs and ys and len(xs) == len(ys):\n",
    "            plt.scatter(xs, ys, label=alg)\n",
    "    plt.xlabel(\"Runtime (seconds) ↓\")\n",
    "    plt.ylabel(\"Robust robot utility ↑\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(fontsize=9)\n",
    "    savefig(\"fig_runtime_vs_robustUR\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # (5) Stability diagnostic: regret vs robust utility\n",
    "    # ----------------------------\n",
    "    plt.figure(figsize=(6.0, 5.2))\n",
    "    for alg in algs:\n",
    "        xs = [float(r.value) for r in rows if r.alg == alg and r.metric == \"max_regret_R\"]\n",
    "        ys = [float(r.value) for r in rows if r.alg == alg and r.metric == \"robust_UR\"]\n",
    "        if xs and ys and len(xs) == len(ys):\n",
    "            plt.scatter(xs, ys, label=alg)\n",
    "    plt.xlabel(\"Max conditional regret (robot) ↓\")\n",
    "    plt.ylabel(\"Robust robot utility ↑\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(fontsize=9)\n",
    "    savefig(\"fig_stability_regret_vs_robustUR\")\n",
    "\n",
    "\n",
    "def run_fixed_games_experiment(\n",
    "    outdir: str = \"results_exp\",\n",
    "    n_games: int = 6,\n",
    "    seeds: Optional[List[int]] = None,\n",
    "    outer_iters: int = 3,\n",
    "    rollouts_payoff: int = 20,\n",
    "    rollouts_br: int = 30,\n",
    "    eval_episodes: int = 120,\n",
    "    episodes_per_sensor: int = 80,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    "    cond_top_k: int = 2,\n",
    "    br_eval_rollouts: int = 8,\n",
    "    add_threshold: float = 0.25,\n",
    "    risk_weight_br: float = 12.0,\n",
    "    include_solvers: Optional[List[str]] = None,\n",
    "    include_baselines: bool = True,\n",
    "    log_level: str = \"QUIET\",\n",
    ") -> None:\n",
    "    \"\"\"Run a clean comparison on a deterministic game set.\n",
    "\n",
    "    What you get in outdir:\n",
    "      - experiments.csv  (all raw numbers)\n",
    "      - exp_bar_*.png    (summary bars)\n",
    "      - exp_scatter_*.png\n",
    "\n",
    "    Recommended presentation story:\n",
    "      1) Robust UR and robust goal rate vs baselines\n",
    "      2) Runtime vs baselines\n",
    "      3) Scatter showing tradeoff: regret (stability) vs robust UR (performance)\n",
    "\n",
    "    NOTE: NBS is not a Nash/CE solver; higher regret is expected. That’s *part of the story*.\n",
    "    \"\"\"\n",
    "\n",
    "    if seeds is None:\n",
    "        seeds = [0, 1, 2]\n",
    "    if include_solvers is None:\n",
    "        include_solvers = [\"correlated\", \"marginal\"]\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    games = build_fixed_game_set(n_games=n_games)\n",
    "\n",
    "    rows: List[ExperimentRow] = []\n",
    "\n",
    "    # Sensor suite for robust evaluation (adversarial-ish)\n",
    "    # You can add Alternate/Random to show generalization, but the key is Mode0/Mode1.\n",
    "    def make_sensor_suite(M_modes: int, seed0: int) -> List[SensorPolicy]:\n",
    "        return [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=seed0 + 999, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "    for g in games:\n",
    "        for s in seeds:\n",
    "            env = GridWorldStealthEnv(g.grid, g.sensor_cfg, fp=0.05, fn=0.10, seed=s)\n",
    "            M_modes = len(g.sensor_cfg.sensors)\n",
    "            sensors_suite = make_sensor_suite(M_modes=M_modes, seed0=s)\n",
    "\n",
    "            # -----------------\n",
    "            # (1) Run solvers\n",
    "            # -----------------\n",
    "            for solver in include_solvers:\n",
    "                # Build an args object compatible with run_training()\n",
    "                args = argparse.Namespace(\n",
    "                    seed=s,\n",
    "                    log_level=log_level,\n",
    "                    solver=solver,\n",
    "                    outer_iters=int(outer_iters),\n",
    "                    rollouts_payoff=int(rollouts_payoff),\n",
    "                    rollouts_br=int(rollouts_br),\n",
    "                    risk_weight_br=float(risk_weight_br),\n",
    "                    disagreement=str(disagreement),\n",
    "                    entropy_tau=float(entropy_tau),\n",
    "                    cond_top_k=int(cond_top_k),\n",
    "                    br_eval_rollouts=int(br_eval_rollouts),\n",
    "                    add_threshold=float(add_threshold),\n",
    "                    eval_episodes=int(eval_episodes),\n",
    "                    debug_rollout_pair=\"\",\n",
    "                    results_dir=\"\",\n",
    "                    save_plots=False,\n",
    "                    run_benchmarks=False,\n",
    "                    bench_episodes=0,\n",
    "                )\n",
    "\n",
    "                t0 = time.time()\n",
    "                log = Logger(log_level)\n",
    "                tr = run_training(env, args, solver=solver, log=log)\n",
    "                runtime_s = float(time.time() - t0)\n",
    "\n",
    "                # Extract sigma_R from final X\n",
    "                sigma_R, _sigma_S = marginals_from_joint(tr.X)\n",
    "                suite_summary = _summarize_against_sensor_suite(\n",
    "                    env,\n",
    "                    robots=tr.robots,\n",
    "                    sigma_R=sigma_R,\n",
    "                    sensors_suite=sensors_suite,\n",
    "                    M_modes=M_modes,\n",
    "                    episodes_per_sensor=int(episodes_per_sensor),\n",
    "                    base_seed=100_000 + 1000 * s,\n",
    "                )\n",
    "\n",
    "                # Also keep solver-internal diagnostics (from last history row)\n",
    "                last = tr.history[-1]\n",
    "\n",
    "                alg = f\"Solver:{solver}\"\n",
    "                for k, v in suite_summary.items():\n",
    "                    rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=k, value=float(v)))\n",
    "\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"runtime_s\", value=runtime_s))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"entropy_X\", value=float(last.entropy_X)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"max_regret_R\", value=float(last.max_regret_R)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"max_regret_S\", value=float(last.max_regret_S)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"selfplay_UR\", value=float(last.selfplay_UR)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"selfplay_US\", value=float(last.selfplay_US)))\n",
    "\n",
    "            # -----------------\n",
    "            # (2) Baseline robots (no training)\n",
    "            # -----------------\n",
    "            if include_baselines:\n",
    "                robots_base, _ = build_initial_policies_for_bench(env, M_modes=M_modes)\n",
    "\n",
    "                for rp in robots_base:\n",
    "                    sigma = np.zeros(len(robots_base), dtype=float)\n",
    "                    sigma[robots_base.index(rp)] = 1.0\n",
    "\n",
    "                    suite_summary = _summarize_against_sensor_suite(\n",
    "                        env,\n",
    "                        robots=robots_base,\n",
    "                        sigma_R=sigma,\n",
    "                        sensors_suite=sensors_suite,\n",
    "                        M_modes=M_modes,\n",
    "                        episodes_per_sensor=int(episodes_per_sensor),\n",
    "                        base_seed=200_000 + 1000 * s,\n",
    "                    )\n",
    "\n",
    "                    alg = f\"Baseline:{rp.name}\"\n",
    "                    for k, v in suite_summary.items():\n",
    "                        rows.append(ExperimentRow(alg=alg, solver=\"baseline\", game_id=g.game_id, seed=s, metric=k, value=float(v)))\n",
    "\n",
    "    # Save raw numbers\n",
    "    csv_path = os.path.join(outdir, \"experiments.csv\")\n",
    "    _write_experiment_csv(rows, csv_path)\n",
    "\n",
    "    # Produce summary plots\n",
    "    plot_experiment_summary(rows, outdir=outdir)\n",
    "\n",
    "    print(f\"[EXP] Done. Wrote: {csv_path}\")\n",
    "    print(f\"[EXP] Plots saved to: {outdir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2f573f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Running sanity tests...\n",
      "[TEST] All sanity tests passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alian\\AppData\\Local\\Temp\\ipykernel_59516\\1881496546.py:2105: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(data, labels=algs, showfliers=False)\n",
      "C:\\Users\\alian\\AppData\\Local\\Temp\\ipykernel_59516\\1881496546.py:2105: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(data, labels=algs, showfliers=False)\n",
      "C:\\Users\\alian\\AppData\\Local\\Temp\\ipykernel_59516\\1881496546.py:2105: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(data, labels=algs, showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXP] Done. Wrote: results_exp\\experiments.csv\n",
      "[EXP] Plots saved to: results_exp\n"
     ]
    }
   ],
   "source": [
    "run_sanity_tests()\n",
    "\n",
    "run_fixed_games_experiment(\n",
    "    outdir=\"results_exp\",\n",
    "    n_games=6,\n",
    "    seeds=[0,1,2,3,4],          # use more for a paper\n",
    "    outer_iters=3,\n",
    "    entropy_tau=0.02,           # IMPORTANT if you want multimodal x*\n",
    "    disagreement=\"uniform\",     # encourages mixture rather than collapsing\n",
    "    log_level=\"QUIET\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9ebde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"approach2_robust_correlated.py\n",
    "\n",
    "Robust, reduced-log implementation of \"Approach 2\" on a stealth gridworld POMDP,\n",
    "with a FIX for the conceptual mismatch:\n",
    "\n",
    "  Old mismatch: solve a joint distribution x*(i,j) over (robot policy i, sensor policy j)\n",
    "  but then compute best responses against the MARGINALS sigma_R, sigma_S.\n",
    "\n",
    "  Fix here: treat x* as a CORRELATION DEVICE (mediator) and compute BRs against\n",
    "  CONDITIONAL distributions:\n",
    "\n",
    "      q_S(.|i) = X[i,:] / sigma_R[i]   (sensor conditional given robot recommendation i)\n",
    "      q_R(.|j) = X[:,j] / sigma_S[j]   (robot conditional given sensor recommendation j)\n",
    "\n",
    "  Then add the most profitable *deviation* policy (oracle) based on the most violated\n",
    "  conditional recommendation.\n",
    "\n",
    "This makes the PSRO-style expansion step consistent with a correlated-strategy viewpoint.\n",
    "\n",
    "Also included:\n",
    "  - Benchmark harness vs simple heuristics on the same game.\n",
    "  - Saved plots (training curves + bar charts) for presentations.\n",
    "\n",
    "Run (script):\n",
    "  python approach2_robust_correlated.py --solver correlated --outer-iters 3 --save-plots\n",
    "\n",
    "Run (notebook):\n",
    "  main(argv=[\"--solver\",\"both\",\"--outer-iters\",\"3\",\"--save-plots\"])  # ignore ipykernel args\n",
    "\n",
    "Tips to encourage MULTIMODAL x* (useful for your \"mode recovery\"):\n",
    "  --disagreement uniform --entropy-tau 0.02\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import heapq\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Stage-based logger with QUIET/INFO/DEBUG.\"\"\"\n",
    "\n",
    "    LEVELS = {\"QUIET\": 0, \"INFO\": 1, \"DEBUG\": 2}\n",
    "\n",
    "    def __init__(self, level: str = \"INFO\"):\n",
    "        level = level.upper()\n",
    "        if level not in self.LEVELS:\n",
    "            raise ValueError(f\"Unknown log level: {level}. Use QUIET/INFO/DEBUG\")\n",
    "        self.level = level\n",
    "        self.k = self.LEVELS[level]\n",
    "\n",
    "    def banner(self, title: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(\"\" + \"=\" * 100)\n",
    "            print(title)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "    def info(self, msg: str) -> None:\n",
    "        if self.k >= 1:\n",
    "            print(msg)\n",
    "\n",
    "    def debug(self, msg: str) -> None:\n",
    "        if self.k >= 2:\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Types\n",
    "# =============================================================================\n",
    "\n",
    "Action = Tuple[int, int]  # (dx, dy)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Environment (grid + hidden sensor mode + noisy alarm observation)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GridConfig:\n",
    "    width: int\n",
    "    height: int\n",
    "    start: Tuple[int, int]\n",
    "    goal: Tuple[int, int]\n",
    "    obstacles: frozenset\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SensorConfig:\n",
    "    sensors: Tuple[Tuple[int, int], ...]\n",
    "    radius: int\n",
    "    base_p: float\n",
    "    hotspot_p: float\n",
    "\n",
    "\n",
    "class GridWorldStealthEnv:\n",
    "    \"\"\"Grid world with detection risk controlled by a hidden/selected sensor 'mode'.\"\"\"\n",
    "\n",
    "    def __init__(self, grid: GridConfig, sensor_cfg: SensorConfig, fp: float = 0.05, fn: float = 0.10, seed: int = 0):\n",
    "        self.grid = grid\n",
    "        self.sensor_cfg = sensor_cfg\n",
    "        self.fp = float(fp)\n",
    "        self.fn = float(fn)\n",
    "\n",
    "        if not (0.0 <= self.fp <= 1.0 and 0.0 <= self.fn <= 1.0):\n",
    "            raise ValueError(\"fp and fn must be in [0,1].\")\n",
    "        if not (0.0 <= sensor_cfg.base_p <= 1.0 and 0.0 <= sensor_cfg.hotspot_p <= 1.0):\n",
    "            raise ValueError(\"base_p and hotspot_p must be in [0,1].\")\n",
    "        if sensor_cfg.radius < 0:\n",
    "            raise ValueError(\"radius must be >= 0\")\n",
    "        if len(sensor_cfg.sensors) == 0:\n",
    "            raise ValueError(\"Need at least one sensor center.\")\n",
    "\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.reset(sensor_mode=0)\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    def reset(self, sensor_mode: int = 0) -> Dict[str, Any]:\n",
    "        self.t = 0\n",
    "        self.pos = self.grid.start\n",
    "        self.sensor_mode = int(sensor_mode)\n",
    "        self.detected = False\n",
    "        self.total_true_risk = 0.0\n",
    "        return {\"pos\": self.pos, \"t\": self.t}\n",
    "\n",
    "    def in_bounds(self, p: Tuple[int, int]) -> bool:\n",
    "        x, y = p\n",
    "        return 0 <= x < self.grid.width and 0 <= y < self.grid.height\n",
    "\n",
    "    def is_free(self, p: Tuple[int, int]) -> bool:\n",
    "        return self.in_bounds(p) and (p not in self.grid.obstacles)\n",
    "\n",
    "    def true_detection_prob(self, p: Tuple[int, int], mode: int) -> float:\n",
    "        if not (0 <= mode < len(self.sensor_cfg.sensors)):\n",
    "            raise ValueError(f\"mode {mode} out of range\")\n",
    "        sx, sy = self.sensor_cfg.sensors[mode]\n",
    "        x, y = p\n",
    "        d = abs(x - sx) + abs(y - sy)\n",
    "        return self.sensor_cfg.hotspot_p if d <= self.sensor_cfg.radius else self.sensor_cfg.base_p\n",
    "\n",
    "    def observation_prob(self, alarm: int, p_true: float) -> float:\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        if alarm == 1:\n",
    "            return p_alarm\n",
    "        if alarm == 0:\n",
    "            return 1.0 - p_alarm\n",
    "        raise ValueError(\"alarm must be 0 or 1\")\n",
    "\n",
    "    def step(self, a: Action) -> Dict[str, Any]:\n",
    "        if self.detected:\n",
    "            return {\"pos\": self.pos, \"t\": self.t, \"alarm\": 1, \"p_true\": 1.0, \"detected\": True, \"done\": True}\n",
    "\n",
    "        self.t += 1\n",
    "        nx = self.pos[0] + int(a[0])\n",
    "        ny = self.pos[1] + int(a[1])\n",
    "        np_ = (nx, ny)\n",
    "        if self.is_free(np_):\n",
    "            self.pos = np_\n",
    "\n",
    "        p_true = float(self.true_detection_prob(self.pos, self.sensor_mode))\n",
    "        self.total_true_risk += p_true\n",
    "\n",
    "        if self.rng.random() < p_true:\n",
    "            self.detected = True\n",
    "\n",
    "        p_alarm = p_true * (1.0 - self.fn) + (1.0 - p_true) * self.fp\n",
    "        alarm = 1 if (self.rng.random() < p_alarm) else 0\n",
    "\n",
    "        done = self.detected or (self.pos == self.grid.goal) or (self.t >= 200)\n",
    "        return {\"pos\": self.pos, \"t\": self.t, \"alarm\": alarm, \"p_true\": p_true, \"detected\": self.detected, \"done\": done}\n",
    "\n",
    "\n",
    "def build_two_corridor_grid(width: int = 15, height: int = 9) -> GridConfig:\n",
    "    obstacles = set()\n",
    "    wall_x = width // 2\n",
    "    gap_ys = {2, 6}\n",
    "    for y in range(height):\n",
    "        if y not in gap_ys:\n",
    "            obstacles.add((wall_x, y))\n",
    "\n",
    "    start = (1, height - 2)\n",
    "    goal = (width - 2, 1)\n",
    "    if start in obstacles or goal in obstacles:\n",
    "        raise RuntimeError(\"Start/goal blocked unexpectedly\")\n",
    "\n",
    "    return GridConfig(width=width, height=height, start=start, goal=goal, obstacles=frozenset(obstacles))\n",
    "\n",
    "\n",
    "def print_grid_ascii(grid: GridConfig, sensor_cfg: SensorConfig) -> None:\n",
    "    W, H = grid.width, grid.height\n",
    "    obs = set(grid.obstacles)\n",
    "    sens = set(sensor_cfg.sensors)\n",
    "    for y in range(H):\n",
    "        row = []\n",
    "        for x in range(W):\n",
    "            p = (x, y)\n",
    "            if p == grid.start:\n",
    "                row.append(\"R\")\n",
    "            elif p == grid.goal:\n",
    "                row.append(\"G\")\n",
    "            elif p in sens:\n",
    "                row.append(\"S\")\n",
    "            elif p in obs:\n",
    "                row.append(\"#\")\n",
    "            else:\n",
    "                row.append(\".\")\n",
    "        print(\"\".join(row))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Belief over modes\n",
    "# =============================================================================\n",
    "\n",
    "class ModeBelief:\n",
    "    \"\"\"Exact belief over discrete modes m in {0..M-1}.\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, init: Optional[np.ndarray] = None):\n",
    "        self.M = int(M)\n",
    "        if self.M <= 0:\n",
    "            raise ValueError(\"M must be >= 1\")\n",
    "        if init is None:\n",
    "            self.b = np.full(self.M, 1.0 / self.M)\n",
    "        else:\n",
    "            init = np.asarray(init, dtype=float).reshape(-1)\n",
    "            if init.shape != (self.M,):\n",
    "                raise ValueError(\"init shape mismatch\")\n",
    "            if np.any(init < 0):\n",
    "                raise ValueError(\"init must be nonnegative\")\n",
    "            s = float(init.sum())\n",
    "            self.b = init / s if s > 0 else np.full(self.M, 1.0 / self.M)\n",
    "\n",
    "    def update(self, env: GridWorldStealthEnv, alarm: int, pos: Tuple[int, int], eps: float = 1e-12) -> None:\n",
    "        like = np.zeros(self.M, dtype=float)\n",
    "        for m in range(self.M):\n",
    "            p_true = env.true_detection_prob(pos, m)\n",
    "            like[m] = env.observation_prob(alarm, p_true)\n",
    "        post = self.b * like\n",
    "        Z = float(post.sum())\n",
    "        if (not np.isfinite(Z)) or Z < eps:\n",
    "            return\n",
    "        self.b = post / Z\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policies\n",
    "# =============================================================================\n",
    "\n",
    "class RobotPolicy:\n",
    "    name: str = \"RobotPolicy\"\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedPathPolicy(RobotPolicy):\n",
    "    def __init__(self, path: List[Tuple[int, int]], name: str):\n",
    "        if len(path) < 2:\n",
    "            raise ValueError(\"Path must have >=2 states\")\n",
    "        self.path = list(path)\n",
    "        self.name = str(name)\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        try:\n",
    "            self._idx = self.path.index(start_pos)\n",
    "        except ValueError:\n",
    "            self._idx = 0\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        cur = env.pos\n",
    "        if self._idx >= len(self.path) - 1:\n",
    "            return (0, 0)\n",
    "        if cur != self.path[self._idx]:\n",
    "            try:\n",
    "                self._idx = self.path.index(cur, self._idx)\n",
    "            except ValueError:\n",
    "                return (0, 0)\n",
    "        nxt = self.path[self._idx + 1]\n",
    "        dx = int(np.clip(nxt[0] - cur[0], -1, 1))\n",
    "        dy = int(np.clip(nxt[1] - cur[1], -1, 1))\n",
    "        self._idx += 1\n",
    "        return (dx, dy)\n",
    "\n",
    "\n",
    "class RandomPolicy(RobotPolicy):\n",
    "    \"\"\"Reproducible random policy using env.rng.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"R_Random\"):\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        pass\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        candidates: List[Action] = []\n",
    "        x, y = env.pos\n",
    "        for a in [(1, 0), (-1, 0), (0, 1), (0, -1), (0, 0)]:\n",
    "            np_ = (x + a[0], y + a[1])\n",
    "            if env.is_free(np_):\n",
    "                candidates.append(a)\n",
    "        if not candidates:\n",
    "            return (0, 0)\n",
    "        return candidates[int(env.rng.integers(0, len(candidates)))]\n",
    "\n",
    "\n",
    "class OnlineBeliefReplanPolicy(RobotPolicy):\n",
    "    \"\"\"POMDP-ish heuristic: replan each step using risk map induced by current belief b_t.\"\"\"\n",
    "\n",
    "    def __init__(self, env: GridWorldStealthEnv, risk_weight: float = 12.0, name: str = \"R_OnlineBeliefReplan\"):\n",
    "        self.env = env\n",
    "        self.risk_weight = float(risk_weight)\n",
    "        self.name = name\n",
    "        self._cached_next: Optional[Tuple[int, int]] = None\n",
    "\n",
    "    def reset(self, start_pos: Tuple[int, int]) -> None:\n",
    "        self._cached_next = None\n",
    "\n",
    "    def act(self, env: GridWorldStealthEnv, belief: ModeBelief, last_obs: Optional[int]) -> Action:\n",
    "        # Build a risk map from belief over modes.\n",
    "        mode_probs = belief.b\n",
    "\n",
    "        def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "            x, y = to\n",
    "            # expected risk at to under belief\n",
    "            r = 0.0\n",
    "            for m, pm in enumerate(mode_probs):\n",
    "                r += float(pm) * float(env.true_detection_prob(to, m))\n",
    "            return 1.0 + self.risk_weight * r\n",
    "\n",
    "        # Plan from current pos to goal (one-step receding horizon)\n",
    "        try:\n",
    "            path = astar_path(env.grid, env.pos, env.grid.goal, step_cost)\n",
    "            if len(path) < 2:\n",
    "                return (0, 0)\n",
    "            nxt = path[1]\n",
    "            dx = int(np.clip(nxt[0] - env.pos[0], -1, 1))\n",
    "            dy = int(np.clip(nxt[1] - env.pos[1], -1, 1))\n",
    "            return (dx, dy)\n",
    "        except Exception:\n",
    "            return (0, 0)\n",
    "\n",
    "\n",
    "class SensorPolicy:\n",
    "    name: str = \"SensorPolicy\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FixedModeSensorPolicy(SensorPolicy):\n",
    "    def __init__(self, mode: int, name: Optional[str] = None):\n",
    "        self.mode = int(mode)\n",
    "        self.name = name or f\"S_Mode{mode}\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return self.mode\n",
    "\n",
    "\n",
    "class AlternatingSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Simple sensor heuristic for benchmarks: alternate modes 0,1,0,1,...\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, name: str = \"S_Alternate\"):\n",
    "        self.M = int(M)\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(t % self.M)\n",
    "\n",
    "\n",
    "class RandomModeSensorPolicy(SensorPolicy):\n",
    "    \"\"\"Benchmark sensor: random mode each step (uses numpy Generator for reproducibility).\"\"\"\n",
    "\n",
    "    def __init__(self, M: int, seed: int = 0, name: str = \"S_RandomMode\"):\n",
    "        self.M = int(M)\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "        self.name = name\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def select_mode(self, t: int) -> int:\n",
    "        return int(self.rng.integers(0, self.M))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# A* (used for planning)\n",
    "# =============================================================================\n",
    "\n",
    "def astar_path(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    "    max_expansions: int = 250_000,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    if start == goal:\n",
    "        return [start]\n",
    "\n",
    "    def h(p: Tuple[int, int]) -> float:\n",
    "        return abs(p[0] - goal[0]) + abs(p[1] - goal[1])\n",
    "\n",
    "    def neighbors(p: Tuple[int, int]):\n",
    "        x, y = p\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            np_ = (x + dx, y + dy)\n",
    "            if 0 <= np_[0] < grid.width and 0 <= np_[1] < grid.height and np_ not in grid.obstacles:\n",
    "                yield np_\n",
    "\n",
    "    open_heap: List[Tuple[float, float, Tuple[int, int]]] = []\n",
    "    heapq.heappush(open_heap, (h(start), 0.0, start))\n",
    "\n",
    "    came: Dict[Tuple[int, int], Optional[Tuple[int, int]]] = {start: None}\n",
    "    gscore: Dict[Tuple[int, int], float] = {start: 0.0}\n",
    "\n",
    "    expansions = 0\n",
    "    while open_heap:\n",
    "        _, _, cur = heapq.heappop(open_heap)\n",
    "        expansions += 1\n",
    "        if cur == goal:\n",
    "            path: List[Tuple[int, int]] = []\n",
    "            while cur is not None:\n",
    "                path.append(cur)\n",
    "                cur = came[cur]\n",
    "            path.reverse()\n",
    "            return path\n",
    "        if expansions > max_expansions:\n",
    "            raise RuntimeError(\"A* exceeded max expansions\")\n",
    "\n",
    "        for nb in neighbors(cur):\n",
    "            tentative = gscore[cur] + float(step_cost(cur, nb))\n",
    "            if (nb not in gscore) or (tentative < gscore[nb] - 1e-12):\n",
    "                gscore[nb] = tentative\n",
    "                came[nb] = cur\n",
    "                heapq.heappush(open_heap, (tentative + h(nb), tentative, nb))\n",
    "\n",
    "    raise RuntimeError(\"A* failed: unreachable goal\")\n",
    "\n",
    "\n",
    "def astar_via_waypoint(\n",
    "    grid: GridConfig,\n",
    "    start: Tuple[int, int],\n",
    "    waypoint: Tuple[int, int],\n",
    "    goal: Tuple[int, int],\n",
    "    step_cost: Callable[[Tuple[int, int], Tuple[int, int]], float],\n",
    ") -> List[Tuple[int, int]]:\n",
    "    p1 = astar_path(grid, start, waypoint, step_cost)\n",
    "    p2 = astar_path(grid, waypoint, goal, step_cost)\n",
    "    return p1[:-1] + p2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rollouts + payoffs\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EpisodeStats:\n",
    "    steps: int\n",
    "    reached_goal: bool\n",
    "    detected: bool\n",
    "    total_true_risk: float\n",
    "    U_R: float\n",
    "    U_S: float\n",
    "\n",
    "\n",
    "def rollout_episode(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    seed: int,\n",
    "    max_steps: int = 200,\n",
    "    lambda_risk: float = 1.0,\n",
    "    det_penalty: float = 50.0,\n",
    "    sensor_energy_per_step: float = 0.2,\n",
    "    step_debug: bool = False,\n",
    ") -> EpisodeStats:\n",
    "    env.seed(seed)\n",
    "    sensor.reset()\n",
    "    env.reset(sensor_mode=sensor.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)\n",
    "    robot.reset(env.pos)\n",
    "\n",
    "    total_risk = 0.0\n",
    "    last_alarm: Optional[int] = None\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.sensor_mode = sensor.select_mode(env.t)\n",
    "        a = robot.act(env, belief, last_alarm)\n",
    "        out = env.step(a)\n",
    "\n",
    "        total_risk += float(out[\"p_true\"])\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "        last_alarm = int(out[\"alarm\"])\n",
    "\n",
    "        if step_debug:\n",
    "            print(\n",
    "                f\"[Step] t={out['t']:3d} pos={out['pos']} a={a} p_true={out['p_true']:.3f} \"\n",
    "                f\"alarm={out['alarm']} det={out['detected']} done={out['done']} b={belief.b.round(3)}\"\n",
    "            )\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = bool(env.detected)\n",
    "    steps = int(env.t)\n",
    "\n",
    "    cost_R = steps + lambda_risk * total_risk + (det_penalty if detected else 0.0)\n",
    "    U_R = -float(cost_R)\n",
    "\n",
    "    U_S = float((det_penalty if detected else 0.0) + lambda_risk * total_risk - sensor_energy_per_step * steps)\n",
    "\n",
    "    return EpisodeStats(steps=steps, reached_goal=reached_goal, detected=detected, total_true_risk=float(total_risk), U_R=U_R, U_S=U_S)\n",
    "\n",
    "\n",
    "def evaluate_payoffs(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    debug_rollout_pair: Optional[Tuple[int, int]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[Tuple[int, int], Dict[str, float]]]:\n",
    "    m, n = len(robots), len(sensors)\n",
    "    U_R = np.zeros((m, n), dtype=float)\n",
    "    U_S = np.zeros((m, n), dtype=float)\n",
    "    diag: Dict[Tuple[int, int], Dict[str, float]] = {}\n",
    "\n",
    "    log.info(f\"[Eval] Estimating payoffs: m={m}, n={n}, rollouts={rollouts}, base_seed={base_seed}\")\n",
    "\n",
    "    for i, rpol in enumerate(robots):\n",
    "        for j, spol in enumerate(sensors):\n",
    "            step_debug = (debug_rollout_pair == (i, j))\n",
    "\n",
    "            r_list: List[float] = []\n",
    "            s_list: List[float] = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            steps_list: List[int] = []\n",
    "            risk_list: List[float] = []\n",
    "\n",
    "            for k in range(rollouts):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rpol, spol, M_modes=M_modes, seed=seed, step_debug=step_debug)\n",
    "                r_list.append(st.U_R)\n",
    "                s_list.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "                steps_list.append(st.steps)\n",
    "                risk_list.append(st.total_true_risk)\n",
    "\n",
    "                if step_debug:\n",
    "                    step_debug = False  # only show one rollout\n",
    "\n",
    "            U_R[i, j] = float(np.mean(r_list))\n",
    "            U_S[i, j] = float(np.mean(s_list))\n",
    "\n",
    "            diag[(i, j)] = {\n",
    "                \"det_rate\": det / rollouts,\n",
    "                \"goal_rate\": goal / rollouts,\n",
    "                \"mean_steps\": float(np.mean(steps_list)),\n",
    "                \"mean_risk\": float(np.mean(risk_list)),\n",
    "                \"std_UR\": float(np.std(r_list)),\n",
    "                \"std_US\": float(np.std(s_list)),\n",
    "            }\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(\"[Eval] Compact payoff summary:\")\n",
    "        for i, rpol in enumerate(robots):\n",
    "            for j, spol in enumerate(sensors):\n",
    "                d = diag[(i, j)]\n",
    "                log.info(\n",
    "                    f\"  (R{i}:{rpol.name}, S{j}:{spol.name}) \"\n",
    "                    f\"UR={U_R[i,j]:8.3f}±{d['std_UR']:.2f} | \"\n",
    "                    f\"US={U_S[i,j]:8.3f}±{d['std_US']:.2f} | \"\n",
    "                    f\"det%={100*d['det_rate']:5.1f} goal%={100*d['goal_rate']:5.1f} \"\n",
    "                    f\"steps={d['mean_steps']:.1f} risk={d['mean_risk']:.2f}\"\n",
    "                )\n",
    "\n",
    "    return U_R, U_S, diag\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NBS solver (with optional entropy regularization)\n",
    "# =============================================================================\n",
    "\n",
    "def project_simplex(v: np.ndarray, z: float = 1.0) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=float).reshape(-1)\n",
    "    if v.size == 0:\n",
    "        raise ValueError(\"Empty vector\")\n",
    "    if z <= 0:\n",
    "        raise ValueError(\"z must be > 0\")\n",
    "\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, v.size + 1) > (cssv - z))[0]\n",
    "    if rho.size == 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    rho = int(rho[-1])\n",
    "    theta = (cssv[rho] - z) / (rho + 1.0)\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    s = float(w.sum())\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full_like(v, z / v.size)\n",
    "    return w * (z / s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NBSResult:\n",
    "    x: np.ndarray\n",
    "    obj: float\n",
    "    gains: Tuple[float, float]\n",
    "    support: int\n",
    "\n",
    "\n",
    "def solve_nbs(\n",
    "    uR: np.ndarray,\n",
    "    uS: np.ndarray,\n",
    "    log: Logger,\n",
    "    max_iters: int = 400,\n",
    "    alpha: float = 0.5,\n",
    "    tol_l1: float = 1e-6,\n",
    "    kappa: float = 1e-6,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    ") -> NBSResult:\n",
    "    uR = np.asarray(uR, dtype=float).reshape(-1)\n",
    "    uS = np.asarray(uS, dtype=float).reshape(-1)\n",
    "    if uR.shape != uS.shape:\n",
    "        raise ValueError(\"uR and uS must have same shape\")\n",
    "    d = uR.size\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Need >=2 joint actions\")\n",
    "\n",
    "    unif = np.full(d, 1.0 / d)\n",
    "\n",
    "    disagreement = disagreement.lower().strip()\n",
    "    if disagreement == \"minminus\":\n",
    "        dR = float(np.min(uR) - 1.0)\n",
    "        dS = float(np.min(uS) - 1.0)\n",
    "    elif disagreement == \"uniform\":\n",
    "        dR = float(uR @ unif)\n",
    "        dS = float(uS @ unif)\n",
    "    else:\n",
    "        raise ValueError(\"disagreement must be 'minminus' or 'uniform'\")\n",
    "\n",
    "    x = unif.copy()\n",
    "\n",
    "    def gains(xv: np.ndarray) -> Tuple[float, float]:\n",
    "        return float(uR @ xv - dR), float(uS @ xv - dS)\n",
    "\n",
    "    def entropy(xv: np.ndarray) -> float:\n",
    "        xx = np.clip(xv, 1e-12, 1.0)\n",
    "        return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "    def obj(xv: np.ndarray) -> float:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        return float(np.log(gR) + np.log(gS) + entropy_tau * entropy(xv))\n",
    "\n",
    "    def grad(xv: np.ndarray) -> np.ndarray:\n",
    "        gR, gS = gains(xv)\n",
    "        gR = max(gR, kappa)\n",
    "        gS = max(gS, kappa)\n",
    "        g = (uR / gR) + (uS / gS)\n",
    "        if entropy_tau > 0:\n",
    "            xx = np.clip(xv, 1e-12, 1.0)\n",
    "            g += entropy_tau * (-(np.log(xx) + 1.0))\n",
    "        return g\n",
    "\n",
    "    last = obj(x)\n",
    "    log.info(f\"[NBS] d={d} disagreement=({dR:.3f},{dS:.3f}) entropy_tau={entropy_tau:.3g}\")\n",
    "\n",
    "    for t in range(1, max_iters + 1):\n",
    "        g = grad(x)\n",
    "        a = alpha\n",
    "        improved = False\n",
    "        for _ in range(30):\n",
    "            x_new = project_simplex(x + a * g)\n",
    "            new_obj = obj(x_new)\n",
    "            if new_obj >= last - 1e-12:\n",
    "                improved = True\n",
    "                break\n",
    "            a *= 0.5\n",
    "            if a < 1e-6:\n",
    "                break\n",
    "        if not improved:\n",
    "            break\n",
    "\n",
    "        delta = float(np.linalg.norm(x_new - x, ord=1))\n",
    "        x = x_new\n",
    "        last = new_obj\n",
    "\n",
    "        if log.k >= 2 and (t <= 5 or t % 25 == 0):\n",
    "            gR, gS = gains(x)\n",
    "            top = np.argsort(-x)[:5]\n",
    "            top_str = \", \".join([f\"{i}:{x[i]:.3f}\" for i in top])\n",
    "            log.debug(f\"[NBS][it={t:3d}] obj={last:.6f} gains=({gR:.3f},{gS:.3f}) L1={delta:.2e} top={top_str}\")\n",
    "\n",
    "        if delta < tol_l1:\n",
    "            break\n",
    "\n",
    "    gR, gS = gains(x)\n",
    "    support = int(np.sum(x > 1e-6))\n",
    "    log.info(f\"[NBS] done: obj={last:.6f} gains=({gR:.3f},{gS:.3f}) support={support}/{d}\")\n",
    "\n",
    "    return NBSResult(x=x, obj=float(last), gains=(float(gR), float(gS)), support=support)\n",
    "\n",
    "\n",
    "def joint_to_matrix(x: np.ndarray, m: int, n: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if x.size != m * n:\n",
    "        raise ValueError(\"x size mismatch\")\n",
    "    X = x.reshape((m, n))\n",
    "    s = float(X.sum())\n",
    "    if not np.isfinite(s) or abs(s - 1.0) > 1e-6:\n",
    "        # Renormalize defensively\n",
    "        X = X / max(s, 1e-12)\n",
    "    return X\n",
    "\n",
    "\n",
    "def marginals_from_joint(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    sigma_R = X.sum(axis=1)\n",
    "    sigma_S = X.sum(axis=0)\n",
    "    if sigma_R.sum() > 0:\n",
    "        sigma_R = sigma_R / sigma_R.sum()\n",
    "    if sigma_S.sum() > 0:\n",
    "        sigma_S = sigma_S / sigma_S.sum()\n",
    "    return sigma_R, sigma_S\n",
    "\n",
    "\n",
    "def entropy_of_joint(X: np.ndarray) -> float:\n",
    "    xx = np.clip(X.reshape(-1), 1e-12, 1.0)\n",
    "    return float(-np.sum(xx * np.log(xx)))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Best responses (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_expected_risk_map_from_policy_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi: np.ndarray,\n",
    "    M_modes: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Expected p_true(cell) under mixture over sensor POLICIES (not modes).\n",
    "\n",
    "    For each sensor policy j, we use its mode at t=0 as its defining mode.\n",
    "    (This matches FixedModeSensorPolicy exactly.)\n",
    "    \"\"\"\n",
    "    pi = np.asarray(pi, dtype=float).reshape(-1)\n",
    "    if pi.size != len(sensors):\n",
    "        raise ValueError(\"mixture length mismatch\")\n",
    "\n",
    "    mode_probs = np.zeros(M_modes, dtype=float)\n",
    "    for j, sp in enumerate(sensors):\n",
    "        m = int(sp.select_mode(0))\n",
    "        if not (0 <= m < M_modes):\n",
    "            raise ValueError(\"invalid sensor mode\")\n",
    "        mode_probs[m] += float(pi[j])\n",
    "    if mode_probs.sum() > 0:\n",
    "        mode_probs = mode_probs / mode_probs.sum()\n",
    "\n",
    "    H, W = env.grid.height, env.grid.width\n",
    "    risk = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in env.grid.obstacles:\n",
    "                risk[y, x] = np.nan\n",
    "                continue\n",
    "            val = 0.0\n",
    "            for m in range(M_modes):\n",
    "                val += float(mode_probs[m]) * float(env.true_detection_prob((x, y), m))\n",
    "            risk[y, x] = float(val)\n",
    "\n",
    "    return risk\n",
    "\n",
    "\n",
    "def plan_risk_weighted_path(env: GridWorldStealthEnv, risk_map: np.ndarray, risk_weight: float) -> List[Tuple[int, int]]:\n",
    "    def step_cost(frm: Tuple[int, int], to: Tuple[int, int]) -> float:\n",
    "        x, y = to\n",
    "        r = risk_map[y, x]\n",
    "        if not np.isfinite(r):\n",
    "            return 1e9\n",
    "        return 1.0 + float(risk_weight) * float(r)\n",
    "\n",
    "    return astar_path(env.grid, env.grid.start, env.grid.goal, step_cost)\n",
    "\n",
    "\n",
    "def robot_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    sensors: List[SensorPolicy],\n",
    "    pi_S: np.ndarray,\n",
    "    robots: List[RobotPolicy],\n",
    "    M_modes: int,\n",
    "    risk_weight: float,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> RobotPolicy:\n",
    "    risk = compute_expected_risk_map_from_policy_mixture(env, sensors, pi_S, M_modes=M_modes)\n",
    "    try:\n",
    "        path = plan_risk_weighted_path(env, risk, risk_weight=risk_weight)\n",
    "    except Exception as e:\n",
    "        log.info(f\"[RobotBR] WARNING A* failed ({tag}): {e}\")\n",
    "        return robots[0]\n",
    "\n",
    "    path_tuple = tuple(path)\n",
    "    for p in robots:\n",
    "        if isinstance(p, FixedPathPolicy) and tuple(p.path) == path_tuple:\n",
    "            log.info(f\"[RobotBR] ({tag}) BR path already exists: {p.name}\")\n",
    "            return p\n",
    "\n",
    "    newp = FixedPathPolicy(path, name=f\"R_BR_{tag}_w{risk_weight:.1f}_len{len(path)}\")\n",
    "    log.info(f\"[RobotBR] ({tag}) Added new robot policy: {newp.name}\")\n",
    "    return newp\n",
    "\n",
    "\n",
    "def sensor_best_response_to_mixture(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    pi_R: np.ndarray,\n",
    "    candidate_modes: List[int],\n",
    "    M_modes: int,\n",
    "    rollouts: int,\n",
    "    base_seed: int,\n",
    "    log: Logger,\n",
    "    tag: str,\n",
    ") -> FixedModeSensorPolicy:\n",
    "    \"\"\"Sensor best response with common-random-numbers (CRN).\n",
    "\n",
    "    Why CRN matters: payoff variance is large (detection is a rare/threshold event).\n",
    "    If each candidate mode is evaluated on different random rollouts, you can pick\n",
    "    the wrong 'best mode' by noise, which then breaks the PSRO expansion logic.\n",
    "\n",
    "    Fix: reuse the same sampled robot indices AND the same episode seeds across all\n",
    "    candidate modes.\n",
    "    \"\"\"\n",
    "    pi_R = np.asarray(pi_R, dtype=float).reshape(-1)\n",
    "    if pi_R.size != len(robots):\n",
    "        raise ValueError(\"pi_R length mismatch\")\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    # Same robot-index samples for every mode\n",
    "    robot_idxs = rng.choice(len(robots), size=rollouts, p=pi_R, replace=True)\n",
    "\n",
    "    # Same episode seeds for every mode (common random numbers)\n",
    "    seeds = (int(base_seed) + np.arange(rollouts)).astype(int)\n",
    "\n",
    "    best_mode: Optional[int] = None\n",
    "    best_val = -1e18\n",
    "\n",
    "    for mode in candidate_modes:\n",
    "        if not (0 <= mode < M_modes):\n",
    "            continue\n",
    "        sp = FixedModeSensorPolicy(mode, name=f\"S_BR_{tag}_Mode{mode}\")\n",
    "\n",
    "        vals: List[float] = []\n",
    "        for k in range(rollouts):\n",
    "            rp = robots[int(robot_idxs[k])]\n",
    "            st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=int(seeds[k]))\n",
    "            vals.append(st.U_S)\n",
    "\n",
    "        mean_u = float(np.mean(vals))\n",
    "        if log.k >= 2:\n",
    "            log.debug(f\"[SensorBR] ({tag}) mode={mode} E[US]={mean_u:.3f} std={float(np.std(vals)):.2f}\")\n",
    "\n",
    "        if mean_u > best_val:\n",
    "            best_val = mean_u\n",
    "            best_mode = mode\n",
    "\n",
    "    if best_mode is None:\n",
    "        raise RuntimeError(\"No valid sensor BR mode found\")\n",
    "\n",
    "    log.info(f\"[SensorBR] ({tag}) Best mode={best_mode} E[US]={best_val:.3f}\")\n",
    "    return FixedModeSensorPolicy(best_mode, name=f\"S_BR_{tag}_Mode{best_mode}\")\n",
    "\n",
    "\n",
    "def conditional_sensor_given_robot(X: np.ndarray, i: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    row = np.asarray(X[i, :], dtype=float)\n",
    "    s = float(row.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(row, 1.0 / row.size)\n",
    "    return row / s\n",
    "\n",
    "\n",
    "def conditional_robot_given_sensor(X: np.ndarray, j: int, eps: float = 1e-12) -> np.ndarray:\n",
    "    col = np.asarray(X[:, j], dtype=float)\n",
    "    s = float(col.sum())\n",
    "    if s <= eps:\n",
    "        return np.full_like(col, 1.0 / col.size)\n",
    "    return col / s\n",
    "\n",
    "\n",
    "def compute_ce_regrets(U_R: np.ndarray, U_S: np.ndarray, X: np.ndarray, eps: float = 1e-12) -> Dict[str, float]:\n",
    "    \"\"\"Conditional recommendation regrets (CE-style) computed on current meta-game.\n",
    "\n",
    "    For robot (given recommendation i):\n",
    "        regret_R(i) = max_{i'} E_{j~q(.|i)}[U_R(i',j) - U_R(i,j)]\n",
    "\n",
    "    For sensor (given recommendation j):\n",
    "        regret_S(j) = max_{j'} E_{i~q(.|j)}[U_S(i,j') - U_S(i,j)]\n",
    "\n",
    "    Returns max and average regrets.\n",
    "    \"\"\"\n",
    "    m, n = U_R.shape\n",
    "    assert U_S.shape == (m, n)\n",
    "    assert X.shape == (m, n)\n",
    "\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "    reg_R = []\n",
    "    for i in range(m):\n",
    "        if sigma_R[i] <= eps:\n",
    "            continue\n",
    "        q = conditional_sensor_given_robot(X, i, eps=eps)\n",
    "        rec = float(np.dot(q, U_R[i, :]))\n",
    "        best = rec\n",
    "        for ip in range(m):\n",
    "            val = float(np.dot(q, U_R[ip, :]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_R.append(best - rec)\n",
    "\n",
    "    reg_S = []\n",
    "    for j in range(n):\n",
    "        if sigma_S[j] <= eps:\n",
    "            continue\n",
    "        q = conditional_robot_given_sensor(X, j, eps=eps)\n",
    "        rec = float(np.dot(q, U_S[:, j]))\n",
    "        best = rec\n",
    "        for jp in range(n):\n",
    "            val = float(np.dot(q, U_S[:, jp]))\n",
    "            if val > best:\n",
    "                best = val\n",
    "        reg_S.append(best - rec)\n",
    "\n",
    "    return {\n",
    "        \"max_regret_R\": float(max(reg_R) if reg_R else 0.0),\n",
    "        \"max_regret_S\": float(max(reg_S) if reg_S else 0.0),\n",
    "        \"mean_regret_R\": float(np.mean(reg_R) if reg_R else 0.0),\n",
    "        \"mean_regret_S\": float(np.mean(reg_S) if reg_S else 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def find_sensor_by_mode(pols: List[SensorPolicy], mode: int) -> Optional[FixedModeSensorPolicy]:\n",
    "    for p in pols:\n",
    "        if isinstance(p, FixedModeSensorPolicy) and p.mode == mode:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policy initialization + evaluator for joint strategy\n",
    "# =============================================================================\n",
    "\n",
    "def build_initial_policies(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "    ]\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StrategyEval:\n",
    "    mean_U_R: float\n",
    "    mean_U_S: float\n",
    "    det_rate: float\n",
    "    goal_rate: float\n",
    "    mean_steps: float\n",
    "    mean_risk: float\n",
    "\n",
    "\n",
    "def evaluate_joint_strategy(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sensors: List[SensorPolicy],\n",
    "    X: np.ndarray,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    m, n = X.shape\n",
    "    probs = X.reshape(-1)\n",
    "    probs = probs / max(float(probs.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR = []\n",
    "    US = []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        idx = int(rng.choice(m * n, p=probs))\n",
    "        i, j = np.unravel_index(idx, (m, n))\n",
    "        seed = base_seed + k\n",
    "        st = rollout_episode(env, robots[i], sensors[j], M_modes=M_modes, seed=seed)\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training loop (marginal vs correlated)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainHistoryRow:\n",
    "    outer_iter: int\n",
    "    m: int\n",
    "    n: int\n",
    "    nbs_obj: float\n",
    "    entropy_X: float\n",
    "    max_regret_R: float\n",
    "    max_regret_S: float\n",
    "    selfplay_UR: float\n",
    "    selfplay_US: float\n",
    "    selfplay_det: float\n",
    "    selfplay_goal: float\n",
    "    seconds: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    solver: str\n",
    "    env: GridWorldStealthEnv\n",
    "    robots: List[RobotPolicy]\n",
    "    sensors: List[SensorPolicy]\n",
    "    X: np.ndarray\n",
    "    history: List[TrainHistoryRow]\n",
    "\n",
    "\n",
    "def run_training(env: GridWorldStealthEnv, args: argparse.Namespace, solver: str, log: Logger) -> TrainResult:\n",
    "    t0_all = time.time()\n",
    "\n",
    "    grid = env.grid\n",
    "    sensor_cfg = env.sensor_cfg\n",
    "    M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "    robots, sensors = build_initial_policies(env, M_modes=M_modes)\n",
    "\n",
    "    # Optional: reduce initial set if you want smaller games.\n",
    "    # (We keep it as-is for benchmarks.)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.info(f\"[{solver}] Initial robots: \" + \", \".join([p.name for p in robots]))\n",
    "        log.info(f\"[{solver}] Initial sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "\n",
    "    debug_pair = None\n",
    "    if args.debug_rollout_pair:\n",
    "        parts = args.debug_rollout_pair.split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            debug_pair = (int(parts[0]), int(parts[1]))\n",
    "            log.info(f\"[{solver}] Will print one step-by-step rollout for pair {debug_pair} (only once).\")\n",
    "\n",
    "    history: List[TrainHistoryRow] = []\n",
    "    X = None\n",
    "\n",
    "    for it in range(1, args.outer_iters + 1):\n",
    "        t0 = time.time()\n",
    "        log.banner(f\"[{solver}] Outer iter {it}/{args.outer_iters}\")\n",
    "\n",
    "        U_R, U_S, _diag = evaluate_payoffs(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            M_modes=M_modes,\n",
    "            rollouts=args.rollouts_payoff,\n",
    "            base_seed=1000 + 100 * it,\n",
    "            log=log,\n",
    "            debug_rollout_pair=debug_pair,\n",
    "        )\n",
    "        debug_pair = None\n",
    "\n",
    "        # Solve NBS over joint actions\n",
    "        uR = U_R.reshape(-1)\n",
    "        uS = U_S.reshape(-1)\n",
    "\n",
    "        nbs = solve_nbs(\n",
    "            uR,\n",
    "            uS,\n",
    "            log=log,\n",
    "            disagreement=args.disagreement,\n",
    "            entropy_tau=args.entropy_tau,\n",
    "        )\n",
    "\n",
    "        m, n = U_R.shape\n",
    "        X = joint_to_matrix(nbs.x, m, n)\n",
    "        sigma_R, sigma_S = marginals_from_joint(X)\n",
    "\n",
    "        # Print top joint actions\n",
    "        top = np.argsort(-X.reshape(-1))[:min(5, X.size)]\n",
    "        log.info(f\"[{solver}] Top joint actions:\")\n",
    "        for k, idx in enumerate(top, start=1):\n",
    "            i, j = np.unravel_index(int(idx), (m, n))\n",
    "            log.info(f\"  #{k}: (R{i}:{robots[i].name}, S{j}:{sensors[j].name}) prob={X[i,j]:.4f}\")\n",
    "        log.info(f\"[{solver}] sigma_R={sigma_R.round(3)}\")\n",
    "        log.info(f\"[{solver}] sigma_S={sigma_S.round(3)}\")\n",
    "\n",
    "        # Stability diagnostics\n",
    "        regrets = compute_ce_regrets(U_R, U_S, X)\n",
    "        ent = entropy_of_joint(X)\n",
    "\n",
    "        # Self-play evaluation under joint X\n",
    "        sp = evaluate_joint_strategy(\n",
    "            env,\n",
    "            robots,\n",
    "            sensors,\n",
    "            X,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.eval_episodes,\n",
    "            base_seed=9000 + 100 * it,\n",
    "        )\n",
    "\n",
    "        log.info(\n",
    "            f\"[{solver}] CE-regrets: maxR={regrets['max_regret_R']:.3f} maxS={regrets['max_regret_S']:.3f} | \"\n",
    "            f\"SelfPlay: UR={sp.mean_U_R:.2f} US={sp.mean_U_S:.2f} det%={100*sp.det_rate:.1f} goal%={100*sp.goal_rate:.1f} | \"\n",
    "            f\"H(X)={ent:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Best-response expansion\n",
    "        if solver == \"marginal\":\n",
    "            br_r = robot_best_response_to_mixture(\n",
    "                env,\n",
    "                sensors,\n",
    "                pi_S=sigma_S,\n",
    "                robots=robots,\n",
    "                M_modes=M_modes,\n",
    "                risk_weight=args.risk_weight_br,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "            br_s = sensor_best_response_to_mixture(\n",
    "                env,\n",
    "                robots,\n",
    "                pi_R=sigma_R,\n",
    "                candidate_modes=list(range(M_modes)),\n",
    "                M_modes=M_modes,\n",
    "                rollouts=args.rollouts_br,\n",
    "                base_seed=2000 + 100 * it,\n",
    "                log=log,\n",
    "                tag=\"Marginal\",\n",
    "            )\n",
    "\n",
    "        elif solver == \"correlated\":\n",
    "            # FIX: choose conditional mixtures q(.|i) and q(.|j)\n",
    "            # We only check top-K recommendations to keep it tractable.\n",
    "            topK = max(1, int(args.cond_top_k))\n",
    "\n",
    "            # Robot: check the top-K robot recommendations by sigma_R\n",
    "            cand_i = [int(i) for i in np.argsort(-sigma_R) if sigma_R[int(i)] > 1e-8][:topK]\n",
    "            if not cand_i:\n",
    "                cand_i = [int(i) for i in np.argsort(-sigma_R)[:topK]]\n",
    "            best_gain = 0.0\n",
    "            br_r = robots[0]\n",
    "\n",
    "            for i in cand_i:\n",
    "                qS = conditional_sensor_given_robot(X, int(i))\n",
    "                tag = f\"Cond_i{i}\"\n",
    "                pol = robot_best_response_to_mixture(\n",
    "                    env,\n",
    "                    sensors,\n",
    "                    pi_S=qS,\n",
    "                    robots=robots,\n",
    "                    M_modes=M_modes,\n",
    "                    risk_weight=args.risk_weight_br,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement: E_q[U_R(pol,j)] - E_q[U_R(i,j)]\n",
    "                # If pol is already in set, its payoff exists in U_R row of that policy.\n",
    "                # Otherwise we simulate pol against each sensor policy j.\n",
    "                if pol in robots:\n",
    "                    ip = robots.index(pol)\n",
    "                    dev = float(np.dot(qS, U_R[ip, :]))\n",
    "                else:\n",
    "                    # simulate quickly vs each sensor policy\n",
    "                    dev_vals = []\n",
    "                    for j in range(n):\n",
    "                        vals = []\n",
    "                        for kk in range(args.br_eval_rollouts):\n",
    "                            seed = 777000 + 1000 * it + 100 * i + 10 * j + kk\n",
    "                            st = rollout_episode(env, pol, sensors[j], M_modes=M_modes, seed=seed)\n",
    "                            vals.append(st.U_R)\n",
    "                        dev_vals.append(float(np.mean(vals)))\n",
    "                    dev = float(np.dot(qS, np.asarray(dev_vals)))\n",
    "\n",
    "                rec = float(np.dot(qS, U_R[i, :]))\n",
    "                gain = dev - rec\n",
    "                if gain > best_gain + 1e-9:\n",
    "                    best_gain = gain\n",
    "                    br_r = pol\n",
    "\n",
    "            if best_gain > args.add_threshold:\n",
    "                if br_r in robots:\n",
    "                    log.info(f\"[{solver}] Best robot deviation already in set; est_gain={best_gain:.3f}\")\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding robot deviation; est_gain={best_gain:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No robot deviation above threshold (best_gain={best_gain:.3f}).\")\n",
    "\n",
    "            # Sensor: check top-K sensor recommendations by sigma_S\n",
    "            cand_j = [int(j) for j in np.argsort(-sigma_S) if sigma_S[int(j)] > 1e-8][:topK]\n",
    "            if not cand_j:\n",
    "                cand_j = [int(j) for j in np.argsort(-sigma_S)[:topK]]\n",
    "            best_gain_s = 0.0\n",
    "            br_s = FixedModeSensorPolicy(0, name=\"S_dummy\")\n",
    "\n",
    "            for j in cand_j:\n",
    "                qR = conditional_robot_given_sensor(X, int(j))\n",
    "                tag = f\"Cond_j{j}\"\n",
    "                polS = sensor_best_response_to_mixture(\n",
    "                    env,\n",
    "                    robots,\n",
    "                    pi_R=qR,\n",
    "                    candidate_modes=list(range(M_modes)),\n",
    "                    M_modes=M_modes,\n",
    "                    rollouts=args.rollouts_br,\n",
    "                    base_seed=333000 + 1000 * it + 10 * j,\n",
    "                    log=log,\n",
    "                    tag=tag,\n",
    "                )\n",
    "\n",
    "                # Estimate conditional improvement for sensor\n",
    "                # rec under recommendation j is E_q[U_S(i,j)]\n",
    "                recS = float(np.dot(qR, U_S[:, j]))\n",
    "\n",
    "                # dev under mode polS.mode: if already present, use its column.\n",
    "                existing_col = None\n",
    "                for jj, spj in enumerate(sensors):\n",
    "                    if isinstance(spj, FixedModeSensorPolicy) and spj.mode == polS.mode:\n",
    "                        existing_col = jj\n",
    "                        break\n",
    "\n",
    "                if existing_col is not None:\n",
    "                    devS = float(np.dot(qR, U_S[:, existing_col]))\n",
    "                else:\n",
    "                    vals = []\n",
    "                    for kk in range(args.br_eval_rollouts):\n",
    "                        i_samp = int(np.random.default_rng(444 + kk).choice(len(robots), p=qR))\n",
    "                        seed = 888000 + 1000 * it + 10 * j + kk\n",
    "                        st = rollout_episode(env, robots[i_samp], polS, M_modes=M_modes, seed=seed)\n",
    "                        vals.append(st.U_S)\n",
    "                    devS = float(np.mean(vals))\n",
    "\n",
    "                gainS = devS - recS\n",
    "                if gainS > best_gain_s + 1e-9:\n",
    "                    best_gain_s = gainS\n",
    "                    br_s = polS\n",
    "\n",
    "            if best_gain_s > args.add_threshold:\n",
    "                if (isinstance(br_s, FixedModeSensorPolicy)) and (find_sensor_by_mode(sensors, br_s.mode) is not None):\n",
    "                    log.info(f\"[{solver}] Best sensor deviation already in set (mode={br_s.mode}); est_gain={best_gain_s:.3f}\")\n",
    "                    br_s = None\n",
    "                else:\n",
    "                    log.info(f\"[{solver}] Adding sensor deviation; est_gain={best_gain_s:.3f}\")\n",
    "            else:\n",
    "                log.info(f\"[{solver}] No sensor deviation above threshold (best_gain={best_gain_s:.3f}).\")\n",
    "                br_s = None\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"solver must be marginal or correlated\")\n",
    "\n",
    "        # Add to sets (dedupe)\n",
    "        if br_r not in robots:\n",
    "            robots.append(br_r)\n",
    "\n",
    "        if isinstance(br_s, FixedModeSensorPolicy):\n",
    "            if find_sensor_by_mode(sensors, br_s.mode) is None:\n",
    "                sensors.append(br_s)\n",
    "            else:\n",
    "                log.info(f\"[{solver}] Sensor mode {br_s.mode} already present; not adding duplicate.\")\n",
    "\n",
    "        seconds = float(time.time() - t0)\n",
    "        history.append(\n",
    "            TrainHistoryRow(\n",
    "                outer_iter=it,\n",
    "                m=len(robots),\n",
    "                n=len(sensors),\n",
    "                nbs_obj=float(nbs.obj),\n",
    "                entropy_X=float(ent),\n",
    "                max_regret_R=float(regrets[\"max_regret_R\"]),\n",
    "                max_regret_S=float(regrets[\"max_regret_S\"]),\n",
    "                selfplay_UR=float(sp.mean_U_R),\n",
    "                selfplay_US=float(sp.mean_U_S),\n",
    "                selfplay_det=float(sp.det_rate),\n",
    "                selfplay_goal=float(sp.goal_rate),\n",
    "                seconds=seconds,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        log.info(f\"[{solver}] Sets: |Pi_R|={len(robots)} |Pi_S|={len(sensors)} | iter_seconds={seconds:.2f}\")\n",
    "\n",
    "    if X is None:\n",
    "        raise RuntimeError(\"Training produced no X\")\n",
    "\n",
    "    log.banner(f\"[{solver}] Finished\")\n",
    "    log.info(f\"[{solver}] Final robots: \" + \", \".join([p.name for p in robots]))\n",
    "    log.info(f\"[{solver}] Final sensors: \" + \", \".join([p.name for p in sensors]))\n",
    "    log.info(f\"[{solver}] Total time: {time.time()-t0_all:.2f}s\")\n",
    "\n",
    "    return TrainResult(solver=solver, env=env, robots=robots, sensors=sensors, X=X, history=history)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Benchmarks + plotting\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BenchCell:\n",
    "    UR: float\n",
    "    US: float\n",
    "    det: float\n",
    "    goal: float\n",
    "\n",
    "\n",
    "def run_policy_matrix_benchmark(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot_methods: List[RobotPolicy],\n",
    "    sensor_methods: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[Tuple[int, int], BenchCell]:\n",
    "    res: Dict[Tuple[int, int], BenchCell] = {}\n",
    "    for i, rp in enumerate(robot_methods):\n",
    "        for j, sp in enumerate(sensor_methods):\n",
    "            UR = []\n",
    "            US = []\n",
    "            det = 0\n",
    "            goal = 0\n",
    "            for k in range(episodes):\n",
    "                seed = base_seed + 100000 * i + 1000 * j + k\n",
    "                st = rollout_episode(env, rp, sp, M_modes=M_modes, seed=seed)\n",
    "                UR.append(st.U_R)\n",
    "                US.append(st.U_S)\n",
    "                det += int(st.detected)\n",
    "                goal += int(st.reached_goal)\n",
    "            res[(i, j)] = BenchCell(\n",
    "                UR=float(np.mean(UR)),\n",
    "                US=float(np.mean(US)),\n",
    "                det=float(det / episodes),\n",
    "                goal=float(goal / episodes),\n",
    "            )\n",
    "    return res\n",
    "\n",
    "\n",
    "def safe_makedirs(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_history_csv(hist: List[TrainHistoryRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    fields = list(TrainHistoryRow.__annotations__.keys())\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fields)\n",
    "        w.writeheader()\n",
    "        for row in hist:\n",
    "            w.writerow({k: getattr(row, k) for k in fields})\n",
    "\n",
    "\n",
    "def plot_training_curves(results: List[TrainResult], outdir: str) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # 1) NBS objective\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.nbs_obj for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"NBS objective\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_nbs_obj.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 2) Max conditional regrets\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_R for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: maxRegret_R\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.max_regret_S for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: maxRegret_S\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Max conditional regret\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_max_regrets.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 3) Entropy of X\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.entropy_X for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=r.solver)\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Entropy H(X)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_entropy_X.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 4) Self-play outcomes\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_UR for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"{r.solver}: UR\")\n",
    "    for r in results:\n",
    "        xs = [h.outer_iter for h in r.history]\n",
    "        ys = [h.selfplay_US for h in r.history]\n",
    "        plt.plot(xs, ys, marker=\"x\", linestyle=\"--\", label=f\"{r.solver}: US\")\n",
    "    plt.xlabel(\"Outer iteration\")\n",
    "    plt.ylabel(\"Expected utility under X\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"curve_selfplay_utils.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_benchmark_bars(\n",
    "    robot_names: List[str],\n",
    "    sensor_names: List[str],\n",
    "    bench: Dict[Tuple[int, int], BenchCell],\n",
    "    outdir: str,\n",
    ") -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # For each sensor, bar chart of robot UR\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].UR for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Robot utility\")\n",
    "        plt.title(f\"Robot utility vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_UR_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # For each sensor, bar chart of robot goal rate\n",
    "    for j, sname in enumerate(sensor_names):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        vals = [bench[(i, j)].goal for i in range(len(robot_names))]\n",
    "        plt.bar(range(len(robot_names)), vals)\n",
    "        plt.xticks(range(len(robot_names)), robot_names, rotation=35, ha=\"right\")\n",
    "        plt.ylabel(\"Goal rate\")\n",
    "        plt.title(f\"Robot goal rate vs sensor={sname}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"bench_goal_vs_{sname}.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main pipeline wrapper\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(args: argparse.Namespace) -> None:\n",
    "    log = Logger(args.log_level)\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "\n",
    "    if log.k >= 1:\n",
    "        log.banner(\"[PIPELINE] Stealth grid game\")\n",
    "        log.info(f\"Grid: {grid.width}x{grid.height} start={grid.start} goal={grid.goal} obstacles={len(grid.obstacles)}\")\n",
    "        log.info(f\"Sensors: {sensor_cfg.sensors} radius={sensor_cfg.radius} base_p={sensor_cfg.base_p} hotspot_p={sensor_cfg.hotspot_p}\")\n",
    "        if log.k >= 2:\n",
    "            log.debug(\"ASCII map:\")\n",
    "            print_grid_ascii(grid, sensor_cfg)\n",
    "\n",
    "    solvers: List[str]\n",
    "    if args.solver == \"both\":\n",
    "        solvers = [\"marginal\", \"correlated\"]\n",
    "    else:\n",
    "        solvers = [args.solver]\n",
    "\n",
    "    results: List[TrainResult] = []\n",
    "    for s in solvers:\n",
    "        # Use a fresh env copy per solver (to avoid RNG coupling)\n",
    "        env_s = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=args.seed)\n",
    "        results.append(run_training(env_s, args, solver=s, log=log))\n",
    "\n",
    "    if args.results_dir:\n",
    "        safe_makedirs(args.results_dir)\n",
    "        for r in results:\n",
    "            save_history_csv(r.history, os.path.join(args.results_dir, f\"history_{r.solver}.csv\"))\n",
    "\n",
    "    if args.save_plots and args.results_dir:\n",
    "        plot_training_curves(results, outdir=args.results_dir)\n",
    "\n",
    "    # Benchmarks on the same game\n",
    "    if args.run_benchmarks:\n",
    "        log.banner(\"[BENCH] Heuristics on same game\")\n",
    "        M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "        # Robot heuristics\n",
    "        #  - fixed paths from initial set\n",
    "        robots_init, sensors_init = build_initial_policies_for_bench(env, M_modes)\n",
    "\n",
    "        # Sensor heuristics\n",
    "        sensor_methods: List[SensorPolicy] = [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=args.seed + 123, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "        bench = run_policy_matrix_benchmark(\n",
    "            env,\n",
    "            robot_methods=robots_init,\n",
    "            sensor_methods=sensor_methods,\n",
    "            M_modes=M_modes,\n",
    "            episodes=args.bench_episodes,\n",
    "            base_seed=555000,\n",
    "        )\n",
    "\n",
    "        # Print a compact table\n",
    "        robot_names = [p.name for p in robots_init]\n",
    "        sensor_names = [p.name for p in sensor_methods]\n",
    "        for j, sname in enumerate(sensor_names):\n",
    "            log.info(f\"[BENCH] Sensor={sname}\")\n",
    "            for i, rname in enumerate(robot_names):\n",
    "                cell = bench[(i, j)]\n",
    "                log.info(f\"  Robot={rname:22s} UR={cell.UR:8.2f} goal%={100*cell.goal:5.1f} det%={100*cell.det:5.1f}\")\n",
    "\n",
    "        if args.save_plots and args.results_dir:\n",
    "            plot_benchmark_bars(robot_names, sensor_names, bench, outdir=args.results_dir)\n",
    "\n",
    "\n",
    "# Helper: initial robot set for benchmark (without the solver-added BR policies)\n",
    "\n",
    "def build_initial_policies_for_bench(env: GridWorldStealthEnv, M_modes: int) -> Tuple[List[RobotPolicy], List[SensorPolicy]]:\n",
    "    grid = env.grid\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    # Static risk-aware A* under UNIFORM mode belief\n",
    "    uniform_mode = np.full(M_modes, 1.0 / M_modes)\n",
    "\n",
    "    # Construct risk map directly under uniform belief\n",
    "    H, W = grid.height, grid.width\n",
    "    risk_uniform = np.zeros((H, W), dtype=float)\n",
    "    risk_worst = np.zeros((H, W), dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in grid.obstacles:\n",
    "                risk_uniform[y, x] = np.nan\n",
    "                risk_worst[y, x] = np.nan\n",
    "                continue\n",
    "            vals = [env.true_detection_prob((x, y), m) for m in range(M_modes)]\n",
    "            risk_uniform[y, x] = float(np.dot(uniform_mode, vals))\n",
    "            risk_worst[y, x] = float(np.max(vals))\n",
    "\n",
    "    p_uniform = plan_risk_weighted_path(env, risk_uniform, risk_weight=12.0)\n",
    "    p_worst = plan_risk_weighted_path(env, risk_worst, risk_weight=12.0)\n",
    "\n",
    "    robots: List[RobotPolicy] = [\n",
    "        FixedPathPolicy(p_short, \"R_Shortest\"),\n",
    "        FixedPathPolicy(p_upper, \"R_UpperCorridor\"),\n",
    "        FixedPathPolicy(p_lower, \"R_LowerCorridor\"),\n",
    "        FixedPathPolicy(p_uniform, \"R_RiskAStar_Uniform\"),\n",
    "        FixedPathPolicy(p_worst, \"R_RiskAStar_WorstCase\"),\n",
    "        OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\"),\n",
    "        RandomPolicy(\"R_Random\"),\n",
    "    ]\n",
    "\n",
    "    sensors: List[SensorPolicy] = [FixedModeSensorPolicy(m, name=f\"S_Mode{m}\") for m in range(M_modes)]\n",
    "    return robots, sensors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLI\n",
    "# =============================================================================\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> Tuple[argparse.Namespace, List[str]]:\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--seed\", type=int, default=0)\n",
    "    p.add_argument(\"--log-level\", type=str, default=\"INFO\", choices=[\"QUIET\", \"INFO\", \"DEBUG\"])\n",
    "\n",
    "    p.add_argument(\"--solver\", type=str, default=\"correlated\", choices=[\"marginal\", \"correlated\", \"both\"])\n",
    "\n",
    "    p.add_argument(\"--outer-iters\", type=int, default=3)\n",
    "\n",
    "    p.add_argument(\"--rollouts-payoff\", type=int, default=20)\n",
    "    p.add_argument(\"--rollouts-br\", type=int, default=30)\n",
    "    p.add_argument(\"--risk-weight-br\", type=float, default=12.0)\n",
    "\n",
    "    # NBS knobs\n",
    "    p.add_argument(\"--disagreement\", type=str, default=\"minminus\", choices=[\"minminus\", \"uniform\"])\n",
    "    p.add_argument(\"--entropy-tau\", type=float, default=0.0)\n",
    "\n",
    "    # Fix mismatch knobs\n",
    "    p.add_argument(\"--cond-top-k\", type=int, default=2, help=\"How many top recommendations to check for conditional BRs\")\n",
    "    p.add_argument(\"--br-eval-rollouts\", type=int, default=8, help=\"Small evaluation rollouts for new deviations\")\n",
    "    p.add_argument(\"--add-threshold\", type=float, default=0.25, help=\"Minimum estimated conditional gain to add a deviation policy\")\n",
    "\n",
    "    # Eval episodes under joint X (self-play)\n",
    "    p.add_argument(\"--eval-episodes\", type=int, default=60)\n",
    "\n",
    "    # Debug\n",
    "    p.add_argument(\"--debug-rollout-pair\", type=str, default=\"\", help=\"Print one step-by-step rollout for i,j\")\n",
    "\n",
    "    # Outputs\n",
    "    p.add_argument(\"--results-dir\", type=str, default=\"results\", help=\"Directory for csv/plots\")\n",
    "    p.add_argument(\"--save-plots\", action=\"store_true\")\n",
    "\n",
    "    # Benchmarks\n",
    "    p.add_argument(\"--run-benchmarks\", action=\"store_true\")\n",
    "    p.add_argument(\"--bench-episodes\", type=int, default=80)\n",
    "\n",
    "    args, unknown = p.parse_known_args(args=argv)\n",
    "    return args, unknown\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    args, unknown = parse_args(argv=argv)\n",
    "    if unknown and (\"ipykernel\" not in sys.modules):\n",
    "        print(f\"[WARN] Ignoring unknown CLI args: {unknown}\")\n",
    "    if args.debug_rollout_pair.strip() == \"\":\n",
    "        args.debug_rollout_pair = \"\"\n",
    "    run_pipeline(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and (\"ipykernel\" not in sys.modules):\n",
    "    main()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENTS (fixed game set + baselines + tests + presentation plots)\n",
    "# =============================================================================\n",
    "#\n",
    "# Keep the training code above exactly as-is.\n",
    "# This section adds:\n",
    "#   (A) sanity tests (quick asserts)\n",
    "#   (B) a fixed game-set generator (deterministic)\n",
    "#   (C) an experiment runner that compares solvers + baselines on the same games\n",
    "#   (D) plots + CSV outputs for slides\n",
    "#   (E) **3 core GAME figures** for your report: risk maps + belief trace + tradeoff scatter\n",
    "#\n",
    "# Notebook usage:\n",
    "#   run_sanity_tests()\n",
    "#   make_game_report_figures(outdir=\"results_game_figs\")\n",
    "#   run_fixed_games_experiment(outdir=\"results_exp\", n_games=6, seeds=[0,1,2])\n",
    "#\n",
    "\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GameInstance:\n",
    "    game_id: str\n",
    "    grid: GridConfig\n",
    "    sensor_cfg: SensorConfig\n",
    "    desc: str\n",
    "\n",
    "\n",
    "def _is_reachable(grid: GridConfig) -> bool:\n",
    "    try:\n",
    "        _ = astar_path(grid, grid.start, grid.goal, step_cost=lambda a, b: 1.0, max_expansions=250_000)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def build_fixed_game_set(\n",
    "    n_games: int = 6,\n",
    "    seed: int = 123,\n",
    "    width: int = 15,\n",
    "    height: int = 9,\n",
    "    base_radius: int = 2,\n",
    "    base_p: float = 0.02,\n",
    "    hotspot_p: float = 0.60,\n",
    ") -> List[GameInstance]:\n",
    "    \"\"\"Deterministic *fixed* game set for fair comparisons.\n",
    "\n",
    "    We generate corridor-variant grids by adding a small number of extra obstacles\n",
    "    (without breaking reachability), while keeping the base corridor wall.\n",
    "\n",
    "    Notes:\n",
    "      - Uses a fixed RNG seed => same games every run.\n",
    "      - Guarantees start->goal reachability (ignoring detection risk).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    base_grid = build_two_corridor_grid(width=width, height=height)\n",
    "    wall_x = width // 2\n",
    "\n",
    "    # Keep the two sensor centers near the corridor gaps for interpretability.\n",
    "    base_sensors = ((wall_x, 2), (wall_x, 6))\n",
    "\n",
    "    games: List[GameInstance] = []\n",
    "    attempts = 0\n",
    "\n",
    "    # Increasing difficulty: more extra obstacles.\n",
    "    obstacle_budgets = [0, 2, 4, 6, 8, 10]\n",
    "    while len(games) < n_games:\n",
    "        attempts += 1\n",
    "        if attempts > 4000:\n",
    "            raise RuntimeError(\"Could not generate enough solvable game instances\")\n",
    "\n",
    "        k = len(games)\n",
    "        extra_obs_target = obstacle_budgets[min(k, len(obstacle_budgets) - 1)]\n",
    "\n",
    "        # Candidate cells for extra obstacles (avoid start/goal/sensors and the wall column).\n",
    "        candidates: List[Tuple[int, int]] = []\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                p = (x, y)\n",
    "                if p in base_grid.obstacles:\n",
    "                    continue\n",
    "                if p == base_grid.start or p == base_grid.goal:\n",
    "                    continue\n",
    "                if p in base_sensors:\n",
    "                    continue\n",
    "                if x == wall_x:\n",
    "                    continue\n",
    "                candidates.append(p)\n",
    "\n",
    "        rng.shuffle(candidates)\n",
    "        extra = set(candidates[:extra_obs_target])\n",
    "\n",
    "        grid = GridConfig(\n",
    "            width=base_grid.width,\n",
    "            height=base_grid.height,\n",
    "            start=base_grid.start,\n",
    "            goal=base_grid.goal,\n",
    "            obstacles=frozenset(set(base_grid.obstacles) | extra),\n",
    "        )\n",
    "        if not _is_reachable(grid):\n",
    "            continue\n",
    "\n",
    "        sensor_cfg = SensorConfig(\n",
    "            sensors=base_sensors,\n",
    "            radius=int(base_radius),\n",
    "            base_p=float(base_p),\n",
    "            hotspot_p=float(hotspot_p),\n",
    "        )\n",
    "\n",
    "        game_id = f\"G{k:02d}_extraObs{extra_obs_target}\"\n",
    "        desc = f\"corridors + {extra_obs_target} extra obstacles\"\n",
    "        games.append(GameInstance(game_id=game_id, grid=grid, sensor_cfg=sensor_cfg, desc=desc))\n",
    "\n",
    "    return games\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Sanity tests\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_sanity_tests() -> None:\n",
    "    \"\"\"Fast, high-signal checks so you can trust comparisons.\"\"\"\n",
    "    print(\"[TEST] Running sanity tests...\")\n",
    "\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=0)\n",
    "\n",
    "    # A* reachability\n",
    "    p = astar_path(grid, grid.start, grid.goal, step_cost=lambda a, b: 1.0)\n",
    "    assert p[0] == grid.start and p[-1] == grid.goal\n",
    "\n",
    "    # Belief update preserves simplex\n",
    "    b = ModeBelief(M=2)\n",
    "    env.reset(sensor_mode=0)\n",
    "    out = env.step((1, 0))\n",
    "    b.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "    assert np.isfinite(b.b).all()\n",
    "    assert abs(float(b.b.sum()) - 1.0) < 1e-6\n",
    "    assert (b.b >= -1e-12).all()\n",
    "\n",
    "    # NBS returns simplex\n",
    "    uR = np.array([-1.0, -2.0, -3.0, -4.0])\n",
    "    uS = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "    nbs = solve_nbs(uR, uS, log=Logger(\"QUIET\"), max_iters=50)\n",
    "    x = nbs.x\n",
    "    assert np.isfinite(x).all()\n",
    "    assert (x >= -1e-10).all()\n",
    "    assert abs(float(x.sum()) - 1.0) < 1e-6\n",
    "\n",
    "    # Joint/marginals/conditionals are sane\n",
    "    X = joint_to_matrix(x, m=2, n=2)\n",
    "    sigma_R, sigma_S = marginals_from_joint(X)\n",
    "    assert abs(float(sigma_R.sum()) - 1.0) < 1e-6\n",
    "    assert abs(float(sigma_S.sum()) - 1.0) < 1e-6\n",
    "    qS = conditional_sensor_given_robot(X, 0)\n",
    "    qR = conditional_robot_given_sensor(X, 0)\n",
    "    assert abs(float(qS.sum()) - 1.0) < 1e-6\n",
    "    assert abs(float(qR.sum()) - 1.0) < 1e-6\n",
    "\n",
    "    # CE regrets should be >= 0 (numerical)\n",
    "    UR = np.random.default_rng(0).normal(size=(3, 2))\n",
    "    US = np.random.default_rng(1).normal(size=(3, 2))\n",
    "    Xr = np.full((3, 2), 1.0 / 6)\n",
    "    reg = compute_ce_regrets(UR, US, Xr)\n",
    "    assert reg[\"max_regret_R\"] >= -1e-9\n",
    "    assert reg[\"max_regret_S\"] >= -1e-9\n",
    "\n",
    "    # Rollout returns finite stats\n",
    "    robots, sensors = build_initial_policies(env, M_modes=2)\n",
    "    st = rollout_episode(env, robots[0], sensors[0], M_modes=2, seed=0)\n",
    "    assert np.isfinite(st.U_R) and np.isfinite(st.U_S)\n",
    "\n",
    "    print(\"[TEST] All sanity tests passed ✅\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Fair comparisons on the same fixed game set\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ExperimentRow:\n",
    "    alg: str\n",
    "    solver: str\n",
    "    game_id: str\n",
    "    seed: int\n",
    "    metric: str\n",
    "    value: float\n",
    "\n",
    "\n",
    "def _evaluate_robot_mixture_against_sensor(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sigma_R: np.ndarray,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> StrategyEval:\n",
    "    sigma_R = np.asarray(sigma_R, dtype=float).reshape(-1)\n",
    "    sigma_R = sigma_R / max(float(sigma_R.sum()), 1e-12)\n",
    "\n",
    "    rng = np.random.default_rng(int(base_seed))\n",
    "\n",
    "    UR, US = [], []\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps_list = []\n",
    "    risk_list = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        i = int(rng.choice(len(robots), p=sigma_R))\n",
    "        st = rollout_episode(env, robots[i], sensor, M_modes=M_modes, seed=int(base_seed + k))\n",
    "        UR.append(st.U_R)\n",
    "        US.append(st.U_S)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps_list.append(st.steps)\n",
    "        risk_list.append(st.total_true_risk)\n",
    "\n",
    "    return StrategyEval(\n",
    "        mean_U_R=float(np.mean(UR)),\n",
    "        mean_U_S=float(np.mean(US)),\n",
    "        det_rate=float(det / episodes),\n",
    "        goal_rate=float(goal / episodes),\n",
    "        mean_steps=float(np.mean(steps_list)),\n",
    "        mean_risk=float(np.mean(risk_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "def _summarize_against_sensor_suite(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robots: List[RobotPolicy],\n",
    "    sigma_R: np.ndarray,\n",
    "    sensors_suite: List[SensorPolicy],\n",
    "    M_modes: int,\n",
    "    episodes_per_sensor: int,\n",
    "    base_seed: int,\n",
    ") -> Dict[str, float]:\n",
    "    vals_UR = []\n",
    "    vals_goal = []\n",
    "    vals_det = []\n",
    "\n",
    "    for j, sp in enumerate(sensors_suite):\n",
    "        ev = _evaluate_robot_mixture_against_sensor(\n",
    "            env,\n",
    "            robots,\n",
    "            sigma_R=sigma_R,\n",
    "            sensor=sp,\n",
    "            M_modes=M_modes,\n",
    "            episodes=episodes_per_sensor,\n",
    "            base_seed=base_seed + 10000 * j,\n",
    "        )\n",
    "        vals_UR.append(ev.mean_U_R)\n",
    "        vals_goal.append(ev.goal_rate)\n",
    "        vals_det.append(ev.det_rate)\n",
    "\n",
    "    # \"Robust\" = worst case over the sensor suite.\n",
    "    return {\n",
    "        \"mean_UR\": float(np.mean(vals_UR)),\n",
    "        \"robust_UR\": float(np.min(vals_UR)),\n",
    "        \"mean_goal\": float(np.mean(vals_goal)),\n",
    "        \"robust_goal\": float(np.min(vals_goal)),\n",
    "        \"mean_det\": float(np.mean(vals_det)),\n",
    "        \"robust_det\": float(np.max(vals_det)),\n",
    "    }\n",
    "\n",
    "\n",
    "def _write_experiment_csv(rows: List[ExperimentRow], path: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    safe_makedirs(os.path.dirname(path) or \".\")\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"alg\", \"solver\", \"game_id\", \"seed\", \"metric\", \"value\"])\n",
    "        for r in rows:\n",
    "            w.writerow([r.alg, r.solver, r.game_id, r.seed, r.metric, f\"{r.value:.8f}\"])\n",
    "\n",
    "\n",
    "def _group_stats(values: List[float]) -> Dict[str, float]:\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    return {\n",
    "        \"mean\": float(np.mean(arr)) if arr.size else float(\"nan\"),\n",
    "        \"std\": float(np.std(arr)) if arr.size else float(\"nan\"),\n",
    "        \"n\": int(arr.size),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_experiment_summary(rows: List[ExperimentRow], outdir: str) -> None:\n",
    "    \"\"\"Create *paper-style* figures from ExperimentRow logs.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    algs = sorted({r.alg for r in rows})\n",
    "    games = sorted({r.game_id for r in rows})\n",
    "\n",
    "    bucket: Dict[Tuple[str, str, str], List[float]] = {}\n",
    "    for r in rows:\n",
    "        key = (r.alg, r.game_id, r.metric)\n",
    "        bucket.setdefault(key, []).append(float(r.value))\n",
    "\n",
    "    def get_vals(alg: str, game_id: str, metric: str) -> List[float]:\n",
    "        return bucket.get((alg, game_id, metric), [])\n",
    "\n",
    "    def mean_ci95(vals: List[float]) -> Tuple[float, float]:\n",
    "        arr = np.asarray(vals, dtype=float)\n",
    "        if arr.size == 0:\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "        mu = float(np.mean(arr))\n",
    "        if arr.size == 1:\n",
    "            return mu, 0.0\n",
    "        se = float(np.std(arr, ddof=1) / np.sqrt(arr.size))\n",
    "        return mu, 1.96 * se\n",
    "\n",
    "    def savefig(base: str) -> None:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, base + \".png\"), dpi=300)\n",
    "        plt.savefig(os.path.join(outdir, base + \".pdf\"))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_by_game(metric: str, ylabel: str, base: str) -> None:\n",
    "        plt.figure(figsize=(9.0, 4.2))\n",
    "        x = np.arange(len(games))\n",
    "        for alg in algs:\n",
    "            ys = []\n",
    "            es = []\n",
    "            for gid in games:\n",
    "                mu, ci = mean_ci95(get_vals(alg, gid, metric))\n",
    "                ys.append(mu)\n",
    "                es.append(ci)\n",
    "            ys = np.asarray(ys, dtype=float)\n",
    "            es = np.asarray(es, dtype=float)\n",
    "            plt.plot(x, ys, marker=\"o\", linewidth=2.0, label=alg)\n",
    "            ok = np.isfinite(ys) & np.isfinite(es)\n",
    "            if np.any(ok):\n",
    "                plt.fill_between(x[ok], (ys - es)[ok], (ys + es)[ok], alpha=0.15)\n",
    "        plt.xticks(x, games, rotation=25, ha=\"right\")\n",
    "        plt.xlabel(\"Game instance (increasing obstacle perturbations)\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True, alpha=0.25)\n",
    "        plt.legend(ncol=2, fontsize=9)\n",
    "        savefig(base)\n",
    "\n",
    "    plot_by_game(\"robust_UR\", \"Robust robot utility (worst-case over sensor suite) ↑\", \"fig_robustUR_by_game\")\n",
    "    plot_by_game(\"robust_goal\", \"Robust goal rate (worst-case over sensor suite) ↑\", \"fig_robust_goal_by_game\")\n",
    "    plot_by_game(\"robust_det\", \"Worst-case detection rate over sensor suite ↓\", \"fig_robust_det_by_game\")\n",
    "\n",
    "    def plot_box(metric: str, ylabel: str, base: str) -> None:\n",
    "        plt.figure(figsize=(9.0, 4.2))\n",
    "        data = []\n",
    "        for alg in algs:\n",
    "            vals = [float(r.value) for r in rows if r.alg == alg and r.metric == metric]\n",
    "            data.append(vals)\n",
    "        plt.boxplot(data, labels=algs, showfliers=False)\n",
    "        plt.xticks(rotation=25, ha=\"right\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True, axis=\"y\", alpha=0.25)\n",
    "        savefig(base)\n",
    "\n",
    "    plot_box(\"robust_UR\", \"Robust robot utility (worst-case over sensor suite) ↑\", \"fig_box_robustUR\")\n",
    "    plot_box(\"robust_goal\", \"Robust goal rate (worst-case over sensor suite) ↑\", \"fig_box_goal\")\n",
    "    plot_box(\"robust_det\", \"Worst-case detection rate over sensor suite ↓\", \"fig_box_det\")\n",
    "\n",
    "    plt.figure(figsize=(6.0, 5.2))\n",
    "    for alg in algs:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for gid in games:\n",
    "            mu_det, _ = mean_ci95(get_vals(alg, gid, \"robust_det\"))\n",
    "            mu_goal, _ = mean_ci95(get_vals(alg, gid, \"robust_goal\"))\n",
    "            if np.isfinite(mu_det) and np.isfinite(mu_goal):\n",
    "                xs.append(mu_det)\n",
    "                ys.append(mu_goal)\n",
    "        if xs and ys:\n",
    "            plt.scatter(xs, ys, label=alg)\n",
    "    plt.xlabel(\"Worst-case detection rate (lower is better)\")\n",
    "    plt.ylabel(\"Worst-case goal rate (higher is better)\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(fontsize=9)\n",
    "    savefig(\"fig_goal_det_tradeoff\")\n",
    "\n",
    "    plt.figure(figsize=(6.0, 5.2))\n",
    "    for alg in algs:\n",
    "        xs = [float(r.value) for r in rows if r.alg == alg and r.metric == \"runtime_s\"]\n",
    "        ys = [float(r.value) for r in rows if r.alg == alg and r.metric == \"robust_UR\"]\n",
    "        if xs and ys and len(xs) == len(ys):\n",
    "            plt.scatter(xs, ys, label=alg)\n",
    "    plt.xlabel(\"Runtime (seconds) ↓\")\n",
    "    plt.ylabel(\"Robust robot utility ↑\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(fontsize=9)\n",
    "    savefig(\"fig_runtime_vs_robustUR\")\n",
    "\n",
    "    plt.figure(figsize=(6.0, 5.2))\n",
    "    for alg in algs:\n",
    "        xs = [float(r.value) for r in rows if r.alg == alg and r.metric == \"max_regret_R\"]\n",
    "        ys = [float(r.value) for r in rows if r.alg == alg and r.metric == \"robust_UR\"]\n",
    "        if xs and ys and len(xs) == len(ys):\n",
    "            plt.scatter(xs, ys, label=alg)\n",
    "    plt.xlabel(\"Max conditional regret (robot) ↓\")\n",
    "    plt.ylabel(\"Robust robot utility ↑\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(fontsize=9)\n",
    "    savefig(\"fig_stability_regret_vs_robustUR\")\n",
    "\n",
    "\n",
    "def run_fixed_games_experiment(\n",
    "    outdir: str = \"results_exp\",\n",
    "    n_games: int = 6,\n",
    "    seeds: Optional[List[int]] = None,\n",
    "    outer_iters: int = 3,\n",
    "    rollouts_payoff: int = 20,\n",
    "    rollouts_br: int = 30,\n",
    "    eval_episodes: int = 120,\n",
    "    episodes_per_sensor: int = 80,\n",
    "    disagreement: str = \"minminus\",\n",
    "    entropy_tau: float = 0.0,\n",
    "    cond_top_k: int = 2,\n",
    "    br_eval_rollouts: int = 8,\n",
    "    add_threshold: float = 0.25,\n",
    "    risk_weight_br: float = 12.0,\n",
    "    include_solvers: Optional[List[str]] = None,\n",
    "    include_baselines: bool = True,\n",
    "    log_level: str = \"QUIET\",\n",
    ") -> None:\n",
    "    \"\"\"Run a clean comparison on a deterministic game set.\"\"\"\n",
    "\n",
    "    if seeds is None:\n",
    "        seeds = [0, 1, 2]\n",
    "    if include_solvers is None:\n",
    "        include_solvers = [\"correlated\", \"marginal\"]\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    games = build_fixed_game_set(n_games=n_games)\n",
    "\n",
    "    rows: List[ExperimentRow] = []\n",
    "\n",
    "    def make_sensor_suite(M_modes: int, seed0: int) -> List[SensorPolicy]:\n",
    "        return [\n",
    "            FixedModeSensorPolicy(0, \"S_Mode0\"),\n",
    "            FixedModeSensorPolicy(1, \"S_Mode1\"),\n",
    "            AlternatingSensorPolicy(M_modes, \"S_Alternate\"),\n",
    "            RandomModeSensorPolicy(M_modes, seed=seed0 + 999, name=\"S_RandomMode\"),\n",
    "        ]\n",
    "\n",
    "    for g in games:\n",
    "        for s in seeds:\n",
    "            env = GridWorldStealthEnv(g.grid, g.sensor_cfg, fp=0.05, fn=0.10, seed=s)\n",
    "            M_modes = len(g.sensor_cfg.sensors)\n",
    "            sensors_suite = make_sensor_suite(M_modes=M_modes, seed0=s)\n",
    "\n",
    "            for solver in include_solvers:\n",
    "                args = argparse.Namespace(\n",
    "                    seed=s,\n",
    "                    log_level=log_level,\n",
    "                    solver=solver,\n",
    "                    outer_iters=int(outer_iters),\n",
    "                    rollouts_payoff=int(rollouts_payoff),\n",
    "                    rollouts_br=int(rollouts_br),\n",
    "                    risk_weight_br=float(risk_weight_br),\n",
    "                    disagreement=str(disagreement),\n",
    "                    entropy_tau=float(entropy_tau),\n",
    "                    cond_top_k=int(cond_top_k),\n",
    "                    br_eval_rollouts=int(br_eval_rollouts),\n",
    "                    add_threshold=float(add_threshold),\n",
    "                    eval_episodes=int(eval_episodes),\n",
    "                    debug_rollout_pair=\"\",\n",
    "                    results_dir=\"\",\n",
    "                    save_plots=False,\n",
    "                    run_benchmarks=False,\n",
    "                    bench_episodes=0,\n",
    "                )\n",
    "\n",
    "                t0 = time.time()\n",
    "                log = Logger(log_level)\n",
    "                tr = run_training(env, args, solver=solver, log=log)\n",
    "                runtime_s = float(time.time() - t0)\n",
    "\n",
    "                sigma_R, _sigma_S = marginals_from_joint(tr.X)\n",
    "                suite_summary = _summarize_against_sensor_suite(\n",
    "                    env,\n",
    "                    robots=tr.robots,\n",
    "                    sigma_R=sigma_R,\n",
    "                    sensors_suite=sensors_suite,\n",
    "                    M_modes=M_modes,\n",
    "                    episodes_per_sensor=int(episodes_per_sensor),\n",
    "                    base_seed=100_000 + 1000 * s,\n",
    "                )\n",
    "\n",
    "                last = tr.history[-1]\n",
    "\n",
    "                alg = f\"Solver:{solver}\"\n",
    "                for k, v in suite_summary.items():\n",
    "                    rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=k, value=float(v)))\n",
    "\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"runtime_s\", value=runtime_s))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"entropy_X\", value=float(last.entropy_X)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"max_regret_R\", value=float(last.max_regret_R)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"max_regret_S\", value=float(last.max_regret_S)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"selfplay_UR\", value=float(last.selfplay_UR)))\n",
    "                rows.append(ExperimentRow(alg=alg, solver=solver, game_id=g.game_id, seed=s, metric=\"selfplay_US\", value=float(last.selfplay_US)))\n",
    "\n",
    "            if include_baselines:\n",
    "                robots_base, _ = build_initial_policies_for_bench(env, M_modes=M_modes)\n",
    "\n",
    "                for rp in robots_base:\n",
    "                    sigma = np.zeros(len(robots_base), dtype=float)\n",
    "                    sigma[robots_base.index(rp)] = 1.0\n",
    "\n",
    "                    suite_summary = _summarize_against_sensor_suite(\n",
    "                        env,\n",
    "                        robots=robots_base,\n",
    "                        sigma_R=sigma,\n",
    "                        sensors_suite=sensors_suite,\n",
    "                        M_modes=M_modes,\n",
    "                        episodes_per_sensor=int(episodes_per_sensor),\n",
    "                        base_seed=200_000 + 1000 * s,\n",
    "                    )\n",
    "\n",
    "                    alg = f\"Baseline:{rp.name}\"\n",
    "                    for k, v in suite_summary.items():\n",
    "                        rows.append(ExperimentRow(alg=alg, solver=\"baseline\", game_id=g.game_id, seed=s, metric=k, value=float(v)))\n",
    "\n",
    "    csv_path = os.path.join(outdir, \"experiments.csv\")\n",
    "    _write_experiment_csv(rows, csv_path)\n",
    "    plot_experiment_summary(rows, outdir=outdir)\n",
    "\n",
    "    print(f\"[EXP] Done. Wrote: {csv_path}\")\n",
    "    print(f\"[EXP] Plots saved to: {outdir}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# (E) 3 CORE GAME FIGURES FOR YOUR REPORT\n",
    "# -----------------------------------------------------------------------------\n",
    "# These are NOT \"training\" plots. They are about the GAME itself:\n",
    "#   1) How the map + sensor modes create different risk landscapes and chokepoints\n",
    "#   2) How partial observability drives BELIEF updates during a rollout\n",
    "#   3) The fundamental tradeoff (goal vs detection vs time) across robot behaviors\n",
    "\n",
    "@dataclass\n",
    "class Trace:\n",
    "    pos: List[Tuple[int, int]]\n",
    "    alarm: List[int]\n",
    "    p_true: List[float]\n",
    "    b0: List[float]\n",
    "    b1: List[float]\n",
    "    detected: bool\n",
    "    reached_goal: bool\n",
    "\n",
    "\n",
    "def rollout_trace(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    seed: int,\n",
    "    max_steps: int = 200,\n",
    ") -> Trace:\n",
    "    \"\"\"Like rollout_episode(), but records the trajectory + belief over time.\"\"\"\n",
    "    env.seed(seed)\n",
    "    sensor.reset()\n",
    "    env.reset(sensor_mode=sensor.select_mode(0))\n",
    "\n",
    "    belief = ModeBelief(M_modes)\n",
    "    robot.reset(env.pos)\n",
    "\n",
    "    pos = [env.pos]\n",
    "    alarm = []\n",
    "    p_true = []\n",
    "    b0 = [float(belief.b[0]) if M_modes > 0 else 1.0]\n",
    "    b1 = [float(belief.b[1]) if M_modes > 1 else 0.0]\n",
    "\n",
    "    last_alarm: Optional[int] = None\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.sensor_mode = sensor.select_mode(env.t)\n",
    "        a = robot.act(env, belief, last_alarm)\n",
    "        out = env.step(a)\n",
    "\n",
    "        belief.update(env, out[\"alarm\"], out[\"pos\"])\n",
    "        last_alarm = int(out[\"alarm\"])\n",
    "\n",
    "        pos.append(out[\"pos\"])\n",
    "        alarm.append(int(out[\"alarm\"]))\n",
    "        p_true.append(float(out[\"p_true\"]))\n",
    "        b0.append(float(belief.b[0]) if M_modes > 0 else 1.0)\n",
    "        b1.append(float(belief.b[1]) if M_modes > 1 else 0.0)\n",
    "\n",
    "        if out[\"done\"]:\n",
    "            break\n",
    "\n",
    "    reached_goal = (env.pos == env.grid.goal)\n",
    "    detected = bool(env.detected)\n",
    "    return Trace(pos=pos, alarm=alarm, p_true=p_true, b0=b0, b1=b1, detected=detected, reached_goal=reached_goal)\n",
    "\n",
    "\n",
    "def compute_mode_risk_map(env: GridWorldStealthEnv, mode: int) -> np.ndarray:\n",
    "    H, W = env.grid.height, env.grid.width\n",
    "    rm = np.full((H, W), np.nan, dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if (x, y) in env.grid.obstacles:\n",
    "                continue\n",
    "            rm[y, x] = float(env.true_detection_prob((x, y), mode))\n",
    "    return rm\n",
    "\n",
    "\n",
    "def _plot_grid_overlay(ax, env: GridWorldStealthEnv) -> None:\n",
    "    # obstacles as black squares\n",
    "    for (x, y) in env.grid.obstacles:\n",
    "        ax.add_patch(__import__(\"matplotlib\").patches.Rectangle((x - 0.5, y - 0.5), 1, 1, fill=True, color=\"black\", alpha=0.95))\n",
    "\n",
    "    # start/goal/sensors\n",
    "    sx, sy = env.grid.start\n",
    "    gx, gy = env.grid.goal\n",
    "    ax.scatter([sx], [sy], marker=\"s\", s=80, edgecolor=\"k\")\n",
    "    ax.text(sx + 0.2, sy + 0.2, \"Start\", fontsize=9)\n",
    "\n",
    "    ax.scatter([gx], [gy], marker=\"*\", s=140, edgecolor=\"k\")\n",
    "    ax.text(gx + 0.2, gy + 0.2, \"Goal\", fontsize=9)\n",
    "\n",
    "    for k, (cx, cy) in enumerate(env.sensor_cfg.sensors):\n",
    "        ax.scatter([cx], [cy], marker=\"^\", s=110, edgecolor=\"k\")\n",
    "        ax.text(cx + 0.2, cy + 0.2, f\"S{k}\", fontsize=9)\n",
    "\n",
    "    ax.set_xlim(-0.5, env.grid.width - 0.5)\n",
    "    ax.set_ylim(env.grid.height - 0.5, -0.5)  # origin at top\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xticks(range(env.grid.width))\n",
    "    ax.set_yticks(range(env.grid.height))\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "\n",
    "def _plot_path(ax, path: List[Tuple[int, int]], label: str) -> None:\n",
    "    xs = [p[0] for p in path]\n",
    "    ys = [p[1] for p in path]\n",
    "    ax.plot(xs, ys, linewidth=2.2, label=label)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PairEval:\n",
    "    goal_rate: float\n",
    "    det_rate: float\n",
    "    mean_steps: float\n",
    "\n",
    "\n",
    "def eval_pair_basic(\n",
    "    env: GridWorldStealthEnv,\n",
    "    robot: RobotPolicy,\n",
    "    sensor: SensorPolicy,\n",
    "    M_modes: int,\n",
    "    episodes: int,\n",
    "    base_seed: int,\n",
    ") -> PairEval:\n",
    "    det = 0\n",
    "    goal = 0\n",
    "    steps = []\n",
    "    for k in range(episodes):\n",
    "        st = rollout_episode(env, robot, sensor, M_modes=M_modes, seed=base_seed + k)\n",
    "        det += int(st.detected)\n",
    "        goal += int(st.reached_goal)\n",
    "        steps.append(int(st.steps))\n",
    "    return PairEval(goal_rate=float(goal / episodes), det_rate=float(det / episodes), mean_steps=float(np.mean(steps)))\n",
    "\n",
    "\n",
    "def make_game_report_figures(\n",
    "    outdir: str = \"results_game_figs\",\n",
    "    seed: int = 0,\n",
    "    episodes_per_point: int = 200,\n",
    ") -> None:\n",
    "    \"\"\"Generate 3 **meaningful, paper-usable** plots about the GAME mechanics.\n",
    "\n",
    "    Saves BOTH PNG (easy to view) and PDF (best for LaTeX/papers).\n",
    "\n",
    "    Figures:\n",
    "      Fig1: risk maps for Mode0/Mode1 + key paths overlaid (shows chokepoints)\n",
    "      Fig2: belief evolution + alarms + true risk along one rollout (shows partial observability)\n",
    "      Fig3: goal/detection tradeoff scatter for multiple robot behaviors under each sensor mode\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "    safe_makedirs(outdir)\n",
    "\n",
    "    # Build the canonical game\n",
    "    grid = build_two_corridor_grid()\n",
    "    sensor_cfg = SensorConfig(\n",
    "        sensors=((grid.width // 2, 2), (grid.width // 2, 6)),\n",
    "        radius=2,\n",
    "        base_p=0.02,\n",
    "        hotspot_p=0.60,\n",
    "    )\n",
    "    env = GridWorldStealthEnv(grid, sensor_cfg, fp=0.05, fn=0.10, seed=seed)\n",
    "    M_modes = len(sensor_cfg.sensors)\n",
    "\n",
    "    # Canonical paths (interpretation baselines)\n",
    "    step_cost = lambda a, b: 1.0\n",
    "    p_short = astar_path(grid, grid.start, grid.goal, step_cost)\n",
    "    wall_x = grid.width // 2\n",
    "    p_upper = astar_via_waypoint(grid, grid.start, (wall_x, 2), grid.goal, step_cost)\n",
    "    p_lower = astar_via_waypoint(grid, grid.start, (wall_x, 6), grid.goal, step_cost)\n",
    "\n",
    "    # ------------------------------\n",
    "    # FIG 1 — Risk landscapes + chokepoint paths\n",
    "    # ------------------------------\n",
    "    rm0 = compute_mode_risk_map(env, mode=0)\n",
    "    rm1 = compute_mode_risk_map(env, mode=1)\n",
    "\n",
    "    vmin = float(np.nanmin([rm0, rm1]))\n",
    "    vmax = float(np.nanmax([rm0, rm1]))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10.8, 4.6), constrained_layout=True)\n",
    "\n",
    "    im0 = axes[0].imshow(rm0, vmin=vmin, vmax=vmax)\n",
    "    _plot_grid_overlay(axes[0], env)\n",
    "    _plot_path(axes[0], p_short, \"Shortest\")\n",
    "    _plot_path(axes[0], p_upper, \"Upper gap\")\n",
    "    _plot_path(axes[0], p_lower, \"Lower gap\")\n",
    "    axes[0].set_title(\"Mode 0 risk map (hotspot at S0)\")\n",
    "    axes[0].legend(fontsize=8, loc=\"lower left\")\n",
    "\n",
    "    im1 = axes[1].imshow(rm1, vmin=vmin, vmax=vmax)\n",
    "    _plot_grid_overlay(axes[1], env)\n",
    "    _plot_path(axes[1], p_short, \"Shortest\")\n",
    "    _plot_path(axes[1], p_upper, \"Upper gap\")\n",
    "    _plot_path(axes[1], p_lower, \"Lower gap\")\n",
    "    axes[1].set_title(\"Mode 1 risk map (hotspot at S1)\")\n",
    "    axes[1].legend(fontsize=8, loc=\"lower left\")\n",
    "\n",
    "    cbar = fig.colorbar(im1, ax=axes.ravel().tolist(), shrink=0.95)\n",
    "    cbar.set_label(\"True detection probability p_true(x)\")\n",
    "\n",
    "    fig.savefig(os.path.join(outdir, \"fig1_riskmaps_paths.png\"), dpi=300)\n",
    "    fig.savefig(os.path.join(outdir, \"fig1_riskmaps_paths.pdf\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ------------------------------\n",
    "    # FIG 2 — Belief evolution on a rollout (partial observability)\n",
    "    # ------------------------------\n",
    "    rp = OnlineBeliefReplanPolicy(env, risk_weight=12.0, name=\"R_OnlineBeliefReplan\")\n",
    "    sp_true = FixedModeSensorPolicy(1, name=\"S_Mode1\")\n",
    "\n",
    "    tr = rollout_trace(env, rp, sp_true, M_modes=M_modes, seed=seed)\n",
    "\n",
    "    T = len(tr.pos) - 1\n",
    "    ts = np.arange(T + 1)\n",
    "    ts_step = np.arange(1, T + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10.0, 6.2), sharex=True, constrained_layout=True)\n",
    "\n",
    "    # (a) belief over modes\n",
    "    axes[0].plot(ts, tr.b0, linewidth=2.2, label=\"Belief P(mode=0)\")\n",
    "    axes[0].plot(ts, tr.b1, linewidth=2.2, label=\"Belief P(mode=1)\")\n",
    "    axes[0].set_ylabel(\"Belief\")\n",
    "    axes[0].set_ylim(-0.02, 1.02)\n",
    "    axes[0].yaxis.set_major_locator(MaxNLocator(6))\n",
    "    axes[0].grid(True, alpha=0.25)\n",
    "    axes[0].legend(loc=\"best\")\n",
    "    title_end = \"DETECTED\" if tr.detected else (\"GOAL\" if tr.reached_goal else \"TIMEOUT\")\n",
    "    axes[0].set_title(f\"Belief update under noisy alarms (true mode=1) — episode ends: {title_end}\")\n",
    "\n",
    "    # (b) alarm + true risk\n",
    "    axes[1].plot(ts_step, tr.p_true, linewidth=2.0, label=\"True p_true at visited cell\")\n",
    "    # show alarm events as vertical markers\n",
    "    alarm_ts = [k for k, a in enumerate(tr.alarm, start=1) if a == 1]\n",
    "    if alarm_ts:\n",
    "        axes[1].vlines(alarm_ts, ymin=min(tr.p_true + [0.0]), ymax=max(tr.p_true + [1.0]), alpha=0.25, label=\"Alarm=1\")\n",
    "    axes[1].set_xlabel(\"Time step\")\n",
    "    axes[1].set_ylabel(\"Signal\")\n",
    "    axes[1].grid(True, alpha=0.25)\n",
    "    axes[1].legend(loc=\"best\")\n",
    "\n",
    "    fig.savefig(os.path.join(outdir, \"fig2_belief_trace.png\"), dpi=300)\n",
    "    fig.savefig(os.path.join(outdir, \"fig2_belief_trace.pdf\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ------------------------------\n",
    "    # FIG 3 — Tradeoff scatter: goal vs detection (and time)\n",
    "    # ------------------------------\n",
    "    robots_base, _ = build_initial_policies_for_bench(env, M_modes=M_modes)\n",
    "    sensors_eval = [FixedModeSensorPolicy(0, \"S_Mode0\"), FixedModeSensorPolicy(1, \"S_Mode1\")]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11.0, 4.8), sharey=True, constrained_layout=True)\n",
    "\n",
    "    for j, sp in enumerate(sensors_eval):\n",
    "        xs, ys, ss, labels = [], [], [], []\n",
    "        for i, rp in enumerate(robots_base):\n",
    "            ev = eval_pair_basic(\n",
    "                env,\n",
    "                robot=rp,\n",
    "                sensor=sp,\n",
    "                M_modes=M_modes,\n",
    "                episodes=int(episodes_per_point),\n",
    "                base_seed=50_000 + 10_000 * j + 1000 * i,\n",
    "            )\n",
    "            xs.append(ev.det_rate)\n",
    "            ys.append(ev.goal_rate)\n",
    "            # marker size encodes mean steps (bigger = slower)\n",
    "            ss.append(25.0 + 2.0 * ev.mean_steps)\n",
    "            labels.append(rp.name)\n",
    "\n",
    "        axes[j].scatter(xs, ys, s=ss, alpha=0.9)\n",
    "        for x, y, name in zip(xs, ys, labels):\n",
    "            axes[j].annotate(name, (x, y), textcoords=\"offset points\", xytext=(6, 4), fontsize=8)\n",
    "\n",
    "        axes[j].set_title(f\"Tradeoff vs {sp.name} (true mode fixed)\")\n",
    "        axes[j].set_xlabel(\"Detection rate (lower is better)\")\n",
    "        axes[j].grid(True, alpha=0.25)\n",
    "\n",
    "    axes[0].set_ylabel(\"Goal rate (higher is better)\")\n",
    "\n",
    "    fig.savefig(os.path.join(outdir, \"fig3_tradeoff_scatter.png\"), dpi=300)\n",
    "    fig.savefig(os.path.join(outdir, \"fig3_tradeoff_scatter.pdf\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[GAME-FIGS] Saved 3 report figures to: {outdir}\")\n",
    "    print(\"[GAME-FIGS] Files: fig1_riskmaps_paths.*, fig2_belief_trace.*, fig3_tradeoff_scatter.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dab4afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "[PIPELINE] Stealth grid game\n",
      "====================================================================================================\n",
      "Grid: 15x9 start=(1, 7) goal=(13, 1) obstacles=7\n",
      "Sensors: ((7, 2), (7, 6)) radius=2 base_p=0.02 hotspot_p=0.6\n",
      "[correlated] Initial robots: R_Shortest, R_UpperCorridor, R_LowerCorridor, R_Random, R_OnlineBeliefReplan\n",
      "[correlated] Initial sensors: S_Mode0, S_Mode1\n",
      "====================================================================================================\n",
      "[correlated] Outer iter 1/3\n",
      "====================================================================================================\n",
      "[Eval] Estimating payoffs: m=5, n=2, rollouts=20, base_seed=1100\n",
      "[Eval] Compact payoff summary:\n",
      "  (R0:R_Shortest, S0:S_Mode0) UR= -60.414±2.50 | US=  49.254±0.41 | det%=100.0 goal%=  0.0 steps=9.3 risk=1.11\n",
      "  (R0:R_Shortest, S1:S_Mode1) UR= -41.321±21.09 | US=  25.061±25.61 | det%= 55.0 goal%= 45.0 steps=13.6 risk=0.27\n",
      "  (R1:R_UpperCorridor, S0:S_Mode0) UR= -60.196±1.83 | US=  49.156±0.29 | det%=100.0 goal%=  0.0 steps=9.2 risk=1.00\n",
      "  (R1:R_UpperCorridor, S1:S_Mode1) UR= -27.545±15.94 | US=   9.845±22.66 | det%= 25.0 goal%= 75.0 steps=14.8 risk=0.30\n",
      "  (R2:R_LowerCorridor, S0:S_Mode0) UR= -61.704±3.33 | US=  48.924±0.38 | det%=100.0 goal%=  0.0 steps=10.7 risk=1.05\n",
      "  (R2:R_LowerCorridor, S1:S_Mode1) UR= -56.160±1.02 | US=  49.800±0.26 | det%=100.0 goal%=  0.0 steps=5.3 risk=0.86\n",
      "  (R3:R_Random, S0:S_Mode0) UR=-118.929±51.25 | US=  38.109±8.97 | det%=100.0 goal%=  0.0 steps=67.3 risk=1.58\n",
      "  (R3:R_Random, S1:S_Mode1) UR= -99.579±49.46 | US=  41.319±8.66 | det%=100.0 goal%=  0.0 steps=48.5 risk=1.03\n",
      "  (R4:R_OnlineBeliefReplan, S0:S_Mode0) UR= -54.050±14.83 | US=  38.510±20.86 | det%= 80.0 goal%= 20.0 steps=12.9 risk=1.10\n",
      "  (R4:R_OnlineBeliefReplan, S1:S_Mode1) UR= -26.764±16.95 | US=   7.144±20.77 | det%= 20.0 goal%= 80.0 steps=16.4 risk=0.41\n",
      "[NBS] d=10 disagreement=(-119.929,6.144) entropy_tau=0\n",
      "[NBS] done: obj=7.931608 gains=(63.769,43.656) support=1/10\n",
      "[correlated] Top joint actions:\n",
      "  #1: (R2:R_LowerCorridor, S1:S_Mode1) prob=1.0000\n",
      "  #2: (R0:R_Shortest, S0:S_Mode0) prob=0.0000\n",
      "  #3: (R1:R_UpperCorridor, S0:S_Mode0) prob=0.0000\n",
      "  #4: (R0:R_Shortest, S1:S_Mode1) prob=0.0000\n",
      "  #5: (R1:R_UpperCorridor, S1:S_Mode1) prob=0.0000\n",
      "[correlated] sigma_R=[0. 0. 1. 0. 0.]\n",
      "[correlated] sigma_S=[0. 1.]\n",
      "[correlated] CE-regrets: maxR=29.396 maxS=0.000 | SelfPlay: UR=-55.99 US=49.09 det%=98.3 goal%=1.7 | H(X)=0.000\n",
      "[RobotBR] (Cond_i2) BR path already exists: R_Shortest\n",
      "[correlated] Best robot deviation already in set; est_gain=14.839\n",
      "[SensorBR] (Cond_j1) Best mode=1 E[US]=49.932\n",
      "[correlated] No sensor deviation above threshold (best_gain=0.000).\n",
      "[correlated] Sets: |Pi_R|=5 |Pi_S|=2 | iter_seconds=0.35\n",
      "====================================================================================================\n",
      "[correlated] Outer iter 2/3\n",
      "====================================================================================================\n",
      "[Eval] Estimating payoffs: m=5, n=2, rollouts=20, base_seed=1200\n",
      "[Eval] Compact payoff summary:\n",
      "  (R0:R_Shortest, S0:S_Mode0) UR= -60.625±2.31 | US=  49.285±0.41 | det%=100.0 goal%=  0.0 steps=9.4 risk=1.18\n",
      "  (R0:R_Shortest, S1:S_Mode1) UR= -35.045±20.62 | US=  17.345±25.22 | det%= 40.0 goal%= 60.0 steps=14.8 risk=0.30\n",
      "  (R1:R_UpperCorridor, S0:S_Mode0) UR= -58.740±2.96 | US=  49.140±0.30 | det%=100.0 goal%=  0.0 steps=8.0 risk=0.74\n",
      "  (R1:R_UpperCorridor, S1:S_Mode1) UR= -28.514±17.73 | US=   9.674±22.37 | det%= 25.0 goal%= 75.0 steps=15.7 risk=0.31\n",
      "  (R2:R_LowerCorridor, S0:S_Mode0) UR= -60.889±3.36 | US=  48.829±0.34 | det%=100.0 goal%=  0.0 steps=10.1 risk=0.84\n",
      "  (R2:R_LowerCorridor, S1:S_Mode1) UR= -54.599±7.72 | US=  47.339±10.94 | det%= 95.0 goal%=  5.0 steps=6.0 risk=1.05\n",
      "  (R3:R_Random, S0:S_Mode0) UR=-109.334±59.80 | US=  39.734±10.42 | det%=100.0 goal%=  0.0 steps=58.0 risk=1.33\n",
      "  (R3:R_Random, S1:S_Mode1) UR= -84.927±40.88 | US=  44.007±7.23 | det%=100.0 goal%=  0.0 steps=34.1 risk=0.83\n",
      "  (R4:R_OnlineBeliefReplan, S0:S_Mode0) UR= -52.230±12.88 | US=  38.490±21.30 | det%= 80.0 goal%= 20.0 steps=11.4 risk=0.78\n",
      "  (R4:R_OnlineBeliefReplan, S1:S_Mode1) UR= -32.545±19.57 | US=  14.845±24.65 | det%= 35.0 goal%= 65.0 steps=14.8 risk=0.30\n",
      "[NBS] d=10 disagreement=(-110.334,8.674) entropy_tau=0\n",
      "[NBS] done: obj=7.675543 gains=(55.735,38.665) support=1/10\n",
      "[correlated] Top joint actions:\n",
      "  #1: (R2:R_LowerCorridor, S1:S_Mode1) prob=1.0000\n",
      "  #2: (R0:R_Shortest, S0:S_Mode0) prob=0.0000\n",
      "  #3: (R1:R_UpperCorridor, S0:S_Mode0) prob=0.0000\n",
      "  #4: (R0:R_Shortest, S1:S_Mode1) prob=0.0000\n",
      "  #5: (R1:R_UpperCorridor, S1:S_Mode1) prob=0.0000\n",
      "[correlated] sigma_R=[0. 0. 1. 0. 0.]\n",
      "[correlated] sigma_S=[0. 1.]\n",
      "[correlated] CE-regrets: maxR=26.085 maxS=1.490 | SelfPlay: UR=-55.90 US=49.10 det%=98.3 goal%=1.7 | H(X)=0.000\n",
      "[RobotBR] (Cond_i2) BR path already exists: R_Shortest\n",
      "[correlated] Best robot deviation already in set; est_gain=19.554\n",
      "[SensorBR] (Cond_j1) Best mode=1 E[US]=49.932\n",
      "[correlated] No sensor deviation above threshold (best_gain=0.000).\n",
      "[correlated] Sets: |Pi_R|=5 |Pi_S|=2 | iter_seconds=0.30\n",
      "====================================================================================================\n",
      "[correlated] Outer iter 3/3\n",
      "====================================================================================================\n",
      "[Eval] Estimating payoffs: m=5, n=2, rollouts=20, base_seed=1300\n",
      "[Eval] Compact payoff summary:\n",
      "  (R0:R_Shortest, S0:S_Mode0) UR= -59.213±3.20 | US=  49.193±0.43 | det%=100.0 goal%=  0.0 steps=8.3 risk=0.86\n",
      "  (R0:R_Shortest, S1:S_Mode1) UR= -29.739±17.43 | US=  12.399±23.89 | det%= 30.0 goal%= 70.0 steps=14.4 risk=0.29\n",
      "  (R1:R_UpperCorridor, S0:S_Mode0) UR= -59.759±2.80 | US=  49.199±0.37 | det%=100.0 goal%=  0.0 steps=8.8 risk=0.96\n",
      "  (R1:R_UpperCorridor, S1:S_Mode1) UR= -33.361±20.52 | US=  14.701±24.45 | det%= 35.0 goal%= 65.0 steps=15.6 risk=0.31\n",
      "  (R2:R_LowerCorridor, S0:S_Mode0) UR= -56.726±12.82 | US=  43.886±15.14 | det%= 90.0 goal%= 10.0 steps=10.7 risk=1.03\n",
      "  (R2:R_LowerCorridor, S1:S_Mode1) UR= -56.778±1.59 | US=  49.998±0.30 | det%=100.0 goal%=  0.0 steps=5.7 risk=1.13\n",
      "  (R3:R_Random, S0:S_Mode0) UR=-109.276±40.87 | US=  39.676±7.04 | det%=100.0 goal%=  0.0 steps=58.0 risk=1.28\n",
      "  (R3:R_Random, S1:S_Mode1) UR= -71.601±18.11 | US=  46.461±3.07 | det%=100.0 goal%=  0.0 steps=20.9 risk=0.65\n",
      "  (R4:R_OnlineBeliefReplan, S0:S_Mode0) UR= -53.250±13.66 | US=  38.310±21.21 | det%= 80.0 goal%= 20.0 steps=12.4 risk=0.80\n",
      "  (R4:R_OnlineBeliefReplan, S1:S_Mode1) UR= -32.188±18.99 | US=  14.908±24.74 | det%= 35.0 goal%= 65.0 steps=14.4 risk=0.29\n",
      "[NBS] d=10 disagreement=(-110.276,11.399) entropy_tau=0\n",
      "[NBS] done: obj=7.632871 gains=(53.498,38.599) support=1/10\n",
      "[correlated] Top joint actions:\n",
      "  #1: (R2:R_LowerCorridor, S1:S_Mode1) prob=1.0000\n",
      "  #2: (R0:R_Shortest, S0:S_Mode0) prob=0.0000\n",
      "  #3: (R1:R_UpperCorridor, S0:S_Mode0) prob=0.0000\n",
      "  #4: (R0:R_Shortest, S1:S_Mode1) prob=0.0000\n",
      "  #5: (R1:R_UpperCorridor, S1:S_Mode1) prob=0.0000\n",
      "[correlated] sigma_R=[0. 0. 1. 0. 0.]\n",
      "[correlated] sigma_S=[0. 1.]\n",
      "[correlated] CE-regrets: maxR=27.039 maxS=0.000 | SelfPlay: UR=-56.23 US=49.89 det%=100.0 goal%=0.0 | H(X)=0.000\n",
      "[RobotBR] (Cond_i2) BR path already exists: R_Shortest\n",
      "[correlated] Best robot deviation already in set; est_gain=27.039\n",
      "[SensorBR] (Cond_j1) Best mode=1 E[US]=49.907\n",
      "[correlated] No sensor deviation above threshold (best_gain=0.000).\n",
      "[correlated] Sets: |Pi_R|=5 |Pi_S|=2 | iter_seconds=0.33\n",
      "====================================================================================================\n",
      "[correlated] Finished\n",
      "====================================================================================================\n",
      "[correlated] Final robots: R_Shortest, R_UpperCorridor, R_LowerCorridor, R_Random, R_OnlineBeliefReplan\n",
      "[correlated] Final sensors: S_Mode0, S_Mode1\n",
      "[correlated] Total time: 0.98s\n",
      "[GAME-FIGS] Saved 3 report figures to: results_game_figs\n",
      "[GAME-FIGS] Files: fig1_riskmaps_paths.*, fig2_belief_trace.*, fig3_tradeoff_scatter.*\n"
     ]
    }
   ],
   "source": [
    "main([]) \n",
    "make_game_report_figures(outdir=\"results_game_figs\", seed=0, episodes_per_point=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
